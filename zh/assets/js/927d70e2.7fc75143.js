"use strict";(self.webpackChunkchaos_mesh_website=self.webpackChunkchaos_mesh_website||[]).push([[1045],{12190:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"/experience-as-a-chaos-mesh-lfx-mentee","metadata":{"permalink":"/zh/blog/experience-as-a-chaos-mesh-lfx-mentee","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2022-01-14-experience-as-an-lfx-mentee-for-chaos-mesh.md","source":"@site/blog/2022-01-14-experience-as-an-lfx-mentee-for-chaos-mesh.md","title":"Experience as an LFX Mentee for Chaos Mesh","description":"Experience as an LFX Mentee for Chaos Mesh","date":"2022-01-14T00:00:00.000Z","formattedDate":"2022\u5e741\u670814\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"LFX Mentorship","permalink":"/zh/blog/tags/lfx-mentorship"},{"label":"Monitoring Metrics","permalink":"/zh/blog/tags/monitoring-metrics"}],"readingTime":6.57,"truncated":true,"authors":[{"name":"Chunxu Zhang","title":"LFX mentee","url":"https://github.com/TangliziGit","imageURL":"https://avatars.githubusercontent.com/u/40566218?v=4"}],"nextItem":{"title":"How to Develop a Daily Reporting System to Track Chaos Testing Results","permalink":"/zh/blog/develop-a-daily-reporting-system"}},"content":"![Experience as an LFX Mentee for Chaos Mesh](/img/lfx-mentee-experience-banner.png)\\n\\nI am a graduate student studying software engineering at Nanjing University. My research focuses on DevOps, which has intrinsic connections with chaos engineering and observability. To get involved in the open-source community, understand Kubernetes more deeply, and experience the daily jobs around infrastructure, I applied for the CNCF LFX Mentorship in Fall 2021 to work on the [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) project.\\n\\n\x3c!--truncate--\x3e\\n\\n## Application Process\\n\\nAt the end of August, I finished an internship of a business nature. As expected, I decided that I was not much into business-related work. However, I always had a strong passion for infrastructure technologies. By chance, I discovered the Chaos Mesh project at CNCF LFX Mentorship.I thought this was a great opportunity to work on an open source project, which I had been dreaming about. I also had the right technology stack, so I submitted my resume right before the deadline.\\n\\nThree days later, I received an interview email from my mentor. As part of the interview, the mentor left a small piece of homework - to write a mini-node-exporter that would expose Prometheus metrics and present them in the Grafana dashboard. I was also required to deploy the mini-node-exporter, the configured Prometheus, and Grafana dashboard on the Kubernetes platform. The design and implementation process was very smooth. The only difficulty was to write the Grafana dashboard as a configuration YAML for the Kubernetes deployment. After a series of queries through documentation and experiments, this problem was finally solved.\\n\\nOn August 30, I was lucky enough to receive the good news that I passed the interview. During the one-on-one meeting with the mentor, we simply talked about my familiarity with Kubernetes and other technologies, the main tasks, and some key timelines. I also raised some concerns, such as the pressure of my graduate lab project that might affect the progress of the mentorship, and the design guidelines of the metrics. My mentor understood me well and addressed my concerns.\\n\\n## Project Process\\n\\nThe project I applied for was called [Monitoring Metrics about Chaos Mesh](https://mentorship.lfx.linuxfoundation.org/project/8db683b0-0273-4a83-9ed9-4c33ee2cfcf0), which aimed to improve the observability of the Chaos Mesh system by collecting metrics and providing a Grafana dashboard.\\n\\nDuring the first two weeks of the project, I got familiar with the business process and some code details of chaos mesh. In the next two weeks, I started to write the design document to sort out all the metrics and collection methods. During this time, I studied the metric design guidelines and met with the mentor to understand the details of the proposal and some of the code logic.\\n\\nMost of these metrics are relatively simple to collect, requiring only simple queries to database objects, k8s objects, or some simple counts. However, there are some special metrics that are difficult to collect. For example, you need to query the data by executing commands in the network namespace of the corresponding container, or query all the containers under the daemon through three different container runtimes, or collect data on the communication between the gRPC client and the server.\\n\\nThese tasks were strange to me. Therefore, I had to ask my mentor for technical support from time to time, and he was always very responsive. I was greatly impressed by my mentor\u2019s extensive knowledge and experience in this field. Under the guidance from my mentor, I was finally able to put together the [RFC](https://github.com/chaos-mesh/rfcs/pull/23) document for my design. Later, in order to track my work, I created a [tracking issue](https://github.com/chaos-mesh/chaos-mesh/issues/2397).\\n\\n![Tracking issue](/img/lfx-mentee-experience-tracking-issue.png)\\n\\nHowever, during the subsequent coding work, I encountered various problems. In retrospect, I found that many of them could have been solved in advance. So I have summarized some suggestions below:\\n\\n**Keep thinking critically**. When I accepted the proposal, I proposed my solution for each metric off the top of my head, but ignored some basic questions: are these metrics necessary? Do we have a better solution that\u2019s available? These basic questions should have been addressed during the proposal phase, but they were propagated to the later design implementation phase. For example, when submitting the RFC, I was reminded by my mentor and reviewers that some metrics were already implemented in the controller-runtime library. When I was working on BPM-related metrics, I was asked similarly by the reviewer. Only then did I realize that I had never paid attention to it.\\n\\n![Comments about BPM metrics](/img/lfx-mentee-experience-thinking-critically.png)\\n\\n**Continuous communication**. How to communicate effectively is a very important issue in this mentorship. There are many lessons learned about communication, but the most profound is that it is better to give options before getting advice. When you have to ask for help, provide some options for the other party to reference. Although these options may not be valid, it contains your own thinking. Therefore, unless you still have no idea after thinking things through, don\u2019t put other people in the middle of your questions.\\n\\n**Understand open source**. This is my first actual experience with open source. Compared with working in a company, things are a lot different. Here are some examples:\\n\\n1. The way information is synchronized. Unlike working in a company where we communicate often with face-to-face meetings, basically most of the communications with an open source community are concentrated in slack channels, GitHub issues, and pull requests. Therefore, we need to record our work so that we can always let other folks know what is going on. In the first few weeks, I maintained an online R&D document based on my previous habit. Later I found that it was better to set up a Kanban or issue on GitHub, so that I would not introduce additional communication cost for my mentor by using a different platform.\\n\\n   ![Online R&D document](/img/lfx-mentee-experience-rd-doc.png)\\n\\n2. Better and more rigorous automated testing. For business companies, automated testing only includes static code analysis, unit testing and simple smoke testing, while manual testing will be more rigorous. But in open source projects, the automated code pipeline contains more detailed and complete test cases, such as integration testing, end-to-end testing, license checking, and so on. The quality and security of the submitted code will be checked initially in this phase.\\n\\n3. Code review. Many people will participate in your code reviews, and the review may last for a long time. Unlike company work, there are no dedicated reviewers in an open source community. It could be users, maintainers, or other community members who are either assigned or voluntarily do the job, which may be part of the reason for the long review duration.\\n\\n## After the project\\n\\nI had a wonderful experience in these 12 weeks. I gained a deeper understanding of Kubernetes, CRD, and observability. I also realized that I was still lacking a lot of knowledge on how to improve code structure, Linux basics, and container technologies. There is still more to learn!\\n\\nAt the same time, because of the unexpected pressure of the graduate lab project, I didn\u2019t have much dedicated time for the mentorship. I didn\u2019t even get to finish the design of the Grafana part within the time frame. I will definitely follow up with it and hope to finish it successfully and give a real conclusion to this project.\\n\\nI would like to thank my mentor [@STRRL](https://github.com/STRRL). During my internship, I encountered many problems in the project, such as Git operations, cycle dependency solutions, and finding the runtime interface for CRI-O. Without my mentor\'s patience and guidance, it would have been difficult for me to complete these unfamiliar technical challenges. I would also like to thank the maintainers of Chaos Mesh for reviewing my code, and the CNCF LFX Mentorship project for providing a great platform for all of us who want to participate in the open-source community.\\n\\n![Mentor\'s LGTM](/img/lfx-mentee-experience-mentors-lgtm.png)\\n\\nFinally, I hope every student who wants to be part of the open-source community can take the first step with LFX Mentorship!"},{"id":"/develop-a-daily-reporting-system","metadata":{"permalink":"/zh/blog/develop-a-daily-reporting-system","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2022-01-11-develop-a-daily-reporting-system.md","source":"@site/blog/2022-01-11-develop-a-daily-reporting-system.md","title":"How to Develop a Daily Reporting System to Track Chaos Testing Results","description":"How to Develop a Daily Reporting System to Track Chaos Testing Results","date":"2022-01-11T00:00:00.000Z","formattedDate":"2022\u5e741\u670811\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Use case","permalink":"/zh/blog/tags/use-case"}],"readingTime":6.995,"truncated":true,"authors":[{"name":"Lei Li","title":"Senior software engineer at Digital China","url":"https://github.com/lileiaab","imageURL":"https://avatars.githubusercontent.com/u/88825087?v=4"}],"prevItem":{"title":"Experience as an LFX Mentee for Chaos Mesh","permalink":"/zh/blog/experience-as-a-chaos-mesh-lfx-mentee"},"nextItem":{"title":"Share your #ChaosMeshStory!","permalink":"/zh/blog/share-your-chaos-mesh-story"}},"content":"![How to Develop a Daily Reporting System to Track Chaos Testing Results](/img/chaos-mesh-digitalchina-banner.png)\\n\\nChaos Mesh is a cloud-native chaos engineering platform that orchestrates chaos experiments on Kubernetes environments. It allows you to test the resilience of your system by simulating problems such as network faults, file system faults, and Pod faults. After each chaos experiment, you can review the testing results by checking the logs. But this is neither direct nor efficient. Therefore, I decided to develop a daily reporting system that would automatically analyze logs and generate reports. This way, it\u2019s easy to examine the logs and identify the issues.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this article, I will give you some insights about how to build a daily reporting system, as well as the problems I encountered during the process and how I fixed them.\\n\\n## Deploy Chaos Mesh on Kubernetes\\n\\nChaos Mesh is designed for Kubernetes, which is one of the important reasons why it can allow users to inject faults into the file system, Pod, or network for specific applications.\\n\\nIn earlier documents, Chaos Mesh offered two ways to quickly deploy a virtual Kubernetes cluster on your machine: [kind](https://github.com/kubernetes-sigs/kind) and [minikube](https://minikube.sigs.k8s.io/docs/start/). Generally, it only takes a one-line command to deploy a Kubernetes cluster as well as install Chaos Mesh. But there are some problems:\\n\\n- Starting Kubernetes clusters locally affects network-related fault types.\\n- Users on the Chinese mainland might experience an extremely slow process to pull the Docker image or even a timeout.\\n\\nIf you use the provided script to deploy a Kubernetes cluster using kind, all Kubernetes nodes are virtual machines (VM). This adds difficulty when you pull the image offline. To address this issue, you can deploy the Kubernetes cluster on multiple physical machines instead, with each physical machine acting as a worker node. To expedite the image pulling process, you can use the `docker load` command to load the required image in advance. Apart from the two problems above, you can install [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) and [Helm](https://helm.sh/) by following the documentation.\\n\\nNote: For the latest installation and deployment instructions, refer to [Chaos Mesh Quick Start](https://chaos-mesh.org/docs/quick-start/).\\n\\n## Deploy TiDB\\n\\nThe next step is to deploy TiDB on Kubernetes. I used TiDB Operator to streamline the process. For details, check out [Get started with TiDB Operator in Kubernetes](https://docs.pingcap.com/tidb-in-kubernetes/stable/get-started).\\n\\nI\u2019d like to highlight two points in this process:\\n\\n- First, install Custom Resource Definitions (CRDs) to implement different components of TiDB Operator. Otherwise, you\u2019ll get errors when you try to install TiDB Operator.\\n- Use [Longhorn](https://longhorn.io/), a distributed block storage system for Kubernetes, to create local persistent volumes (PV) for your Kubernetes cluster. This way, you don\u2019t have to create PVs in advance: whenever a Pod is pulled, a PV is automatically created and mounted.\\n\\nThe biggest problem that I encountered was that pulling the image could be extremely slow when deploying the service. If the nodes in your Kubernetes cluster are virtual machines, pull the required images in advance and load them to the Docker of each machine:\\n\\n```bash\\n## Pull required images on a machine with a good network connection\\ndocker pull pingcap/tikv:latest\\ndocker pull pingcap/tidb:latest\\ndocker pull pingcap/pd:latest\\n\\n## Export images and save them to each machine in the Kubernetes cluster\\ndocker save -o tikv.tar pingcap/tikv:latest\\ndocker save -o tidb.tar pingcap/tidb:latest\\ndocker save -o pd.tar pingcap/pd:latest\\n\\n## Load images to each machine\\ndocker load &lt; tikv.tar\\ndocker load &lt; tidb.tar\\ndocker load &lt; pd.tar\\n```\\n\\nThe above commands allow you to use the TiDB image in the local Docker registry to deploy the latest TiDB cluster, saving you the trouble of pulling the image from the remote repository. The idea also applies to the Chaos Mesh installation as described earlier. If you do not know which images you need to pull, install Chaos Mesh using Helm to trigger the installation process, then use the `kubectl describe` command to verify:\\n\\n```bash\\n## Check pods that are deployed in a specific namespace.\\nkubectl describe pods -n tidb-test\\n```\\n\\nThe mirror pulling process usually takes the longest time to complete. If the Pod is being scheduled to a node, check it later.\\n\\n## Run a chaos experiment\\n\\nTo run a chaos experiment, you have to define it first through YAML files and use `kubectl apply` to start it. In this example, I created a chaos experiment using PodChaos to simulate a Pod crashing. For detailed instructions, refer to [Run a Chaos Experiment](https://chaos-mesh.org/docs/run-a-chaos-experiment/).\\n\\n## Generate daily report\\n\\n### Collect logs\\n\\nUsually, when you run chaos experiments on TiDB clusters, many errors are returned. To collect those error logs, run the `kubectl logs` command:\\n\\n```bash\\nkubectl logs &lt;podname> -n tidb-test --since=24h >> tidb.log\\n```\\n\\nAll logs generated in the past 24 hours of the specific Pod in the `tidb-test` namespace will be saved to the `tidb.log` file.\\n\\n### Filter errors and warnings\\n\\nIn this step, you have to filter error messages and warning messages from logs. There are two options:\\n\\n- Use text processing tools, such as awk. This requires a proficient understanding of Linux/Unix commands.\\n- Write a script. If you\u2019re not familiar with Linux/Unix commands, this is the better option.\\n\\n### Draw a plot\\n\\nFor plotting, I used [gnuplot](http://www.gnuplot.info/), a Linux command-line graphing utility. In the example below, I imported the pressure measurement results and created a line graph to show how queries per second (QPS) were affected when a specific Pod became unavailable. Since the chaos experiment was executed periodically, the number of QPS exhibited a pattern: it would drop abruptly and then quickly return to normal.\\n\\n![QPS line graph](/img/qps-line-graph.png)\\n\\n<p class=\\"caption-center\\">QPS line graph</p>\\n\\n### Generate the report in PDF\\n\\nCurrently, there is no available API for generating Chaos Mesh reports or analyzing results. I decided to generate the report in PDF format so it would be readable on different browsers. In my case, I used [gopdf](https://github.com/signintech/gopdf), a support library that allows users to create PDF files. It also lets me insert images or draw tables, which meets my needs.\\n\\nTo generate a daily report, I used [crond](https://www.linux.org/docs/man8/cron.html), a command-line utility that executes cron jobs in the background, to execute the commands early each morning. So, when I start work, there is a daily report waiting for me.\\n\\n## Build a web application for daily reporting\\n\\nBut I want to make the report more readable and accessible. Isn\u2019t it nicer if you can check reports on a web application? At first, I wanted to add a backend API and track when the report was generated. It sounds applicable but it may be too much work since all I want is to know which report requires further troubleshooting. The exact information is shown in the file name, for example: `report-2021-07-09-bad.pdf`. Thus, the reporting system\u2019s workload and complexity are greatly reduced.\\n\\nStill, it is necessary to improve the backend interfaces as well as enrich the report content. But for now, a daily, workable reporting system is just fine.\\n\\nIn my case, I used [Vue.js](https://github.com/vuejs/vue) to scaffold the web application using a UI library [antd](https://www.antdv.com/docs/vue/introduce/). After that, I updated the page content by saving the automatically generated report to the static resources folder `static`. This allows the web application to read the static reports and then render them to the front end page. For details, check out [Use antd in vue-cli 3](https://www.antdv.com/docs/vue/use-with-vue-cli/).\\n\\nBelow is an example of a web application that I developed for daily reporting. The red card indicates that I should take a look at the testing report because exceptions are thrown after running chaos experiments.\\n\\n![Web application for daily reporting](/img/web-app-for-daily-reporting.png)\\n\\n<p class=\\"caption-center\\">Web application for daily reporting</p>\\n\\nClicking the red card will open the report, as shown below. I used [pdf.js](https://github.com/mozilla/pdf.js) to view the PDF.\\n\\n![Daily report in PDF](/img/daily-report-pdf.png)\\n\\n<p class=\\"caption-center\\">Daily report in PDF</p>\\n\\n## Summary\\n\\nChaos Mesh enables you to simulate faults that most cloud-native applications might encounter. In this article, I created a PodChaos experiment and observed that QPS in the TiDB cluster was affected when the Pod became unavailable. After analyzing the logs, I can enhance the robustness and high availability of the system. I built a web application to generate daily reports for troubleshooting and debugging. You can also customize the reports to meet your own requirements.\\n\\nOur team is also working on a project to [make TiDB compatible with PostgreSQL](https://github.com/DigitalChinaOpenSource/TiDB-for-PostgreSQL). If you are interested and want to make contributions, you are welcome to pick an issue and get started.\\n\\n**Originally published at _[The New Stack](https://thenewstack.io/develop-a-daily-reporting-system-for-chaos-mesh-to-improve-system-resilience/)_.**"},{"id":"/share-your-chaos-mesh-story","metadata":{"permalink":"/zh/blog/share-your-chaos-mesh-story","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-12-27-chaos-mesh-story.md","source":"@site/blog/2021-12-27-chaos-mesh-story.md","title":"Share your #ChaosMeshStory!","description":"Share your #ChaosMeshStory!","date":"2021-12-27T00:00:00.000Z","formattedDate":"2021\u5e7412\u670827\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"community","permalink":"/zh/blog/tags/community"},{"label":"event","permalink":"/zh/blog/tags/event"}],"readingTime":0.98,"truncated":true,"authors":[{"name":"Chaos Mesh Community","url":"https://github.com/chaos-mesh","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"How to Develop a Daily Reporting System to Track Chaos Testing Results","permalink":"/zh/blog/develop-a-daily-reporting-system"},"nextItem":{"title":"Deploy Chaos Mesh on KubeSphere","permalink":"/zh/blog/deploy-chaos-mesh-on-kubesphere"}},"content":"![Share your #ChaosMeshStory!](/img/chaos-mesh-story.jpeg)\\n\\nHey community,\\n\\n\ud83e\udd73 Chaos Mesh will turn 2 on 2021.12.31! We\'re grateful for every contribution from you that helped this project grow. And we\'d like to hear your Chaos Mesh story! How did you hear about the project? How did you get involved? Are you an adopter or a contributor? What do you think of it? It can be anything! Share your **[#ChaosMeshStory](https://twitter.com/intent/tweet?text=%23ChaosMeshStory)** over on Twitter and win a Chaos Mesh Tee!\\n\\n\x3c!--truncate--\x3e\\n\\n## \ud83c\udf7c #ChaosMeshStory Entry via Twitter\\n\\n### Rules\\n\\n- Eligibility: anyone with a Twitter account\\n- Event Period: December 27, 2021, at 9:00 AM - December 31, 2021, at 11:59 PM (PT)\\n- How to participate:\\n  1. Follow [@chaos_mesh](https://twitter.com/chaos_mesh).\\n  2. Add the **[#ChaosMeshStory](https://twitter.com/intent/tweet?text=%23ChaosMeshStory)** hashtag.\\n  3. Share your experience with Chaos Mesh!\\n- Judging Criteria: all qualified entries will be eligible to receive a Chaos Mesh Tee. The link to collect your contact information will be available on January 1st, 2022. Stay tuned with [@chaos_mesh](https://twitter.com/chaos_mesh) on Twitter!\\n- Rules: please comply with the [Code of Conduct](https://github.com/chaos-mesh/chaos-mesh/blob/master/CODE_OF_CONDUCT.md), otherwise you will be ineligible to participate.\\n\\nIf you have any questions regarding the event, please DM [@chaos_mesh](https://twitter.com/chaos_mesh).\\n\\nHave fun!\\n\\nYours truly,\\n\\nChaos Mesh community"},{"id":"/deploy-chaos-mesh-on-kubesphere","metadata":{"permalink":"/zh/blog/deploy-chaos-mesh-on-kubesphere","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-12-22-deploy-chaos-mesh-on-kubesphere.md","source":"@site/blog/2021-12-22-deploy-chaos-mesh-on-kubesphere.md","title":"Deploy Chaos Mesh on KubeSphere","description":"Deploy Chaos Mesh on KubeSphere","date":"2021-12-22T00:00:00.000Z","formattedDate":"2021\u5e7412\u670822\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"community","permalink":"/zh/blog/tags/community"}],"readingTime":3.065,"truncated":true,"authors":[{"name":"Cwen Yin","title":"Chaos Mesh Maintainer","url":"https://github.com/cwen0","imageURL":"https://avatars.githubusercontent.com/u/22956341?v=4"}],"prevItem":{"title":"Share your #ChaosMeshStory!","permalink":"/zh/blog/share-your-chaos-mesh-story"},"nextItem":{"title":"Chaos Mesh + SkyWalking: Better Observability for Chaos Engineering","permalink":"/zh/blog/better-observability-for-chaos-engineering"}},"content":"![Deploy Chaos Mesh on KubeSphere](/img/chaos-mesh-kubesphere-banner.png)\\n\\n\\n[Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is a cloud-native Chaos Engineering platform that orchestrates chaos in Kubernetes environments. With Chaos Mesh, you can test your system\'s resilience and robustness on Kubernetes by injecting various types of faults into Pods, network, file system, and even the kernel.\\n\\n\x3c!--truncate--\x3e\\n\\n![Chaos Mesh architecture](/img/chaos-mesh-architecture-2.0.png)\\n\\n## What\u2019s KubeSphere\\n\\n[KubeSphere](https://kubesphere.io/) is a distributed operating system for cloud-native application management, using Kubernetes as its kernel. It provides a plug-and-play architecture, allowing third-party applications to be seamlessly integrated into its ecosystem.\\n\\nKubeSphere 3.2.0 adds the feature of dynamically loading community-developed Helm charts into the [KubeSphere App Store](https://kubesphere.io/docs/pluggable-components/app-store/). Thanks to this new feature, Chaos Mesh is now available on KubeSphere. In this tutorial, you will learn how to deploy Chaos Mesh on KubeSphere to conduct chaos experiments.\\n\\n## Enable App Store on KubeSphere \\n\\n1. Make sure you have installed and enabled the [KubeSphere App Store](https://kubesphere.io/docs/pluggable-components/app-store/).\\n\\n2. You need to create a workspace, a project, and a user account (project-regular) for this tutorial. The account needs to be a platform regular user and to be invited as the project operator with the operator role. For more information, see [Create Workspaces, Projects, Users and Roles](https://kubesphere.io/docs/quick-start/create-workspace-and-project/).\\n\\n## Chaos experiments with Chaos Mesh\\n\\n### Step 1: Deploy Chaos Mesh \\n  \\n1. Login KubeSphere as `project-regular`, search for **chaos-mesh** in the **App Store**, and click on the search result to enter the app. \\n\\n    ![Chaos Mesh app](/img/chaos-mesh-app.png)\\n        \\n2.  In the **App Information** page, click **Install** on the upper right corner.\\n\\n    ![Install Chaos Mesh](/img/install-chaos-mesh.png)\\n        \\n3. In the **App Settings** page, set the application **Name,** **Location** (as your Namespace), and **App Version**, and then click **Next** on the upper right corner.\\n\\n    ![Chaos Mesh basic information](/img/chaos-mesh-basic-info.png)\\n\\n4. Configure the `values.yaml` file as needed, or click **Install** to use the default configuration.\\n\\n    ![Chaos Mesh configurations](/img/chaos-mesh-config.png)\\n\\n5. Wait for the deployment to be finished. Upon completion, Chaos Mesh will be shown as **Running** in KubeSphere. \\n\\n    ![Chaos Mesh deployed](/img/chaos-mesh-deployed.png)\\n\\n### Step 2: Visit Chaos Dashboard\\n\\n1. In the **Resource Status** page, copy the **NodePort **of `chaos-dashboard`.\\n\\n    ![Chaos Mesh NodePort](/img/chaos-mesh-nodeport.png)\\n\\n2. Access the Chaos Dashboard by entering `${NodeIP}:${NODEPORT}` in your browser. Refer to [Manage User Permissions](https://chaos-mesh.org/docs/manage-user-permissions/) to generate a Token and log into Chaos Dashboard. \\n\\n    ![Login to Chaos Dashboard](/img/login-to-dashboard.png)\\n\\n### Step 3: Create a chaos experiment\\n\\nBefore creating a chaos experiment, you should identify and deploy your experiment target, for example, to test how an application works under network latency. Here, we use a demo application `web-show` as the target application to be tested, and the test goal is to observe the system network latency. You can deploy a demo application `web-show` with the following command: `web-show`.   \\n\\n```bash\\ncurl -sSL https://mirrors.chaos-mesh.org/latest/web-show/deploy.sh | bash\\n```  \\n    \\n> Note: The network latency of the Pod can be observed directly from the web-show application pad to the kube-system pod.\\n    \\n1. From your web browser, visit `${NodeIP}:8081` to access the **Web Show** application.\\n\\n    ![Chaos Mesh web show app](/img/web-show-app.png)\\n\\n2. Log in to Chaos Dashboard to create a chaos experiment. To observe the effect of network latency on the application, we set the **Target **as \\"Network Attack\\" to simulate a network delay scenario. \\n    \\n    ![Chaos Dashboard](/img/chaos-dashboard-networkchaos.png)\\n        \\n    The **Scope** of the experiment is set to `app: web-show`.\\n        \\n    ![Chaos Experiment scope](/img/chaos-experiment-scope.png)   \\n        \\n3. Start the chaos experiment by submitting it. \\n\\n    ![Submit Chaos Experiment](/img/start-chaos-experiment.png)  \\n\\nNow, you should be able to visit **Web Show** to observe experiment results:    \\n\\n![Chaos Experiment result](/img/experiment-result.png)  \\n\\n\\n## To summarize\\n\\nKubeSphere makes cloud-native application deployments and maintenance easy. Thanks to the App Store, users can easily deploy Chaos Mesh on KubeSphere with just a few clicks, enabling you to quickly start your own chaos experiments.\\n\\nTo learn more about Chaos Mesh, refer to the [Chaos Mesh docs](https://chaos-mesh.org/docs/) or join the community Slack ([CNCF](https://slack.cncf.io/)/#project-chaos-mesh)."},{"id":"/better-observability-for-chaos-engineering","metadata":{"permalink":"/zh/blog/better-observability-for-chaos-engineering","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-12-16-chaos-mesh-skywalking.md","source":"@site/blog/2021-12-16-chaos-mesh-skywalking.md","title":"Chaos Mesh + SkyWalking: Better Observability for Chaos Engineering","description":"Chaos Mesh + SkyWalking: Better Observability for Chaos Engineering","date":"2021-12-16T00:00:00.000Z","formattedDate":"2021\u5e7412\u670816\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Tutorial","permalink":"/zh/blog/tags/tutorial"}],"readingTime":5.14,"truncated":true,"authors":[{"name":"Ningxuan Wang","title":"Chaos Mesh Contributor","url":"https://github.com/FingerLeader","imageURL":"https://avatars.githubusercontent.com/u/43462394?v=4"}],"prevItem":{"title":"Deploy Chaos Mesh on KubeSphere","permalink":"/zh/blog/deploy-chaos-mesh-on-kubesphere"},"nextItem":{"title":"Implementing Chaos Engineering in K8s: Chaos Mesh Principle Analysis and Control Plane Development","permalink":"/zh/blog/implement-chaos-engineering-in-k8s"}},"content":"![Chaos Mesh + SkyWalking: Better Observability for Chaos Engineering](/img/chaos-mesh-skywalking-banner.png)\\n\\n[Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is an open-source cloud-native [chaos engineering](https://en.wikipedia.org/wiki/Chaos_engineering) platform. You can use Chaos Mesh to conveniently inject failures and simulate abnormalities that might occur in reality, so you can identify potential problems in your system. Chaos Mesh also offers a Chaos Dashboard which allows you to monitor the status of a chaos experiment. However, this dashboard cannot let you observe how the failures in the experiment impact the service performance of applications. This hinders us from further testing our systems and finding potential problems. \\n\\n\x3c!--truncate--\x3e\\n\\n[Apache SkyWalking](https://github.com/apache/skywalking) is an open-source application performance monitor (APM), specially designed to monitor, track, and diagnose cloud native, container-based distributed systems. It collects events that occur and then displays them on its dashboard, allowing you to observe directly the type and number of events that have occurred in your system and how different events impact the service performance. \\n\\nWhen you use SkyWalking and Chaos Mesh together during chaos experiments, you can observe how different failures impact the service performance. \\n\\nThis tutorial will show you how to configure SkyWalking and Chaos Mesh. You\u2019ll also learn how to leverage the two systems to monitor events and observe in real time how chaos experiments impact applications\u2019 service performance. \\n\\n## Preparation\\n\\nBefore you start to use SkyWalking and Chaos Mesh, you have to:\\n\\n* Set up a SkyWalking cluster according to [the SkyWalking configuration guide](https://github.com/apache/skywalking-kubernetes#install).\\n* Deploy Chao Mesh [using Helm](https://chaos-mesh.org/docs/production-installation-using-helm/).\\n* Install [JMeter](https://jmeter.apache.org/index.html) or other Java testing tools (to increase service loads).\\n* Configure SkyWalking and Chaos Mesh according to [this guide](https://github.com/chaos-mesh/chaos-mesh-on-skywalking) if you just want to run a demo.\\n\\nNow, you are fully prepared, and we can cut to the chase. \\n\\n## Step 1: Access the SkyWalking cluster\\n\\nAfter you install the SkyWalking cluster, you can access its user interface (UI). However, no service is running at this point, so before you start monitoring, you have to add one and set the agents.\\n\\nIn this tutorial, we take Spring Boot, a lightweight microservice framework, as an example to build a simplified demo environment.\\n\\n1. Create a SkyWalking demo in Spring Boot by referring to [this document](https://github.com/chaos-mesh/chaos-mesh-on-skywalking/blob/master/demo-deployment.yaml).\\n2. Execute the command `kubectl apply -f demo-deployment.yaml -n skywalking` to deploy the demo. \\n\\nAfter you finish deployment, you can observe the real-time monitoring results at the SkyWalking UI. \\n\\n**Note:** Spring Boot and SkyWalking have the same default port number: 8080. Be careful when you configure the port forwarding; otherise, you may have port conflicts. For example, you can set Spring Boot\u2019s port to 8079 by using a command like `kubectl port-forward svc/spring-boot-skywalking-demo 8079:8080 -n skywalking` to avoid conflicts. \\n\\n## Step 2: Deploy SkyWalking Kubernetes Event Exporter\\n\\n[SkyWalking Kubernetes Event Exporter](https://github.com/apache/skywalking-kubernetes-event-exporter) is able to watch, filter, and send Kubernetes events into the SkyWalking backend. SkyWalking then associates the events with the system metrics and displays an overview about when and how the metrics are affected by the events.\\n\\nIf you want to deploy SkyWalking Kubernetes Event Explorer with one line of commands, refer to [this document](https://github.com/chaos-mesh/chaos-mesh-on-skywalking/blob/master/exporter-deployment.yaml) to create configuration files in YAML format and then customize the parameters in the filters and exporters. Now, you can use the command `kubectl apply` to deploy SkyWalking Kubernetes Event Explorer. \\n\\n## Step 3: Use JMeter to increase service loads\\n\\nTo better observe the change in service performance, you need to increase the service loads on Spring Boot. In this tutorial, we use JMeter, a widely adopted Java testing tool, to increase the service loads. \\n\\nPerform a stress test on `localhost:8079` using JMeter and add five threads to continuously increase the service loads. \\n\\n![JMeter Dashboard 1](/img/jmeter-1.png)\\n\\n\\n![JMeter Dashboard 2](/img/jmeter-2.png)\\n\\nOpen the SkyWalking Dashboard. You can see that the access rate is 100%, and that the service loads reach about 5,300 calls per minute (CPM). \\n\\n![SkyWalking Dashboard](/img/skywalking-dashboard.png)\\n\\n## Step 4: Inject failures via Chaos Mesh and observe results \\n\\nAfter you finish the three steps above, you can use the Chaos Dashboard to simulate stress scenarios and observe the change in service performance during chaos experiments. \\n\\n![StressChaos on Chaos Dashboard](/img/chaos-dashboard-stresschaos.png)\\n\\nThe following sections describe how service performance varies under the stress of three chaos conditions:\\n\\n* CPU load: 10%;  memory load: 128 MB\\n   \\n    The first chaos experiment simulates low CPU usage. To display when a chaos experiment starts and ends, click the switching button on the right side of the dashboard. To learn whether the experiment is Applied to the system or Recovered from the system, move your cursor onto the short, green line. \\n    \\n    During the time period between the two short, green lines, the service load decreases to 4,929 CPM, but returns to normal after the chaos experiment ends. \\n    \\n    ![Test 1](/img/cpuload-1.png)\\n\\n* CPU load: 50%; memory load: 128 MB\\n    \\n    When the application\u2019s CPU load increases to 50%,  the service load decreases to 4,307 CPM.\\n    \\n    ![Test 2](/img/cpuload-2.png)\\n\\n* CPU load: 100%; memory load: 128 MB\\n\\n    When the CPU usage is at 100%, the service load decreases to only 40% of what it would be if no chaos experiments were taking place. \\n    \\n    ![Test 3](/img/cpuload-3.png)\\n    \\n    Because the process scheduling under the Linux system does not allow a process to occupy the CPU all the time, the deployed Spring Boot Demo can still handle 40% of the access requests even in the extreme case of a full CPU load.\\n\\n## Summary\\n\\nBy combining SkyWalking and Chaos Mesh, you can clearly observe when and to what extent chaos experiments affect application service performance. This combination of tools lets you observe the service performance in various extreme conditions, thus boosting your confidence in your services. \\n\\nChaos Mesh has grown a lot in 2021 thanks to the unremitting efforts of all PingCAP engineers and community contributors. In order to continue to upgrade our support for our wide variety of users and learn more about users\u2019 experience in Chaos Engineering, we\u2019d like to invite you to take[ this survey](https://www.surveymonkey.com/r/X77BCNM) and give us your valuable feedback. \\n\\nIf you want to know more about Chaos Mesh, you\u2019re welcome to join [the Chaos Mesh community on GitHub](https://github.com/chaos-mesh) or our [Slack discussions](https://slack.cncf.io/) (#project-chaos-mesh). If you find any bugs or missing features when using Chaos Mesh, you can submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/implement-chaos-engineering-in-k8s","metadata":{"permalink":"/zh/blog/implement-chaos-engineering-in-k8s","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-12-10-implement-chaos-engineering-in-k8s.md","source":"@site/blog/2021-12-10-implement-chaos-engineering-in-k8s.md","title":"Implementing Chaos Engineering in K8s: Chaos Mesh Principle Analysis and Control Plane Development","description":"Implementing Chaos Engineering in K8s","date":"2021-12-10T00:00:00.000Z","formattedDate":"2021\u5e7412\u670810\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":17.915,"truncated":true,"authors":[{"name":"Mayo Cream","title":"Kubernetes Member, CNCF Security TAG Member, OSS Contributor","url":"https://github.com/mayocream","imageURL":"https://avatars.githubusercontent.com/u/35420264?v=4"}],"prevItem":{"title":"Chaos Mesh + SkyWalking: Better Observability for Chaos Engineering","permalink":"/zh/blog/better-observability-for-chaos-engineering"},"nextItem":{"title":"Hacktoberfest 2021: hack with Chaos Mesh!","permalink":"/zh/blog/chaos-mesh-hacktoberfest-2021"}},"content":"![Implementing Chaos Engineering in K8s](/img/implement-chaos-engineering-in-k8s.png)\\n\\n[Chaos Mesh](https://chaos-mesh.org/docs/) is an open-source, cloud-native Chaos Engineering platform built on Kubernetes (K8s) custom resource definitions (CRDs). Chaos Mesh can simulate various types of faults and has an enormous capability to orchestrate fault scenarios. You can use Chaos Mesh to conveniently simulate various abnormalities that might occur in development, testing, and production environments and find potential problems in the system.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this article, I\'ll explore the practice of Chaos Engineering in Kubernetes clusters, discuss important Chaos Mesh features through analysis of its source code, and explain how to develop Chaos Mesh\'s control plane with code examples.\\n\\nIf you\'re not familiar with Chaos Mesh, please review the [Chaos Mesh documentation](https://chaos-mesh.org/docs/#architecture-overview) to get a basic knowledge of Chaos Mesh\'s architecture.\\n\\nFor the test code in this article, see the [mayocream/chaos-mesh-controlpanel-demo](https://github.com/mayocream/chaos-mesh-controlpanel-demo) repository on GitHub.\\n\\n## How Chaos Mesh creates chaos\\n\\nChaos Mesh is a Swiss army knife for implementing Chaos Engineering on Kubernetes. This section introduces how it works.\\n\\n### Privileged mode\\n\\nChaos Mesh runs privileged containers in Kubernetes to create failures. Chaos Daemon\'s Pod runs as `DaemonSet` and adds additional [capabilities](https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities) to the Pod\'s container runtime via the Pod\'s security context.\\n\\n```yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nspec:\\n template:\\n   metadata: ...\\n   spec:\\n     containers:\\n       - name: chaos-daemon\\n         securityContext:\\n           {{- if .Values.chaosDaemon.privileged }}\\n           privileged: true\\n           capabilities:\\n             add:\\n               - SYS_PTRACE\\n           {{- else }}\\n           capabilities:\\n             add:\\n               - SYS_PTRACE\\n               - NET_ADMIN\\n               - MKNOD\\n               - SYS_CHROOT\\n               - SYS_ADMIN\\n               - KILL\\n               # CAP_IPC_LOCK is used to lock memory\\n               - IPC_LOCK\\n           {{- end }}\\n```\\n\\nThe Linux capabilities grant containers privileges to create and access the `/dev/fuse` Filesystem in Userspace (FUSE) pipe. FUSE is the Linux userspace filesystem interface. It lets non-privileged users create their own file systems without editing the kernel code.\\n\\nAccording to [pull request #1109](https://github.com/chaos-mesh/chaos-mesh/pull/1109) on GitHub, the `DaemonSet` program uses cgo to call the Linux `makedev` function to create a FUSE pipe.\\n\\n```go\\n// #include <sys/sysmacros.h>\\n// #include <sys/types.h>\\n// // makedev is a macro, so a wrapper is needed\\n// dev_t Makedev(unsigned int maj, unsigned int min) {\\n//   return makedev(maj, min);\\n// }\\n// EnsureFuseDev ensures /dev/fuse exists. If not, it will create one\\n\\nfunc EnsureFuseDev() {\\n   if _, err := os.Open(\\"/dev/fuse\\"); os.IsNotExist(err) {\\n       // 10, 229 according to https://www.kernel.org/doc/Documentation/admin-guide/devices.txt\\n       fuse := C.Makedev(10, 229)\\n       syscall.Mknod(\\"/dev/fuse\\", 0o666|syscall.S_IFCHR, int(fuse))\\n   }\\n}\\n```\\n\\nIn [pull request #1453](https://github.com/chaos-mesh/chaos-mesh/pull/1453), Chaos Daemon enables privileged mode by default; that is, it sets `privileged: true` in the container\'s `SecurityContext`.\\n\\n### Killing Pods\\n\\n`PodKill`, `PodFailure`, and `ContainerKill` belong to the `PodChaos` category. `PodKill` randomly kills a Pod. It calls the API server to send the kill command.\\n\\n```go\\nimport (\\n   \\"context\\"\\n   v1 \\"k8s.io/api/core/v1\\"\\n   \\"sigs.k8s.io/controller-runtime/pkg/client\\"\\n)\\n\\ntype Impl struct {\\n   client.Client\\n}\\n\\nfunc (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {\\n   ...\\n   err = impl.Get(ctx, namespacedName, &pod)\\n   if err != nil {\\n       // TODO: handle this error\\n       return v1alpha1.NotInjected, err\\n   }\\n   err = impl.Delete(ctx, &pod, &client.DeleteOptions{\\n       GracePeriodSeconds: &podchaos.Spec.GracePeriod, // PeriodSeconds has to be set specifically\\n   })\\n   ...\\n   return v1alpha1.Injected, nil\\n}\\n```\\n\\nThe `GracePeriodSeconds` parameter lets Kubernetes [forcibly terminate a Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced). For example, if you need to delete a Pod immediately, use the `kubectl delete pod --grace-period=0 --force` command.\\n\\n`PodFailure` patches the Pod object resource to replace the image in the Pod with a wrong one. Chaos only modifies the `image` fields of `containers` and `initContainers`. This is because most of the metadata about a Pod is immutable. For more details, see [Pod update and replacement](https://kubernetes.io/docs/concepts/workloads/pods/#pod-update-and-replacement).\\n\\n```go\\nfunc (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {\\n   ...\\n   pod := origin.DeepCopy()\\n   for index := range pod.Spec.Containers {\\n       originImage := pod.Spec.Containers[index].Image\\n       name := pod.Spec.Containers[index].Name\\n       key := annotation.GenKeyForImage(podchaos, name, false)\\n       if pod.Annotations == nil {\\n           pod.Annotations = make(map[string]string)\\n       }\\n       // If the annotation is already existed, we could skip the reconcile for this container\\n       if _, ok := pod.Annotations[key]; ok {\\n           continue\\n       }\\n       pod.Annotations[key] = originImage\\n       pod.Spec.Containers[index].Image = config.ControllerCfg.PodFailurePauseImage\\n   }\\n   for index := range pod.Spec.InitContainers {\\n       originImage := pod.Spec.InitContainers[index].Image\\n       name := pod.Spec.InitContainers[index].Name\\n       key := annotation.GenKeyForImage(podchaos, name, true)\\n       if pod.Annotations == nil {\\n           pod.Annotations = make(map[string]string)\\n       }\\n       // If the annotation is already existed, we could skip the reconcile for this container\\n       if _, ok := pod.Annotations[key]; ok {\\n           continue\\n       }\\n       pod.Annotations[key] = originImage\\n       pod.Spec.InitContainers[index].Image = config.ControllerCfg.PodFailurePauseImage\\n   }\\n   err = impl.Patch(ctx, pod, client.MergeFrom(&origin))\\n   if err != nil {\\n       // TODO: handle this error\\n       return v1alpha1.NotInjected, err\\n   }\\n   return v1alpha1.Injected, nil\\n}\\n```\\n\\nThe default container image that causes failures is `gcr.io/google-containers/pause:latest`.\\n\\n`PodKill` and `PodFailure` control the Pod lifecycle through the Kubernetes API server. But `ContainerKill` does this through Chaos Daemon that runs on the cluster node. `ContainerKill` uses Chaos Controller Manager to run the client to initiate gRPC calls to Chaos Daemon.\\n\\n```go\\nfunc (b *ChaosDaemonClientBuilder) Build(ctx context.Context, pod *v1.Pod) (chaosdaemonclient.ChaosDaemonClientInterface, error) {\\n   ...\\n   daemonIP, err := b.FindDaemonIP(ctx, pod)\\n   if err != nil {\\n       return nil, err\\n   }\\n   builder := grpcUtils.Builder(daemonIP, config.ControllerCfg.ChaosDaemonPort).WithDefaultTimeout()\\n   if config.ControllerCfg.TLSConfig.ChaosMeshCACert != \\"\\" {\\n       builder.TLSFromFile(config.ControllerCfg.TLSConfig.ChaosMeshCACert, config.ControllerCfg.TLSConfig.ChaosDaemonClientCert, config.ControllerCfg.TLSConfig.ChaosDaemonClientKey)\\n   } else {\\n       builder.Insecure()\\n   }\\n   cc, err := builder.Build()\\n   if err != nil {\\n       return nil, err\\n   }\\n   return chaosdaemonclient.New(cc), nil\\n}\\n```\\n\\nWhen Chaos Controller Manager sends commands to Chaos Daemon, it creates a corresponding client based on the Pod information. For example, to control a Pod on a node, it creates a client by getting the `ClusterIP` of the node where the Pod is located. If the Transport Layer Security (TLS) certificate configuration exists, Controller Manager adds the TLS certificate for the client.\\n\\nWhen Chaos Daemon starts, if it has a TLS certificate it attaches the certificate to enable gRPCS. The TLS configuration option `RequireAndVerifyClientCert` indicates whether to enable mutual TLS (mTLS) authentication.\\n\\n```go\\nfunc newGRPCServer(containerRuntime string, reg prometheus.Registerer, tlsConf tlsConfig) (*grpc.Server, error) {\\n   ...\\n   if tlsConf != (tlsConfig{}) {\\n       caCert, err := ioutil.ReadFile(tlsConf.CaCert)\\n       if err != nil {\\n           return nil, err\\n       }\\n       caCertPool := x509.NewCertPool()\\n       caCertPool.AppendCertsFromPEM(caCert)\\n       serverCert, err := tls.LoadX509KeyPair(tlsConf.Cert, tlsConf.Key)\\n       if err != nil {\\n           return nil, err\\n       }\\n       creds := credentials.NewTLS(&tls.Config{\\n           Certificates: []tls.Certificate{serverCert},\\n           ClientCAs:    caCertPool,\\n           ClientAuth:   tls.RequireAndVerifyClientCert,\\n       })\\n       grpcOpts = append(grpcOpts, grpc.Creds(creds))\\n   }\\n   s := grpc.NewServer(grpcOpts...)\\n   grpcMetrics.InitializeMetrics(s)\\n   pb.RegisterChaosDaemonServer(s, ds)\\n   reflection.Register(s)\\n   return s, nil\\n}\\n```\\n\\nChaos Daemon provides the following gRPC interfaces to call:\\n\\n```go\\n// ChaosDaemonClient is the client API for ChaosDaemon service.\\n//\\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\\n\\ntype ChaosDaemonClient interface {\\n\\n   SetTcs(ctx context.Context, in *TcsRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n   FlushIPSets(ctx context.Context, in *IPSetsRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n   SetIptablesChains(ctx context.Context, in *IptablesChainsRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n   SetTimeOffset(ctx context.Context, in *TimeRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n   RecoverTimeOffset(ctx context.Context, in *TimeRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n   ContainerKill(ctx context.Context, in *ContainerRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n   ContainerGetPid(ctx context.Context, in *ContainerRequest, opts ...grpc.CallOption) (*ContainerResponse, error)\\n   ExecStressors(ctx context.Context, in *ExecStressRequest, opts ...grpc.CallOption) (*ExecStressResponse, error)\\n   CancelStressors(ctx context.Context, in *CancelStressRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n   ApplyIOChaos(ctx context.Context, in *ApplyIOChaosRequest, opts ...grpc.CallOption) (*ApplyIOChaosResponse, error)\\n   ApplyHttpChaos(ctx context.Context, in *ApplyHttpChaosRequest, opts ...grpc.CallOption) (*ApplyHttpChaosResponse, error)\\n   SetDNSServer(ctx context.Context, in *SetDNSServerRequest, opts ...grpc.CallOption) (*empty.Empty, error)\\n}\\n```\\n\\n### Network failure injection\\n\\nFrom [pull request #41](https://github.com/chaos-mesh/chaos-mesh/pull/41), we know that Chaos Mesh injects network failures this way: it calls `pbClient.SetNetem` to encapsulate parameters into a request and send the request to the Chaos Daemon on the node for processing.\\n\\nThe network failure injection code is shown below as it appeared in 2019. As the project developed, the functions were distributed among several files.\\n\\n```go\\nfunc (r *Reconciler) applyPod(ctx context.Context, pod *v1.Pod, networkchaos *v1alpha1.NetworkChaos) error {\\n   ...\\n   pbClient := pb.NewChaosDaemonClient(c)\\n   containerId := pod.Status.ContainerStatuses[0].ContainerID\\n   netem, err := spec.ToNetem()\\n   if err != nil {\\n       return err\\n   }\\n   _, err = pbClient.SetNetem(ctx, &pb.NetemRequest{\\n       ContainerId: containerId,\\n       Netem:       netem,\\n   })\\n   return err\\n}\\n```\\n\\nIn the `pkg/chaosdaemon` package, we can see how Chaos Daemon processes requests.\\n\\n```go\\nfunc (s *Server) SetNetem(ctx context.Context, in *pb.NetemRequest) (*empty.Empty, error) {\\n   log.Info(\\"Set netem\\", \\"Request\\", in)\\n   pid, err := s.crClient.GetPidFromContainerID(ctx, in.ContainerId)\\n   if err != nil {\\n       return nil, status.Errorf(codes.Internal, \\"get pid from containerID error: %v\\", err)\\n   }\\n   if err := Apply(in.Netem, pid); err != nil {\\n       return nil, status.Errorf(codes.Internal, \\"netem apply error: %v\\", err)\\n   }\\n   return &empty.Empty{}, nil\\n}\\n\\n// Apply applies a netem on eth0 in pid related namespace\\n\\nfunc Apply(netem *pb.Netem, pid uint32) error {\\n   log.Info(\\"Apply netem on PID\\", \\"pid\\", pid)\\n   ns, err := netns.GetFromPath(GenNetnsPath(pid))\\n   if err != nil {\\n       log.Error(err, \\"failed to find network namespace\\", \\"pid\\", pid)\\n       return errors.Trace(err)\\n   }\\n   defer ns.Close()\\n   handle, err := netlink.NewHandleAt(ns)\\n   if err != nil {\\n       log.Error(err, \\"failed to get handle at network namespace\\", \\"network namespace\\", ns)\\n       return err\\n   }\\n   link, err := handle.LinkByName(\\"eth0\\") // TODO: check whether interface name is eth0\\n   if err != nil {\\n       log.Error(err, \\"failed to find eth0 interface\\")\\n       return errors.Trace(err)\\n   }\\n   netemQdisc := netlink.NewNetem(netlink.QdiscAttrs{\\n       LinkIndex: link.Attrs().Index,\\n       Handle:    netlink.MakeHandle(1, 0),\\n       Parent:    netlink.HANDLE_ROOT,\\n   }, ToNetlinkNetemAttrs(netem))\\n   if err = handle.QdiscAdd(netemQdisc); err != nil {\\n       if !strings.Contains(err.Error(), \\"file exists\\") {\\n           log.Error(err, \\"failed to add Qdisc\\")\\n           return errors.Trace(err)\\n       }\\n   }\\n   return nil\\n}\\n```\\n\\nFinally, the [`vishvananda/netlink` library](https://github.com/vishvananda/netlink) operates the Linux network interface to complete the job.\\n\\nFrom here, `NetworkChaos` manipulates the Linux host network to create chaos. It includes tools such as iptables and ipset.\\n\\nIn Chaos Daemon\'s Dockerfile, you can see the Linux tool chain that it depends on:\\n\\n```dockerfile\\nRUN apt-get update && \\\\\\n   apt-get install -y tzdata iptables ipset stress-ng iproute2 fuse util-linux procps curl && \\\\\\n   rm -rf /var/lib/apt/lists/*\\n```\\n\\n### Stress test\\n\\nChaos Daemon also implements `StressChaos`. After the Controller Manager calculates the rules, it sends the task to the specific `Daemon`. The assembled parameters are shown below. They are combined into command execution parameters and appended to the `stress-ng` command for execution.\\n\\n```go\\n// Normalize the stressors to comply with stress-ng\\nfunc (in *Stressors) Normalize() (string, error) {\\n   stressors := \\"\\"\\n   if in.MemoryStressor != nil && in.MemoryStressor.Workers != 0 {\\n       stressors += fmt.Sprintf(\\" --vm %d --vm-keep\\", in.MemoryStressor.Workers)\\n       if len(in.MemoryStressor.Size) != 0 {\\n           if in.MemoryStressor.Size[len(in.MemoryStressor.Size)-1] != \'%\' {\\n               size, err := units.FromHumanSize(string(in.MemoryStressor.Size))\\n               if err != nil {\\n                   return \\"\\", err\\n               }\\n               stressors += fmt.Sprintf(\\" --vm-bytes %d\\", size)\\n           } else {\\n               stressors += fmt.Sprintf(\\" --vm-bytes %s\\",\\n                   in.MemoryStressor.Size)\\n           }\\n       }\\n       if in.MemoryStressor.Options != nil {\\n           for _, v := range in.MemoryStressor.Options {\\n               stressors += fmt.Sprintf(\\" %v \\", v)\\n           }\\n       }\\n   }\\n   if in.CPUStressor != nil && in.CPUStressor.Workers != 0 {\\n       stressors += fmt.Sprintf(\\" --cpu %d\\", in.CPUStressor.Workers)\\n       if in.CPUStressor.Load != nil {\\n           stressors += fmt.Sprintf(\\" --cpu-load %d\\",\\n               *in.CPUStressor.Load)\\n       }\\n       if in.CPUStressor.Options != nil {\\n           for _, v := range in.CPUStressor.Options {\\n               stressors += fmt.Sprintf(\\" %v \\", v)\\n           }\\n       }\\n   }\\n   return stressors, nil\\n}\\n```\\n\\nThe Chaos Daemon server side processes the function\'s execution command to call the official Go package `os/exec`. For details, see the [`pkg/chaosdaemon/stress_server_linux.go`](https://github.com/chaos-mesh/chaos-mesh/blob/98af3a0e7832a4971d6b133a32069539d982ef0a/pkg/chaosdaemon/stress_server_linux.go#L33) file. There is also a file with the same name that ends with darwin. `*_darwin` files prevent possible errors when the program is running on macOS.\\n\\nThe code uses the [`shirou/gopsutil`](https://github.com/shirou/gopsutil) package to obtain the PID process status and reads the stdout and stderr standard outputs. I\'ve seen this processing mode in [`hashicorp/go-plugin`](https://github.com/hashicorp/go-plugin), and go-plugin does this better.\\n\\n### I/O fault injection\\n\\n[Pull request #826](https://github.com/chaos-mesh/chaos-mesh/pull/826) introduces a new implementation of IOChaos, without the use of sidecar injection. It uses Chaos Daemon to directly manipulate the Linux namespace through the underlying commands of the [runc](https://github.com/opencontainers/runc) container and runs the [chaos-mesh/toda](https://github.com/chaos-mesh/toda) FUSE program developed by Rust to inject container I/O chaos. The [JSON-RPC 2.0](https://pkg.go.dev/github.com/ethereum/go-ethereum/rpc) protocol is used to communicate between toda and the control plane.\\n\\nThe new IOChaos implementation doesn\'t modify the Pod resources. When you define the IOChaos chaos experiment, for each Pod filtered by the selector field, a corresponding PodIOChaos resource is created. PodIoChaos\' [owner reference](https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/) is the Pod. At the same time, a set of [finalizers](https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/) is added to PodIoChaos to release PodIoChaos resources before PodIoChaos is deleted.\\n\\n```go\\n// Apply implements the reconciler.InnerReconciler.Apply\\n\\nfunc (r *Reconciler) Apply(ctx context.Context, req ctrl.Request, chaos v1alpha1.InnerObject) error {\\n   iochaos, ok := chaos.(*v1alpha1.IoChaos)\\n   if !ok {\\n       err := errors.New(\\"chaos is not IoChaos\\")\\n       r.Log.Error(err, \\"chaos is not IoChaos\\", \\"chaos\\", chaos)\\n       return err\\n   }\\n   source := iochaos.Namespace + \\"/\\" + iochaos.Name\\n   m := podiochaosmanager.New(source, r.Log, r.Client)\\n   pods, err := utils.SelectAndFilterPods(ctx, r.Client, r.Reader, &iochaos.Spec)\\n   if err != nil {\\n       r.Log.Error(err, \\"failed to select and filter pods\\")\\n       return err\\n   }\\n   r.Log.Info(\\"applying iochaos\\", \\"iochaos\\", iochaos)\\n   for _, pod := range pods {\\n       t := m.WithInit(types.NamespacedName{\\n           Name:      pod.Name,\\n           Namespace: pod.Namespace,\\n       })\\n\\n       // TODO: support chaos on multiple volume\\n\\n       t.SetVolumePath(iochaos.Spec.VolumePath)\\n       t.Append(v1alpha1.IoChaosAction{\\n           Type: iochaos.Spec.Action,\\n           Filter: v1alpha1.Filter{\\n               Path:    iochaos.Spec.Path,\\n               Percent: iochaos.Spec.Percent,\\n               Methods: iochaos.Spec.Methods,\\n           },\\n           Faults: []v1alpha1.IoFault{\\n               {\\n                   Errno:  iochaos.Spec.Errno,\\n                   Weight: 1,\\n               },\\n           },\\n           Latency:          iochaos.Spec.Delay,\\n           AttrOverrideSpec: iochaos.Spec.Attr,\\n           Source:           m.Source,\\n       })\\n       key, err := cache.MetaNamespaceKeyFunc(&pod)\\n       if err != nil {\\n           return err\\n       }\\n       iochaos.Finalizers = utils.InsertFinalizer(iochaos.Finalizers, key)\\n   }\\n   r.Log.Info(\\"commiting updates of podiochaos\\")\\n   err = m.Commit(ctx)\\n   if err != nil {\\n       r.Log.Error(err, \\"fail to commit\\")\\n       return err\\n   }\\n   r.Event(iochaos, v1.EventTypeNormal, utils.EventChaosInjected, \\"\\")\\n   return nil\\n}\\n```\\n\\nIn the controller of the PodIoChaos resource, Controller Manager encapsulates the resource into parameters and calls the Chaos Daemon interface to process the parameters.\\n\\n```go\\n// Apply flushes io configuration on pod\\n\\nfunc (h *Handler) Apply(ctx context.Context, chaos *v1alpha1.PodIoChaos) error {\\n   h.Log.Info(\\"updating io chaos\\", \\"pod\\", chaos.Namespace+\\"/\\"+chaos.Name, \\"spec\\", chaos.Spec)\\n   ...\\n   res, err := pbClient.ApplyIoChaos(ctx, &pb.ApplyIoChaosRequest{\\n       Actions:     input,\\n       Volume:      chaos.Spec.VolumeMountPath,\\n       ContainerId: containerID,\\n       Instance:  chaos.Spec.Pid,\\n       StartTime: chaos.Spec.StartTime,\\n   })\\n   if err != nil {\\n       return err\\n   }\\n   chaos.Spec.Pid = res.Instance\\n   chaos.Spec.StartTime = res.StartTime\\n   chaos.OwnerReferences = []metav1.OwnerReference{\\n       {\\n           APIVersion: pod.APIVersion,\\n           Kind:       pod.Kind,\\n           Name:       pod.Name,\\n           UID:        pod.UID,\\n       },\\n   }\\n   return nil\\n}\\n```\\n\\nThe `pkg/chaosdaemon/iochaos_server.go` file processes IOChaos. \u200b\u200bIn this file, a FUSE program needs to be injected into the container. As discussed in issue [#2305](https://github.com/chaos-mesh/chaos-mesh/issues/2305) on GitHub, the `/usr/local/bin/nsexec -l- p /proc/119186/ns/pid -m /proc/119186/ns/mnt - /usr/local/bin/toda --path /tmp --verbose info` command is executed to run the toda program under the same namespace as the Pod.\\n\\n```go\\nfunc (s *DaemonServer) ApplyIOChaos(ctx context.Context, in *pb.ApplyIOChaosRequest) (*pb.ApplyIOChaosResponse, error) {\\n   ...\\n   pid, err := s.crClient.GetPidFromContainerID(ctx, in.ContainerId)\\n   if err != nil {\\n       log.Error(err, \\"error while getting PID\\")\\n       return nil, err\\n   }\\n   args := fmt.Sprintf(\\"--path %s --verbose info\\", in.Volume)\\n   log.Info(\\"executing\\", \\"cmd\\", todaBin+\\" \\"+args)\\n   processBuilder := bpm.DefaultProcessBuilder(todaBin, strings.Split(args, \\" \\")...).\\n       EnableLocalMnt().\\n       SetIdentifier(in.ContainerId)\\n   if in.EnterNS {\\n       processBuilder = processBuilder.SetNS(pid, bpm.MountNS).SetNS(pid, bpm.PidNS)\\n   }\\n   ...\\n\\n   // Calls JSON RPC\\n\\n   client, err := jrpc.DialIO(ctx, receiver, caller)\\n   if err != nil {\\n       return nil, err\\n   }\\n   cmd := processBuilder.Build()\\n   procState, err := s.backgroundProcessManager.StartProcess(cmd)\\n   if err != nil {\\n       return nil, err\\n   }\\n   ...\\n}\\n```\\n\\nThe following code sample builds the running commands. These commands are the underlying namespace isolation implementation of runc:\\n\\n```go\\n// GetNsPath returns corresponding namespace path\\n\\nfunc GetNsPath(pid uint32, typ NsType) string {\\n   return fmt.Sprintf(\\"%s/%d/ns/%s\\", DefaultProcPrefix, pid, string(typ))\\n}\\n\\n// SetNS sets the namespace of the process\\n\\nfunc (b *ProcessBuilder) SetNS(pid uint32, typ NsType) *ProcessBuilder {\\n   return b.SetNSOpt([]nsOption{{\\n       Typ:  typ,\\n       Path: GetNsPath(pid, typ),\\n   }})\\n}\\n\\n// Build builds the process\\n\\nfunc (b *ProcessBuilder) Build() *ManagedProcess {\\n   args := b.args\\n   cmd := b.cmd\\n   if len(b.nsOptions) > 0 {\\n       args = append([]string{\\"--\\", cmd}, args...)\\n       for _, option := range b.nsOptions {\\n           args = append([]string{\\"-\\" + nsArgMap[option.Typ], option.Path}, args...)\\n       }\\n       if b.localMnt {\\n           args = append([]string{\\"-l\\"}, args...)\\n       }\\n       cmd = nsexecPath\\n   }\\n   ...\\n}\\n```\\n\\n## Control plane\\n\\nChaos Mesh is an open-source chaos engineering system under the Apache 2.0 protocol. As discussed above, it has rich capabilities and a good ecosystem. The maintenance team developed the [`chaos-mesh/toda`](https://github.com/chaos-mesh/toda) FUSE based on the chaos system, the [`chaos-mesh/k8s_dns_chaos`](https://github.com/chaos-mesh/k8s_dns_chaos) CoreDNS chaos plug-in, and Berkeley Packet Filter (BPF)-based kernel error injection [`chaos-mesh/bpfki`](https://github.com/chaos-mesh/bpfki).\\n\\nNow, I\'ll describe the server side code required to build an end-user-oriented chaos engineering platform. This implementation is only an example\u2014not necessarily the best example. If you want to see the development practice on a real world platform, you can refer to Chaos Mesh\'s [Dashboard](https://github.com/chaos-mesh/chaos-mesh/tree/master/pkg/dashboard). It uses the [`uber-go/fx`](https://github.com/uber-go/fx) dependency injection framework and the controller runtime\'s manager mode.\\n\\n### Key Chaos Mesh features\\n\\nAs shown in the Chaos Mesh workflow below, we need to implement a server that sends YAML to the Kubernetes API. Chaos Controller Manager implements complex rule verification and rule delivery to Chaos Daemon. If you want to use Chaos Mesh with your own platform, you only need to connect to the process of creating CRD resources.\\n\\n![Chaos Mesh\'s basic workflow](/img/chaos-mesh-basic-workflow.png)\\n\\n<p className=\\"caption-center\\">Chaos Mesh\'s basic workflow</p>\\n\\nLet\'s take a look at the example on the Chaos Mesh website:\\n\\n```go\\nimport (\\n   \\"context\\"\\n   \\"github.com/pingcap/chaos-mesh/api/v1alpha1\\"\\n   \\"sigs.k8s.io/controller-runtime/pkg/client\\"\\n)\\n\\nfunc main() {\\n   ...\\n   delay := &chaosv1alpha1.NetworkChaos{\\n       Spec: chaosv1alpha1.NetworkChaosSpec{...},\\n   }\\n   k8sClient := client.New(conf, client.Options{ Scheme: scheme.Scheme })\\n   k8sClient.Create(context.TODO(), delay)\\n   k8sClient.Delete(context.TODO(), delay)\\n}\\n```\\n\\nChaos Mesh provides APIs corresponding to all CRDs. We use the [controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) developed by Kubernetes [API Machinery SIG](https://github.com/kubernetes/community/tree/master/sig-api-machinery) to simplify the interaction with the Kubernetes API.\\n\\n### Inject chaos\\n\\nSuppose we want to create a `PodKill` resource by calling a program. After the resource is sent to the Kubernetes API server, it passes Chaos Controller Manager\'s [validating admission controller](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) to verify data. When we create a chaos experiment, if the admission controller fails to verify the input data, it returns an error to the client. For specific parameters, you can read [Create experiments using YAML configuration files](https://chaos-mesh.org/docs/simulate-pod-chaos-on-kubernetes/#create-experiments-using-yaml-configuration-files).\\n\\n`NewClient` creates a Kubernetes API client. You can refer to this example:\\n\\n```go\\npackage main\\n\\nimport (\\n   \\"context\\"\\n   \\"controlpanel\\"\\n   \\"log\\"\\n   \\"github.com/chaos-mesh/chaos-mesh/api/v1alpha1\\"\\n   \\"github.com/pkg/errors\\"\\n   metav1 \\"k8s.io/apimachinery/pkg/apis/meta/v1\\"\\n)\\n\\nfunc applyPodKill(name, namespace string, labels map[string]string) error {\\n   cli, err := controlpanel.NewClient()\\n   if err != nil {\\n       return errors.Wrap(err, \\"create client\\")\\n   }\\n   cr := &v1alpha1.PodChaos{\\n       ObjectMeta: metav1.ObjectMeta{\\n           GenerateName: name,\\n           Namespace:    namespace,\\n       },\\n       Spec: v1alpha1.PodChaosSpec{\\n           Action: v1alpha1.PodKillAction,\\n           ContainerSelector: v1alpha1.ContainerSelector{\\n               PodSelector: v1alpha1.PodSelector{\\n                   Mode: v1alpha1.OnePodMode,\\n                   Selector: v1alpha1.PodSelectorSpec{\\n                       Namespaces:     []string{namespace},\\n                       LabelSelectors: labels,\\n                   },\\n               },\\n           },\\n       },\\n   }\\n\\n   if err := cli.Create(context.Background(), cr); err != nil {\\n       return errors.Wrap(err, \\"create podkill\\")\\n   }\\n   return nil\\n}\\n```\\n\\nThe log output of the running program is:\\n\\n```bash\\nI1021 00:51:55.225502   23781 request.go:665] Waited for 1.033116256s due to client-side throttling, not priority and fairness, request: GET:https://***\\n2021/10/21 00:51:56 apply podkill\\n```\\n\\nUse kubectl to check the status of the `PodKill` resource:\\n\\n```bash\\n$ k describe podchaos.chaos-mesh.org -n dev podkillvjn77\\nName:         podkillvjn77\\nNamespace:    dev\\nLabels:       <none>\\nAnnotations:  <none>\\nAPI Version:  chaos-mesh.org/v1alpha1\\nKind:         PodChaos\\n\\nMetadata:\\n Creation Timestamp:  2021-10-20T16:51:56Z\\n Finalizers:\\n   chaos-mesh/records\\n Generate Name:     podkill\\n Generation:        7\\n Resource Version:  938921488\\n Self Link:         /apis/chaos-mesh.org/v1alpha1/namespaces/dev/podchaos/podkillvjn77\\n UID:               afbb40b3-ade8-48ba-89db-04918d89fd0b\\n\\nSpec:\\n Action:        pod-kill\\n Grace Period:  0\\n Mode:          one\\n Selector:\\n   Label Selectors:\\n     app:  nginx\\n   Namespaces:\\n     dev\\n\\nStatus:\\n Conditions:\\n   Reason:\\n   Status:  False\\n   Type:    Paused\\n   Reason:\\n   Status:  True\\n   Type:    Selected\\n   Reason:\\n   Status:  True\\n   Type:    AllInjected\\n   Reason:\\n   Status:  False\\n   Type:    AllRecovered\\n\\n Experiment:\\n   Container Records:\\n     Id:            dev/nginx\\n     Phase:         Injected\\n     Selector Key:  .\\n   Desired Phase:   Run\\n\\nEvents:\\n Type    Reason           Age    From          Message\\n ----    ------           ----   ----          -------\\n Normal  FinalizerInited  6m35s  finalizer     Finalizer has been inited\\n Normal  Updated          6m35s  finalizer     Successfully update finalizer of resource\\n Normal  Updated          6m35s  records       Successfully update records of resource\\n Normal  Updated          6m35s  desiredphase  Successfully update desiredPhase of resource\\n Normal  Applied          6m35s  records       Successfully apply chaos for dev/nginx\\n Normal  Updated          6m35s  records       Successfully update records of resource\\n```\\n\\nThe control plane also needs to query and acquire Chaos resources, so that platform users can view all chaos experiments\' implementation status and manage them. To achieve this, we can call the `REST` API to send the `Get` or `List` request. But in practice, we need to pay attention to the details. At our company, we\'ve noticed that each time the controller requests the full amount of resource data, the load of the Kubernetes API server increases.\\n\\nI recommend that you read the [How to use the controller-runtime client](https://zoetrope.github.io/kubebuilder-training/controller-runtime/client.html) (in Japanese) controller runtime tutorial. If you don\'t understand Japanese, you can still learn a lot from the tutorial by reading the source code. It covers many details. For example, by default, the controller runtime reads kubeconfig, flags, environment variables, and the service account automatically mounted in the Pod from multiple locations. [Pull request #21](https://github.com/armosec/kubescape/pull/21) for [`armosec/kubescape`](https://github.com/armosec/kubescape) uses this feature. This tutorial also includes common operations, such as how to paginate, update, and overwrite objects. I haven\'t seen any English tutorials that are so detailed.\\n\\nHere are examples of `Get` and `List` requests:\\n\\n```go\\npackage controlpanel\\n\\nimport (\\n   \\"context\\"\\n   \\"github.com/chaos-mesh/chaos-mesh/api/v1alpha1\\"\\n   \\"github.com/pkg/errors\\"\\n   \\"sigs.k8s.io/controller-runtime/pkg/client\\"\\n)\\n\\nfunc GetPodChaos(name, namespace string) (*v1alpha1.PodChaos, error) {\\n   cli := mgr.GetClient()\\n   item := new(v1alpha1.PodChaos)\\n   if err := cli.Get(context.Background(), client.ObjectKey{Name: name, Namespace: namespace}, item); err != nil {\\n       return nil, errors.Wrap(err, \\"get cr\\")\\n   }\\n   return item, nil\\n}\\n\\nfunc ListPodChaos(namespace string, labels map[string]string) ([]v1alpha1.PodChaos, error) {\\n   cli := mgr.GetClient()\\n   list := new(v1alpha1.PodChaosList)\\n   if err := cli.List(context.Background(), list, client.InNamespace(namespace), client.MatchingLabels(labels)); err != nil {\\n       return nil, err\\n   }\\n   return list.Items, nil\\n}\\n```\\n\\nThis example uses the manager. This mode prevents the cache mechanism from repetitively fetching large amounts of data. The following [figure](https://zoetrope.github.io/kubebuilder-training/controller-runtime/client.html) shows the workflow:\\n\\n1. Get the Pod.\\n\\n2. Get the `List` request\'s full data for the first time.\\n\\n3. Update the cache when the watch data changes.\\n\\n![List request](/img/list-request.png)\\n\\n<p className=\\"caption-center\\">List request</p>\\n\\n### Orchestrate chaos\\n\\nThe container runtime interface (CRI) container runtime provides strong underlying isolation capabilities that can support the stable operation of the container. But for more complex and scalable scenarios, container orchestration is required. Chaos Mesh also provides [`Schedule`](https://chaos-mesh.org/docs/define-scheduling-rules/) and [`Workflow`](https://chaos-mesh.org/docs/create-chaos-mesh-workflow/) features. Based on the set `Cron` time, `Schedule` can trigger faults regularly and at intervals. `Workflow` can schedule multiple fault tests like Argo Workflows.\\n\\nChaos Controller Manager does most of the work for us. The control plane mainly manages these YAML resources. You only need to consider the features you want to provide to end users.\\n\\n### Platform features\\n\\nThe following figure shows Chaos Mesh Dashboard. We need to consider what features the platform should provide to end users.\\n\\n![Chaos Mesh Dashboard](/img/chaos-mesh-dashboard-k8s.png)\\n\\n<p className=\\"caption-center\\">Chaos Mesh Dashboard</p>\\n\\nFrom the Dashboard, we know that the platform may have these features:\\n\\n- Chaos injection\\n- Pod crash\\n- Network failure\\n- Load test\\n- I/O failure\\n- Event tracking\\n- Associated alarm\\n- Timing telemetry\\n\\nIf you are interested in Chaos Mesh and would like to improve it, join its [Slack channel](https://slack.cncf.io/) (#project-chaos-mesh) or submit your pull requests or issues to its [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/chaos-mesh-hacktoberfest-2021","metadata":{"permalink":"/zh/blog/chaos-mesh-hacktoberfest-2021","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-09-28-chaos-mesh-hacktoberfest-2021.md","source":"@site/blog/2021-09-28-chaos-mesh-hacktoberfest-2021.md","title":"Hacktoberfest 2021: hack with Chaos Mesh!","description":"Chaos Mesh x Hacktoberfest 2021","date":"2021-09-28T00:00:00.000Z","formattedDate":"2021\u5e749\u670828\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"open source","permalink":"/zh/blog/tags/open-source"}],"readingTime":2.73,"truncated":true,"authors":[{"name":"Chaos Mesh Community","url":"https://github.com/chaos-mesh","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"Implementing Chaos Engineering in K8s: Chaos Mesh Principle Analysis and Control Plane Development","permalink":"/zh/blog/implement-chaos-engineering-in-k8s"},"nextItem":{"title":"How to run chaos experiments on your physical machine","permalink":"/zh/blog/run-chaos-experiments-on-physical-machines"}},"content":"![Chaos Mesh x Hacktoberfest 2021](/img/chaos-mesh-hacktoberfest-2021.png)\\n\\nHappy [Hacktoberfest](https://hacktoberfest.digitalocean.com/) 2021! We are excited to announce that [Chaos Mesh](https://github.com/chaos-mesh) will be participating in the 8th annual Hacktoberfest hosted by DigitalOcean. During the month of October, anyone is welcome to join in on this global celebration of open-source by contributing changes, and earn one of 55,000 custom-made Hacktoberfest T-shirts!\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Chaos Mesh?\\n\\nChaos Mesh is a cloud-native Chaos Engineering platform that orchestrates chaos in Kubernetes environments. With Chaos Mesh, you can test your system\'s resilience and robustness on Kubernetes by injecting all types of faults into Pods, network, file system, and even the kernel. Chaos Mesh is currently a CNCF Sandbox project.\\n\\nMore importantly, Chaos Mesh fully embraces open source: ever since open sourced 1.5 years ago, the project has gained more than 4k stars with over 1.2k commits from 140+ contributors all over the world. It is through the open-source world that we are able to collaborate with an amazing community. Simply put, Chaos Mesh grew alongside its community and would not be where it is today without the dedication and commitment to open source, which is why we are more than proud to be back again in Hacktoberfest!\\n\\n## Why Hacktoberfest?\\n\\nIf you are interested in chaos engineering, open-source, trying to come up with a project for school, or looking into a potential career path as an SRE/DevOps engineer, then this is your golden opportunity: throughout Hacktoberfest, anyone, regardless of background and experience, can join and contribute changes - big or small. So grab the chance and learn about how to make a system more resilient! The Chaos Mesh community welcomes you with open arms and is more than willing to work and share feedback with you. Your contributions can make a big difference!\\n\\n## Quick start\\n\\nHere\u2019s a quick run-through of how to be part of Hacktoberfest 2021, and you can check out a detailed how to be part of Hacktoberfest on the [official website](https://hacktoberfest.digitalocean.com/resources/participation):\\n\\n1. Sign up for [Hacktoberfest](https://hacktoberfest.digitalocean.com/) anytime between Oct 1 and Oct 31.\\n2. Join the #project-chaos-mesh channel under [CNCF Slack](https://slack.cncf.io/), just in case you have any questions, or need help.\\n3. Start creating and submitting your PRs! Here are some top tips:\\n   1. Check out the [Chaos Mesh Contribution guide](https://github.com/chaos-mesh/chaos-mesh/blob/master/CONTRIBUTING.md) before making contributions.\\n   2. Have a go at any [issue](https://github.com/chaos-mesh/chaos-mesh/issues) labeled with \\"Hacktoberfest\\", note that these are the ones that we think might be good for those new to open source or Chaos Mesh, so it only serves as a starting point!\\n\\n## Some notes\\n\\n- To get a shirt, you must make 4 approved PRs on opted-in projects between October 1-31 in any time zone. If a repository has no \u201cHacktoberfest\u201d topic set, please reach out to us or mention Hacktoberfest in your PR so we can set repository topics.\\n- No spams please (e.g. creating a PR just for the sake of it and not adding any value in any way)! Our maintainer will mark a PR as invalid if it\u2019s determined to be spam, which does NOT count towards your PR total.\\n- Note that if our maintainer reports behavior that\u2019s not in line with the [code of conduct](https://github.com/chaos-mesh/chaos-mesh/blob/master/CODE_OF_CONDUCT.md), you will be ineligible to participate.\\n\\nLastly, good luck, on your marks, get set, and hack away!"},{"id":"/run-chaos-experiments-on-physical-machines","metadata":{"permalink":"/zh/blog/run-chaos-experiments-on-physical-machines","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-09-15-run-chaos-experiments-on-physical-machines.md","source":"@site/blog/2021-09-15-run-chaos-experiments-on-physical-machines.md","title":"How to run chaos experiments on your physical machine","description":"How to run chaos experiments on your physical machine","date":"2021-09-15T00:00:00.000Z","formattedDate":"2021\u5e749\u670815\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"chaosd","permalink":"/zh/blog/tags/chaosd"}],"readingTime":3.935,"truncated":true,"authors":[{"name":"Xiang Wang","title":"Chaos Mesh Committer","url":"https://github.com/WangXiangUSTC","imageURL":"https://avatars.githubusercontent.com/u/5793595?v=4"}],"prevItem":{"title":"Hacktoberfest 2021: hack with Chaos Mesh!","permalink":"/zh/blog/chaos-mesh-hacktoberfest-2021"},"nextItem":{"title":"Securing Online Gaming: Combine Chaos Engineering with DevOps Practices","permalink":"/zh/blog/Securing-Online-Gaming-Combine-Chaos-Engineering-with-DevOps-Practices"}},"content":"![How to run chaos experiments on your physical machine](/img/chaosd-banner.png)\\n\\n[Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is a cloud-native Chaos Engineering platform that orchestrates chaos in Kubernetes environments. With Chaos Mesh, you can simulate a variety of failures, and use Chaos Dashboard, a web UI, to manage chaos experiments directly. Since it was open-sourced, Chaos Mesh has been adopted by many companies to ensure their systems\u2019 resilience and robustness. But over the past year, we have frequently heard requests from the community asking how to run chaos experiments when the services are not deployed on Kubernetes.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is chaosd\\n\\nTo meet the growing needs of chaos testing on physical machines, we are excited to present an enhanced toolkit called chaosd. You might find the name familiar. That\u2019s because it evolved from `chaos-daemon`, a key component in Chaos Mesh. At TiDB Hackathon 2020, we [refactored chaosd to make it more than a command-line tool](https://en.pingcap.com/blog/chaos-mesh-remake-one-step-closer-toward-chaos-as-a-service#refactor-chaosd). Now with [chaosd v1.0.1](https://github.com/chaos-mesh/chaosd/releases/tag/v1.0.1), you can simulate specific errors that target physical machines, and then, undo the chaos experiments like nothing had happened.\\n\\n## Benefits of chaosd\\n\\nchaosd has the following advantages:\\n\\n- **Easy-to-use**: You can easily create and manage chaos experiments with chaosd commands.\\n- **Various fault types**: You can simulate faults to be injected on physical machines at different levels, including process faults, network faults, Java Virtual Machine (JVM) application faults, stress scenarios, disk faults, and host faults.\\n- **Multiple work modes**: You can use chaosd as a command-line tool or as a service.\\n\\nWithout further ado, let\u2019s give it a try.\\n\\n## How to use chaosd\\n\\nIn this section, I will walk you through how to inject a network fault with chaosd. Your glibc version must be v2.17 or later versions.\\n\\n### 1. Download and unzip chaosd\\n\\nTo download chaosd, run the following command:\\n\\n```bash\\ncurl -fsSL -o chaosd-v1.0.1-linux-amd64.tar.gz https://mirrors.chaos-mesh.org/chaosd-v1.0.1-linux-amd64.tar.gz\\n```\\n\\nUnzip the file. It contains two file folders:\\n\\n- `chaosd` contains the tool entry of chaosd.\\n- `tools` contains the tools needed to perform the chaos experiment, including [stress-ng](https://wiki.ubuntu.com/Kernel/Reference/stress-ng) (to simulate stress scenarios), [Byteman](https://github.com/chaos-mesh/byteman) (to simulate JVM application faults), and PortOccupyTool (to simulate network faults).\\n\\n### 2. Create a chaos experiment\\n\\nIn this chaos experiment, the server will be unable to access chaos-mesh.org.\\n\\nRun the following command:\\n\\n```bash\\nsudo ./chaosd attack network loss --percent 100 --hostname chaos-mesh.org --device ens33\\n```\\n\\nExample output:\\n\\n```bash\\nAttack network successfully, uid: c55a84c5-c181-426b-ae31-99c8d4615dbe\\n```\\n\\nIn this simulation, the ens33 network interface card cannot send network packets to or receive packets from [chaos-mesh.org](http://chaos-mesh.org). The reason why you have to use `sudo` commands is that the chaos experiment modifies network rules, which require root privileges.\\n\\nAlso, don\u2019t forget to save the `uid` of the chaos experiment. You\u2019ll be entering that later as part of the recovery process.\\n\\n### 3. Verify the results\\n\\nUse the `ping` command to see if the server can access chaos-mesh.org:\\n\\n```bash\\nping chaos-mesh.org\\nPING chaos-mesh.org (185.199.109.153) 56(84) bytes of data.\\n```\\n\\nWhen you execute the command, it\u2019s very likely that the site won\u2019t respond. Press `CTRL`+`C` to stop the ping process. You should be able to see the statistics of the `ping` command: `100% packet loss`.\\n\\nExample output:\\n\\n```bash\\n2 packets transmitted, 0 received, 100% packet loss, time 1021ms\\n```\\n\\n### 4. Recover the experiment\\n\\nTo recover the experiment, run the following command:\\n\\n```bash\\nsudo ./chaosd recover c55a84c5-c181-426b-ae31-99c8d4615dbe\\n```\\n\\nExample output:\\n\\n```bash\\nRecover c55a84c5-c181-426b-ae31-99c8d4615dbe successfully\\n```\\n\\nIn this step, you also need to use `sudo` commands because root privileges are required. When you finish recovering the experiment, try to ping chaos-mesh.org again to verify the connection.\\n\\n## Next steps\\n\\n### Support dashboard web\\n\\nAs you can see, chaosd is fairly easy to use. But we can make it easier\u2014a dashboard web for chaosd is currently under extensive development.\\n\\nWe will continue to enhance its usability and implement more functionalities such as managing chaos experiments run with chaosd as well as those run with Chaos Mesh. This will provide a consistent and unified user experience for chaos testing on Kubernetes and physical machines. The architecture below is just a simple example:\\n\\n![Chaos Mesh\'s optimized architecture](/img/chaos-mesh-optimized-architecture.png)\\n\\n<p className=\\"caption-center\\">Chaos Mesh\'s optimized architecture</p>\\n\\nFor more, check out [Chaos Mesh\'s optimized architecture](https://en.pingcap.com/blog/chaos-mesh-remake-one-step-closer-toward-chaos-as-a-service#developing-chaos-mesh-towards-caas).\\n\\n### Add more fault injection types\\n\\nCurrently, chaosd provides six fault injection types. We plan to develop more types that have been supported by Chaos Mesh, including HTTPChaos and IOChaos.\\n\\nIf you are interested in helping us improve chaosd, you are welcome to [pick an issue](https://github.com/chaos-mesh/chaosd/labels/help%20wanted) and get started!\\n\\n## Try it out!\\n\\nIf you are interested in using chaosd and want to explore more, check out the [documentation](https://chaos-mesh.org/docs/chaosd-overview). If you come across an issue when you run chaosd, or if you have a feature request, feel free to [create an issue](https://github.com/chaos-mesh/chaosd/issues). We would love to hear your voice!"},{"id":"/Securing-Online-Gaming-Combine-Chaos-Engineering-with-DevOps-Practices","metadata":{"permalink":"/zh/blog/Securing-Online-Gaming-Combine-Chaos-Engineering-with-DevOps-Practices","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-08-26-securing-online-gaming-combine-chaos-engineering-with-devops-practices.md","source":"@site/blog/2021-08-26-securing-online-gaming-combine-chaos-engineering-with-devops-practices.md","title":"Securing Online Gaming: Combine Chaos Engineering with DevOps Practices","description":"Securing Online Gaming: Combine Chaos Engineering with DevOps Practices","date":"2021-08-26T00:00:00.000Z","formattedDate":"2021\u5e748\u670826\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Use case","permalink":"/zh/blog/tags/use-case"}],"readingTime":7.635,"truncated":true,"authors":[{"name":"Zhaojun Wu","title":"Senior DevOps Engineer at Tencent Interactive Entertainment Group","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"How to run chaos experiments on your physical machine","permalink":"/zh/blog/run-chaos-experiments-on-physical-machines"},"nextItem":{"title":"How Chaos Mesh Helps Apache APISIX Improve System Stability","permalink":"/zh/blog/How-Chaos-Mesh-Helps-Apache-APISIX-Improve-System-Stability"}},"content":"![Securing Online Gaming: Combine Chaos Engineering with DevOps Practices](/img/chaos-mesh-tencent-ieg.jpeg)\\n\\nInteractive Entertainment Group (IEG) is a division of Tencent Holdings that focuses on the development of online video games and other digital content such as live broadcasts. It is well-known for being the publisher of some of the most popular video games.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this article, I will explain why and how we introduce chaos engineering into our DevOps process.\\n\\nFor each day, we handle over 10,000,000 total visits, and, during peak hours, we process over 1,000,000 queries per second (QPS). To guarantee players a fun and engaging experience, we launch various daily or seasonal game events. Sometimes, that means we must update the event code over 500 times per day. As our user base grows, the total amount of data quickly multiplies. Currently, the figure stands at 200 terabytes. We have to manage the massive user queries and rapid release iterations, and we managed it well.\\n\\nA cloud-native DevOps solution frees our events operator from the growing number of online events. We developed a pipeline that takes care of everything they need, from writing code to launching events in production environments: once new event codes are detected, the operation platform automatically builds images from them and deploys the image to Tencent Kubernetes Engine (TKE). You might be wondering how long this entire automated process takes: only 5 minutes.\\n\\nCurrently, almost all IEG operation services run in TKE. Elastic scaling promises faster capacity expansion and reduction of cloud services thanks to cloud-native technology.\\n\\nIn addition, we expect the iterations to be easier. A best practice is to break down the large, hard-to-maintain service into many \u201csmaller\u201d services that we can maintain independently. \u201cSmall\u201d services have less code and simpler logic, with lower handover and training costs. We as developers continue to practice this kind of microservices architecture as part of DevOps initiatives. Yet similar issues persist. As the number of services increases, so does the complexity of making calls between them. **Worse, if one \u201csmall\u201d service fails, it could set off a chain reaction that brings all the services down\u2014a microservice dependency hell.**\\n\\nThe thing is, fault tolerance varies by service. Some support downgrading, while others don\u2019t. Not to mention that some services are unable to provide timely alerts or lack an effective debugging tool. As a result, debugging services has become a tricky and increasingly pressing issue in our day-to-day work.\\n\\nBut we can\u2019t just let it be. What if the unstable performance constantly chases our players away? What if there is a catastrophic failure?\\n\\n## Let there be faults\\n\\nNetflix introduced the idea of chaos engineering. This approach tests the resilience of the system against all kinds of edgy cases by injecting faults in a non-production environment to achieve ideal system reliability. According to one Gartner article, by 2023, 40% of organizations will use chaos engineering to meet their top DevOps objectives, reducing unplanned downtime by 20%.\\n\\nThis is exactly how we avoid the worst-case scenario. Fault injection, in my opinion, is now a must-do in every technical team. In our early test cases, developers would bring down a node before launching a service to see if the primary node automatically switched to the secondary node and if disaster recovery worked.\\n\\n**But chaos engineering is more than fault injection.** It is a field that constantly drives new techniques, professional testing tools, and solid theories. That\u2019s why we continue to explore it.\\n\\nIEG officially launched its chaos engineering project over a year ago. We wanted to do this right the first time. The key is to select a chaos engineering tool that supports running experiments in the Kubernetes environment. **After a careful comparison, we believe [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is our best option** because:\\n\\n- It is a Cloud Native Computing Foundation (CNCF) Sandbox project with a friendly and productive community.\\n- It does not intrude on existing applications.\\n- It provides a web UI and a variety of fault injection types, as shown in the image below.\\n\\n![A comparison of chaos engineering tools](/img/comparison-of-chaos-engineering-tools.png)\\n\\n<p className=\\"caption-center\\"> A comparison of chaos engineering tools </p>\\n\\n> Note: This comparison is outdated and is intended simply to compare fault injection features supported by Chaos Mesh with other well-known chaos engineering platforms. It is not intended to favor or position one project over another. Any corrections are welcome.\\n\\n## Build a chaos testing platform\\n\\nOur chaos engineering team embedded Chaos Mesh into our continuous integration and continuous delivery pipelines. As shown in the diagram below, Chaos Mesh now plays an important role in our operation platform. We use Chaos Mesh\'s dashboard API to create, run, and delete chaos experiments and monitor them on our own platform. We can simulate basic system-level faults in Pods, container, network, and IO.\\n\\n![Chaos Mesh embedded in IEG\'s operation platform](/img/chaos-mesh-embedded-in-IEG\'s-operation-platform.png)\\n\\n<p className=\\"caption-center\\">Chaos Mesh embedded in IEG\'s operation platform</p>\\n\\nIn IEG, **chaos engineering is generally summarized as a closed loop with several key phases**:\\n\\n- Improve overall system resilience.\\n\\n  Build a chaos testing platform that we can modify as our needs change.\\n\\n- Design a testing plan.\\n\\n  The testing plan must specify the target, scope, fault to be injected, monitoring metrics, etc. Make sure the testing is well-controlled.\\n\\n- Execute chaos experiments and review the results.\\n\\n  Compare the system\u2019s performance before and after the chaos experiment.\\n\\n- Resolve any issues that may arise.\\n\\n  Fix found issues and upgrade the system for the follow-up experiment.\\n\\n- Repeat chaos experiments and verify performance.\\n\\n  Repeat chaos experiments to see if the system\u2019s performance meets expectations. If it does, design another testing plan.\\n\\n![Five phases of chaos engineering in IEG](/img/five-phases-of-chaos-engineering-in-IEG.png)\\n\\n<p className=\\"caption-center\\">Five phases of chaos engineering in IEG</p>\\n\\nWe frequently **test the performance of services under high CPU usage**, for example. We begin by orchestrating and scheduling experiments. Following that, we run experiments and monitor the performance of related services. Multiple monitoring metrics, such as QPS, latency, response success, are immediately visible through the operation platform. The platform then generates reports for us to review, so we can check whether these experiments met our expectations.\\n\\n## Use cases\\n\\nThe following are a few examples of how we use chaos engineering in our DevOps workflow.\\n\\n### Finer granularity of fault injection\\n\\nThere is no need to shut down the entire system to see if our games are still available to players. Sometimes we only want to inject faults, say, network latency, into a single game account, and observe how it responds. We are now able to achieve this finer granularity by hijacking traffic and running experiments at the gateway.\\n\\n### Red teaming\\n\\nUnderstandably, our team members grew bored of regular chaos experiments. After all, it\u2019s something like telling your left hand to fight against your right hand. Here at IEG, **we integrate a testing practice called red teaming into chaos engineering to ensure that our system resiliency improves in an organic way.** Red teaming is similar to penetration testing, but more targeted. It requires a group of testers to emulate real-world attacks from an outsider\u2019s perspective. If I were in charge of IT operations, I would simulate faults to specific services, and check to see whether my developer colleges were doing a good job. If I found any potential faults, well, be prepared for some \u201chard talk.\u201d On the other hand, developers would actively perform chaos experiments and make sure no risk was left behind to avoid being blamed.\\n\\n![The red teaming process in IEG](/img/red-teaming-process-in-IEG.png)\\n\\n<p className=\\"caption-center\\">The red teaming process in IEG</p>\\n\\n### Dependency analysis\\n\\nIt\u2019s important to manage dependencies for microservices. In our case, non-core services cannot be the bottleneck for core services. Fortunately, with chaos engineering, we can run dependency analysis simply by injecting faults into called services and observing how badly the main service is affected. Based on the results, we can optimize the service calling chain in a specific scenario.\\n\\n### Automated fault detection and diagnosis\\n\\nWe are also exploring AI bots to help us detect and diagnose faults. As services become more complex, the likelihood of failure increases. **Our goal is to train a fault detection model through large-scale chaos experiments in production or other controlled environments.**\\n\\n## Chaos engineering empowers DevOps practices\\n\\nCurrently, on average, more than 50 people run chaos experiments each week, running more than 150 tests, and detecting more than 100 problems in total.\\n\\nGone are the days when performing fault injection requires a handwritten script, which can be a tough thing to do for those who are unfamiliar with it. **The benefits of combining chaos engineering with DevOps practices are obvious: within a few minutes, you can orchestrate various fault types by simply dragging and dropping, execute them with a single click, and monitor the results in real-time\u2014all in one platform.**\\n\\n![Chaos engineering with DevOps ensures efficient fault injection](/img/chaos-engineering-with-devops.png)\\n\\n<p className=\\"caption-center\\">Chaos engineering with DevOps ensures efficient fault injection</p>\\n\\nThanks to full-featured chaos engineering tools and streamlined DevOps processes, we estimate that the efficiency of fault injection and chaos-based optimization at IEG has been improved at least by 10 times in the last six months. If you were unsure about implementing chaos engineering in your business, I hope our experience can be of some help."},{"id":"/How-Chaos-Mesh-Helps-Apache-APISIX-Improve-System-Stability","metadata":{"permalink":"/zh/blog/How-Chaos-Mesh-Helps-Apache-APISIX-Improve-System-Stability","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-08-20-chaos-mesh-apisix.md","source":"@site/blog/2021-08-20-chaos-mesh-apisix.md","title":"How Chaos Mesh Helps Apache APISIX Improve System Stability","description":"Chaos Mesh helps Apache APISIX improve system stability","date":"2021-08-20T00:00:00.000Z","formattedDate":"2021\u5e748\u670820\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":5.535,"truncated":true,"authors":[{"name":"Shuyang Wu","title":"Chaos Mesh Committer","url":"https://github.com/Yiyiyimu","imageURL":"https://avatars.githubusercontent.com/u/34589752?v=4"}],"prevItem":{"title":"Securing Online Gaming: Combine Chaos Engineering with DevOps Practices","permalink":"/zh/blog/Securing-Online-Gaming-Combine-Chaos-Engineering-with-DevOps-Practices"},"nextItem":{"title":"Chaos Mesh 2.0: To a Chaos Engineering Ecology","permalink":"/zh/blog/chaos-mesh-2.0-to-a-chaos-engineering-ecology"}},"content":"![Chaos Mesh helps Apache APISIX improve system stability](/img/chaos-mesh-apisix.jpeg)\\n\\n[Apache APISIX](https://github.com/apache/apisix) is a cloud-native, high-performance, scaling microservices API gateway. It is one of the Apache Software Foundation\'s top-level projects and serves hundreds of companies around the world, processing their mission-critical traffic, including finance, the Internet, manufacturing, retail, and operators. Our customers include NASA, the European Union\'s digital factory, China Mobile, and Tencent.\\n\\n\x3c!--truncate--\x3e\\n\\nAs our community grows, Apache APISIX\'s features more frequently interact with external components, making our system more complex and increasing the possibility of errors. To identify potential system failures and build confidence in the production environment, we introduced the concept of Chaos Engineering.\\n\\n![Apache APISIX architecture](/img/apache-apisix-architecture.jpg)\\n\\n<p className=\\"caption-center\\"> Apache APISIX architecture </p>\\n\\nIn this post, we\'ll share how we use [Chaos Mesh](https://chaos-mesh.org/) to improve our system stability.\\n\\n## Our pain points\\n\\nApache APISIX processes tens of billions of requests a day. At that volume level, our users have noticed a couple of issues:\\n\\n- **Scenario #1:** In Apache APISIX\'s configuration center, when unexpectedly high network latency occurs between etcd and Apache APISIX, can Apache APISIX still filter and forward traffic normally?\\n- **Scenario #2:** When a node in the etcd cluster fails and the cluster can still run normally, an error is reported for the node\'s interaction with the Apache APISIX admin API.\\n\\nAlthough Apache APISIX has covered many scenarios through unit, end-to-end (E2E), and fuzz tests in continuous integration (CI), it has not covered the interaction scenario with external components. If the system behaves abnormally, for example, if the network jitters, a hard disk fails, or a process is killed, can Apache APISIX give appropriate error messages? Can it keep running or restore itself to normal operation?\\n\\n## Why we chose Chaos Mesh\\n\\nTo test these user scenarios and to discover similar problems before our product goes into production, our community decided to use Chaos Mesh for chaos testing.\\n\\nChaos Mesh is a cloud-native Chaos Engineering platform that features all-around fault injection methods for complex systems on Kubernetes, covering faults in Pod, the network, file system, and even the kernel. It helps users find weaknesses in the system and ensures that the system can resist out-of-control situations in the production environment.\\n\\nLike Apache APISIX, Chaos Mesh has an active open source community. We know that an active community can ensure stable software use and rapid iteration. This makes Chaos Mesh more attractive.\\n\\n## How we use Chaos Mesh in APISIX\\n\\nChaos Engineering has grown beyond simple fault injection and now forms a complete methodology. To create a chaos experiment, we determined what the normal operation or \\"steady state\\" of our application should be. We then introduced potential problems to see how the system responded. If the problems knocked the application out of its steady state, we fixed them.\\n\\nNow, we\'ll take the two scenarios we mentioned to show you how we use Chaos Mesh in Apache APISIX.\\n\\n### Scenario #1\\n\\nWe deployed a Chaos Engineering experiment using the following steps:\\n\\n1. We found metrics to measure whether Apache APISIX is running normally. In the test, the most important method is to use Grafana to monitor the Apache APISIX\'s running metrics. We extracted data from Prometheus in CI for comparison. Here, we used the routing and forwarding requests per second (RPS) and etcd connectivity as evaluation metrics. We analyzed the log. For Apache APISIX, we checked Nginx\'s error log to determine whether there was an error and whether the error was in line with our expectations.\\n\\n2. We performed a test in the control group. We found that both `create route` and `access route` were successful, and we could connect to etcd. We recorded the RPS.\\n\\n3. We used network chaos to add a five second network latency and then retested. This time, `set route` failed, `get route` succeeded, etcd could be connected to, and RPS had no significant change compared to the previous experiment. The experiment met our expectations.\\n\\n![High network latency occurs between etcd and Apache APISIX](/img/high-network-latency-between-etcd-and-apache-apisix.jpg)\\n\\n<p className=\\"caption-center\\"> High network latency occurs between etcd and Apache APISIX </p>\\n\\n### Scenario #2\\n\\nAfter we conducted the same experiment as above in the control group, we introduced pod-kill chaos and reproduced the expected error. When we randomly deleted a small number of etcd nodes in the cluster, sometimes APISIX could connect to etcd and sometimes not, and the log printed a large number of connection rejection errors.\\n\\nWhen we deleted the first or third node in the etcd endpoint list, the `set route` returned a result normally. However, when we deleted the second node in the list, the `set route` returned the error \\"connection refused.\\"\\n\\nOur troubleshooting revealed that the etcd Lua API used by Apache APISIX selected the endpoint sequentially, not randomly. Therefore, when we created an etcd client, we bound to only one etcd endpoint. This led to continuous failure.\\n\\nAfter we fixed this problem, we added a health check to the etcd Lua API to ensure that a large number of requests would not be sent to the disconnected etcd node. To avoid flooding the log with errors, we added a fallback mechanism when the etcd cluster was completely disconnected.\\n\\n![Error Reported from etcd Node Interaction](/img/error-reported-from-etcd-node-interaction.jpg)\\n\\n<p className=\\"caption-center\\"> An error is reported from one etcd node\'s interaction with the Apache APISIX admin API </p>\\n\\n## Our future plans\\n\\n### Run a chaos test in E2E simulation scenarios\\n\\nIn Apache APISIX, we manually identify system weaknesses for testing and repair. As in the open source community, we test in CI, so we don\'t need to worry about the impact of Chaos Engineering\'s failure radius on the production environment. But the test cannot cover complicated and comprehensive application scenarios in the production environment.\\n\\nTo cover more scenarios, the community plans to use the existing E2E test to simulate more complete scenarios and conduct chaos tests that are more random and cover a larger range.\\n\\n### Add chaos tests to more Apache APISIX projects\\n\\nIn addition to finding more vulnerabilities for Apache APISIX, the community plans to add chaos tests to more projects such as Apache APISIX Dashboard and Apache APISIX Ingress Controller.\\n\\n### Add features to Chaos Mesh\\n\\nWhen we deployed Chaos Mesh, some features were temporarily unsupported. For example, we couldn\'t select a service as a network latency target or specify container port injection as network chaos. In the future, the Apache APISIX community will assist Chaos Mesh to add related features.\\n\\nYou\'re welcome to contribute to the [Apache APISIX project](https://github.com/apache/apisix) on GitHub. If you are interested in Chaos Mesh and would like to improve it, join our [Slack channel](https://slack.cncf.io/) (#project-chaos-mesh) or submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/chaos-mesh-2.0-to-a-chaos-engineering-ecology","metadata":{"permalink":"/zh/blog/chaos-mesh-2.0-to-a-chaos-engineering-ecology","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-08-13-chaos-mesh-2.0-to-a-chaos-engineering-ecology.md","source":"@site/blog/2021-08-13-chaos-mesh-2.0-to-a-chaos-engineering-ecology.md","title":"Chaos Mesh 2.0: To a Chaos Engineering Ecology","description":"Chaos Mesh 2.0: To a Chaos Engineering Ecology","date":"2021-08-13T00:00:00.000Z","formattedDate":"2021\u5e748\u670813\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Announcement","permalink":"/zh/blog/tags/announcement"}],"readingTime":4.32,"truncated":true,"authors":[{"name":"Chaos Mesh Maintainers","url":"https://github.com/chaos-mesh","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"How Chaos Mesh Helps Apache APISIX Improve System Stability","permalink":"/zh/blog/How-Chaos-Mesh-Helps-Apache-APISIX-Improve-System-Stability"},"nextItem":{"title":"Chaos Mesh Celebrates 100th Contributor","permalink":"/zh/blog/chaos-mesh-celebrates-100th-contributor"}},"content":"![Chaos Mesh 2.0: To a Chaos Engineering Ecology](/img/chaos-mesh-2.0-ga.png)\\n\\nOn July 23, 2021, Chaos Mesh 2.0 was made generally available! It\u2019s an exciting release, marking a solid milestone towards the chaos engineering ecology that we hope to build.\\n\\n\x3c!--truncate--\x3e\\n\\nMaking chaos engineering easier has always been Chaos Mesh\u2019s unswerving goal, and this release is a key step. After almost a year of continuous efforts, we have made major improvements in three main areas: ease of use, native experiment orchestration & scheduling, along with the richness of fault injection types.\\n\\n## Ease of use\\n\\nWe are committed to improving the usability of Chaos Mesh, and a key path to this is Chaos Dashboard, a web interface for users to orchestrate chaos experiments. For Chaos Mesh 2.0, we have improved the Chaos Dashboard in the following ways, further simplifying the complexity of chaos experiments:\\n\\n- It now supports the creation, viewing, and updating of AWSChaos and GCPChaos, so that conducting chaos experiments in a cloud environment can provide a consistent experience as in Kubernetes; \\n- It can display more detailed records of each experiment, further enhancing its visibility.\\n\\n![Chaos Mesh 2.0 - Experiment scheduling](/img/chaos-mesh-scheduling-2.0.png)\\n\\n## Native experiment orchestration & scheduling\\n\\nWhen conducting chaos experiments, a single experiment is often not enough to simulate a complete testing scenario, and manually starting or stopping the experiment would be a tedious and dangerous thing to do. Previously, we [combined Argo with Chaos Mesh](https://chaos-mesh.org/blog/building_automated_testing_framework) to inject faults automatically as a workflow. However, we later realized that Argo workflow is not the best way to describe declarative chaos experiments, and decided to write another workflow engine. Chaos Mesh 2.0 features native Workflow to support experiment orchestration, which means you can serially or parallely execute multiple experiments. You can even weave in notifications and health checks to simulate more complex experimental scenarios.\\n\\n![Chaos Mesh 2.0 - Workflow](/img/chaos-mesh-workflow-2.0.png)\\n\\nIn previous versions, we used the `cron` and `duration` fields to define chaos experiments that were executed periodically.  It didn\u2019t take us long to realize that describing behavior this way was not fitting. For example, a single execution often takes longer than an execution cycle. This definition works fine, but lacks a suitable description for the study of expected behavior. We referred to CronJob and introduced Schedule, a new custom object, to Chaos Mesh. It adds more explicit properties to periodically executed tasks, such as whether multiple experiments are allowed to be executed at the same time, thereby restricting behavior.\\n\\n![Chaos Mesh 2.0 - Schedule](/img/chaos-dashboard-schedule-2.0.png)\\n\\n## Richer fault injection types\\n\\nChaos Mesh already supports system-level fault injection types, as well as fault injections into cloud environments such as AWSChaos and GCPChaos. Starting from 2.0, injecting chaos into the application layer has been made possible with the introduction of JVMChaos and HTTPChaos.\\n\\n### JVMChaos\\n\\nJVM languages such as Java and Kotlin are widely used in the industry. A JVMChaos can be easily simulated through methods like JVM bytecode enhancement and Java Agent. Currently, JVMChaos uses [chaosblade-exec-jvm](https://github.com/chaosblade-io/chaosblade-exec-jvm), and supports injecting various application-level fault types including method delay, specify return value, OOM and throw custom exception. For more info, you can refer to the document: [Simulate JVM Application Faults](https://chaos-mesh.org/docs/simulate-jvm-application-chaos).\\n\\n### HTTPChaos\\n\\nHTTPChaos is a brand new Chaos type supported in the 2.0 version. It can hijack HTTP service requests and responses from the server side, as well as interrupt links, delay injection, or modify Header/Body. It is suitable for all scenarios that use HTTP as the communication protocol. For more information, refer to [Simulate HTTP Faults](https://chaos-mesh.org/docs/simulate-http-chaos-on-kubernetes).\\n\\n## Chaosd: an fault injection tool for physical nodes\\n\\nChaos Mesh is designed for Kubernetes. For physical machine environments, we present [Chaosd](https://github.com/chaos-mesh/chaosd). It evolved from chaos-daemon, a key component in Chaos Mesh, and we have added specific chaos experiments based on the characteristics of physical machines. Currently, Chaosd supports process kill, network, JVM, pressure, disk and a few other types of fault injection onto the physical machine.\\n\\n## Looking ahead\\n\\nChaos Mesh is still under active development, and we have some more powerful features in the works, including:\\n\\n- To inject JVMChaos at runtime, lowering the cost of JVMChaos and making it more easy-to-use.\\n- To introduce a plug-in mechanism to build custom chaos experiments, while the Scheduling function remains unimpaired.\\n\\nIn addition, we noticed that chaos experiments can be reused in a number of scenarios, hence we plan to launch a platform, where customized experiments can be turned into templates. This will enable our users to share and reuse not only specific chaos experiments, but also Workflows for different scenarios.\\n\\n## Try it out!\\n\\nTry out the [Chaos Mesh 2.0 interactive scenarios](https://chaos-mesh.org/interactive-tutorial) from your browser! There\u2019s no need to install or configure, as the complete development environment has been preconfigured with everything you need. Otherwise, you can visit [the Chaos Mesh docs](https://chaos-mesh.org/docs) for more info.\\n\\n\\n## A big thank you\\n\\nThanks to all [Chaos Mesh contributors](https://github.com/chaos-mesh/chaos-mesh/graphs/contributors), Chaos Mesh couldn\u2019t have come from 1.0 to 2.0 without all of your efforts!\\n\\nIf you are interested in Chaos Mesh and would like to help us improve it, you\u2019re welcome to join [our Slack channel](https://slack.cncf.io/) or submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh). Chaos Mesh looks forward to your participation and feedback!"},{"id":"/chaos-mesh-celebrates-100th-contributor","metadata":{"permalink":"/zh/blog/chaos-mesh-celebrates-100th-contributor","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-08-05-chaos-mesh-celebrates-100-contributors.md","source":"@site/blog/2021-08-05-chaos-mesh-celebrates-100-contributors.md","title":"Chaos Mesh Celebrates 100th Contributor","description":"Chaos Mesh Celebrates 100th Contributor","date":"2021-08-05T00:00:00.000Z","formattedDate":"2021\u5e748\u67085\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"community","permalink":"/zh/blog/tags/community"}],"readingTime":3.31,"truncated":true,"authors":[{"name":"Chaos Mesh Maintainers","url":"https://github.com/chaos-mesh/chaos-mesh/blob/master/MAINTAINERS.md","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"Chaos Mesh 2.0: To a Chaos Engineering Ecology","permalink":"/zh/blog/chaos-mesh-2.0-to-a-chaos-engineering-ecology"},"nextItem":{"title":"Chaos Mesh Q&A","permalink":"/zh/blog/chaos-mesh-q&a"}},"content":"![Chaos Mesh Celebrates 100th Contributor](/img/chaos-mesh-celebrates-100-contributors.png)\\n\\nThe [Chaos Mesh project](https://github.com/chaos-mesh/chaos-mesh) just hit two major milestones: the community recently welcomed our [100th contributor](https://github.com/chaos-mesh/chaos-mesh/graphs/contributors) to the chaos-mesh repo and 1,000 followers on [Twitter](https://twitter.com/chaos_mesh)!\\n\\n\x3c!--truncate--\x3e\\n\\nChaos Mesh is a Chaos Engineering platform that orchestrates chaos experiments on Kubernetes environments. Ever since first open-sourced on GitHub on Dec 31st, 2019, it has not stopped: in July 2020, Chaos Mesh joined CNCF [as a Sandbox project](https://chaos-mesh.org/blog/chaos-mesh-join-cncf-sandbox-project); a few months later in September, Chaos Mesh 1.0 was [officially released](https://chaos-mesh.org/blog/chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier). In July 2021, after a few beta versions, [Chaos Mesh 2.0 was announced generally available](https://github.com/chaos-mesh/chaos-mesh/releases/tag/v2.0.0)!\\n\\nSo far, Chaos Mesh has brought out 35 releases, received 1,500+ commits from 100+ contributors, won over 3.8k+ stargazers and 420+ forks. All these achievements would not have been possible without the wonderful community.\\n\\n![Chaos Mesh contributors](/img/chaos-mesh-all-contributors.jpeg)\\n\\n<p className=\\"caption-center\\">Chaos Mesh contributors (as of 2021.08.02)</p>\\n\\nHere are a few of our favourite contributions to highlight:\\n\\n- [@YangKeao](https://github.com/YangKeao) introduced `kubebuilder` to Chaos Mesh, an SDK for building Kubernetes APIs using CRD, which simplified the steps to implement the Controller.\\n- [@g1eny0ung](https://github.com/g1eny0ung) brought in the Chaos Dashboard, a Web UI for manipulating and observing chaos experiments.\\n- [@Yiyiyimu](https://github.com/Yiyiyimu) contributed `chaosctl`, a tool that simplifies chaos development and debugging.\\n- [@Gallardot](https://github.com/Gallardot) helped implement JVMChaos, making it possible for Chaos Mesh to simulate JVM application faults.\\n- [@STRRL](https://github.com/STRRL) started the work on Chaos Mesh Workflow, a built-in workflow engine which enables running different chaos experiments in a serial or parallel manner to simulate production-level errors.\\n\\nFor those who enjoy chaos engineering and open source equally, our mission is to make sure that this is where you belong by enriching the contribution journey, and here\u2019s where we are at so far:\\n\\n- We published the Chaos Mesh [Governance](https://github.com/chaos-mesh/chaos-mesh/blob/master/GOVERNANCE.md) in the beginning of 2021, making clear the roles and responsibilities of each community member as well as the decision-making process, and has since promoted 9 Committers.\\n- We have mentored 4 mentees through the LFX mentorship programs so far. Our mentees have written blogs and hosted talks sharing their LFX experience.\\n- We have participated in 3 KubeCons, where we participated in the bug bash contest and hosted Office Hours to meet and chat with old faces and welcome new members to our community. We even posted a [Q&A](https://chaos-mesh.org/blog/chaos-mesh-q&a) after the KubeCon EU 2021 since we received so many questions!\\n- We are currently applying to propose Chaos Mesh to be promoted to the CNCF [incubating stage](https://github.com/cncf/toc/pull/683), hoping that being promoted to the next stage of maturity brings the project new chances and more exposure.\\n\\nAlthough this is an achievement worth celebrating, we know that there is still a lot of work ahead:\\n\\n- We have also been working with the community to refine the Chaos Mesh [documentation](https://chaos-mesh.org/docs/): updating English versions as per each release and adding Chinese versions for our growing number of Chinese adopters and contributors.\\n- We hope to continue to contribute to the Cloud-Native ecosystem: for example, by developing and amplifying chaos engineering related content, and collaborating with other communities for meetups and projects.\\n\\nAnother goal of ours is to continue building a more diverse and engaging community\u2014 there is no barrier to being part of the Chaos Mesh community and becoming a Chaos Mesh contributor, as contributions are not limited to coding: writing documentation, offering ideas for features, posting issues, writing blogs, answering community questions, or sharing cases are all part of the contribution journey.\\n\\n## To sum up\\n\\nFrom the bottom of our hearts, thank you! We hope that we can keep up the good work and continue to build up this not-so-little community of ours, and continue to contribute to the CNCF and the chaos engineering ecology.\\n\\nIf this is the first time you are hearing of Chaos Mesh, and would like to learn more, find the #project-chaos-mesh channel in [CNCF slack workspace](https://slack.cncf.io/), submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh), or sign up to join in on our next [monthly community meeting](https://community.cncf.io/chaos-mesh-community/)!"},{"id":"/chaos-mesh-q&a","metadata":{"permalink":"/zh/blog/chaos-mesh-q&a","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-07-09-chaos-mesh-q&a.md","source":"@site/blog/2021-07-09-chaos-mesh-q&a.md","title":"Chaos Mesh Q&A","description":"Chaos Mesh Q&A","date":"2021-07-09T00:00:00.000Z","formattedDate":"2021\u5e747\u67089\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":3.035,"truncated":true,"authors":[{"name":"Chaos Mesh Community","url":"https://github.com/chaos-mesh","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"Chaos Mesh Celebrates 100th Contributor","permalink":"/zh/blog/chaos-mesh-celebrates-100th-contributor"},"nextItem":{"title":"Securing tenant namespaces using restrict authorization feature in Chaos Mesh","permalink":"/zh/blog/securing-tenant-namespaces-using-restrict-authorization-feature"}},"content":"![Chaos Mesh Q&A](/img/chaos-mesh-q&a.jpeg)\\n\\nAt KubeCon EU 2021, the [Chaos Mesh](https://chaos-mesh.org/) team hosted two \u201coffice hours sessions\u201d where newcomers, community members, and project maintainers had a chance to chat, get to know each other, and learn more about the project.\\n\\n\x3c!--truncate--\x3e\\n\\nBig thanks to the more than 200 of you who joined us! We received so many great questions during the session, we thought we\u2019d do a round up Q&A.\\n\\n## Your questions answered\\n\\n**Q: Is Chaos Mesh compatible with Service Meshes, such as Istio?**\\n\\n**A:** Yes, you can use Chaos Mesh in the Service Mesh environment. At one of our [previous community meetings](https://www.youtube.com/watch?v=paIgJYOhdGw), Sergio M\xe9ndez and Jossie Castrillo from the University of San Carlos of Guatemala shared how they used Linkerd and Chaos Mesh to conduct chaos experiments for their project, \u201c[COVID-19 Realtime Vaccinated People Visualizer](https://github.com/sergioarmgpl/operating-systems-usac-course/blob/master/lang/en/projects/project1v3/project1.md)\u201d.\\n\\n![Project Architecture](/img/chaos-mesh-linkerd-architecture.png)\\n\\n<p className=\\"caption-center\\">Project Architecture</p>\\n\\n**Q: Can I use Chaos Mesh on-premises or do I need Amazon Web Services (AWS) or Google Cloud Platform (GCP)?**\\n\\n**A:** You can do either! You can deploy Chaos Mesh on your Kubernetes cluster, so it does not matter whether you manage it yourself or have it hosted on AWS or GCP. However, if you would like to use it in a Kubernetes environment, you need to [set relevant parameters](https://chaos-mesh.org/docs/1.2.4/user_guides/installation) during installation.\\n\\n**Q: How do \\"chaos actions\\" work?**\\n\\n**A:** Chaos Mesh uses Kubernetes CustomResourceDefinitions (CRDs) to manage chaos experiments. Different fault injection behaviors are implemented in different ways, but the overall idea is the same: Chaos Mesh uses an application\'s execution link to inject chaos into the application. For example, when we inject chaos into the overall link of network interaction, the network interaction card is passed through. Because Linux uses traffic control to increase interference to the specific network interaction card, we can directly use traffic control for network fault injection.\\n\\n**Q: Are you going to add probe support to Chaos Mesh for steady state detection and experiment validation?**\\n\\n**A:** Currently, there is no plan to add this support. Steady state detection and experiment validation are necessary if an application is ready for production. Chaos Mesh itself does not monitor related work, but provides an interface to access existing monitoring systems or the status interface of the application to monitor and detect the application\u2019s steady state.\\n\\n**Q: What elevated privileges do the Chaos Mesh pods need?**\\n\\n**A:** By default, the Chaos Daemon components in Chaos Mesh run in the `privileged` mode. If your Kubernetes cluster version is v3.11 or higher, you can replace `privileged` mode by configuring `capabilities`.\\n\\n**Q: Can I implement Chaos Mesh inside build pipelines to log specific test results?**\\n\\n**A:** Yes, that\u2019s easy to do. You can integrate Chaos Mesh with pipeline systems such as Argo, Jenkins, GitHub Action, and Spanner. Chaos Mesh uses Kubernetes CRDs to manage chaos experiments. To inject chaos, you only need to create the chaos CRD object you want in the pipeline. You can obtain the running status of an experiment through its status structure and event.\\n\\n**Q: What can we expect from the 2.0 release? Can you share some updates on HTTPChaos?**\\n\\n**A:** Chaos Mesh 2.0 will provide native workflow support, and users can arrange chaos experiments in Chaos Mesh. In addition, for Chaos Mesh 2.0, we have reconstructed the existing chaos controller so that users can more easily add new fault injection types. As for HTTPChaos, we\u2019re adding network failure simulation to the HTTP application layer!\\n\\n## Join the Chaos Mesh community\\n\\nIf you are interested in Chaos Mesh and would like to help us improve it, you\'re welcome to join [our Slack channel](https://slack.cncf.io/) or submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/securing-tenant-namespaces-using-restrict-authorization-feature","metadata":{"permalink":"/zh/blog/securing-tenant-namespaces-using-restrict-authorization-feature","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-07-07-restrict-authorization.md","source":"@site/blog/2021-07-07-restrict-authorization.md","title":"Securing tenant namespaces using restrict authorization feature in Chaos Mesh","description":"Chaos engineering tools","date":"2021-07-07T00:00:00.000Z","formattedDate":"2021\u5e747\u67087\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":2.795,"truncated":true,"authors":[{"name":"Anurag Paliwal","title":"Contributor of Chaos Mesh","url":"https://github.com/anuragpaliwal80","imageURL":"https://avatars.githubusercontent.com/u/3283882?v=4"}],"prevItem":{"title":"Chaos Mesh Q&A","permalink":"/zh/blog/chaos-mesh-q&a"},"nextItem":{"title":"How to efficiently stress test Pod memory","permalink":"/zh/blog/how-to-efficiently-stress-test-pod-memory"}},"content":"![Chaos engineering tools](/img/chaos-mesh-restrict-authorization.jpeg)\\n\\nA [multi-tenant](https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview) cluster is shared by multiple users and/or workloads which are referred to as \\"tenants\\".The operators of multi-tenant clusters must isolate tenants from each other to minimize the damage that a compromised or malicious tenant can do to the cluster and other tenants.\\n\\n\x3c!--truncate--\x3e\\n\\n## Cluster multi-tenancy\\n\\nWhen you plan a multi-tenant architecture, you should consider the layers of resource isolation in Kubernetes: cluster, namespace, node, Pod, and container.\\n\\nAlthough Kubernetes cannot guarantee perfectly secure isolation between tenants, it does offer features that may be sufficient for specific use cases. You can separate each tenant and their Kubernetes resources into their own [namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/).\\nKubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces. [Namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) are intended for use in environments with many users spread across multiple teams, or projects.\\n\\n## Cluster having Chaos Mesh\\n\\nYou designed your Kubernetes cluster to have multiple tenant services. You followed the best security practices for Kubernetes: each tenant service is running in its own namespaces, users of these tenant services have appropriate access that also only for their respective namespaces, etc.\\n\\n\x3c!--truncate--\x3e\\n\\nYou enabled Chaos Mesh ([Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is a cloud-native Chaos Engineering platform that orchestrates chaos on Kubernetes environments) on the cluster so that your tenant services can perform different chaos activities to make sure their application/system is resilient. You have also given Chaos Mesh specific rights to those tenant service users so that they can manage Chaos Mesh resources using [RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/).\\n\\n\x3c!--truncate--\x3e\\n\\nSuppose one of the tenant users wants to perform pod kill operations in his/her namespace i.e. chaos-testing. To achieve the same, the user created the below Chaos Mesh YAML file:\\n\\n```yml\\napiVersion: chaos-mesh.org/v1alpha1\\nkind: PodChaos\\nmetadata:\\n  name: pod-kill\\n  namespace: chaos-testing\\nspec:\\n  action: pod-kill\\n  mode: one\\n  selector:\\n    namespaces:\\n      - tidb-cluster-demo\\n    labelSelectors:\\n      \'app.kubernetes.io/component\': \'tikv\'\\n  scheduler:\\n    cron: \'@every 1m\'\\n```\\n\\nThe user has required rights to namespace chaos-testing, but does not have rights on tidb-cluster-demo namespace. When the user applies the above YAML file using kubectl, it will create the pod-kill Chaos Mesh resource in chaos-testing namespace. As we can see in the selector section, the user has specified some other namespace (tidb-cluster-demo), which means the pods which will be selected for this chaos operation will be from tidb-cluster-demo namespace, and not from the one for which the user has access i.e. chaos-testing. This means that this user is able to impact the other namespace for which (s)he does not have the rights. **Problem!!!**\\n\\n\x3c!--truncate--\x3e\\n\\nSince the release of Chaos Mesh 1.1.3, this security issue has been fixed with a restricted authorization feature. Now when user applies the above YAML file, the system shows the error similar to:\\n\\n```yml\\nError when creating \\"pod/pod-kill.yaml\\": admission webhook \\"vauth.kb.io\\" denied the request: ... is forbidden on namespace\\ntidb-cluster-demo\\n```\\n\\n**Problem solved!**\\n\\nPlease note, if the user has required rights on tidb-cluster-demo namespace as well, then there will be no such error.\\n\\n## For more tutorials\\n\\nIn case you want to enforce that no user should be allowed to create chaos across namespaces, you can check out my previous blog: [Securing tenant services while using chaos mesh using OPA](https://anuragpaliwal-93749.medium.com/securing-tenant-services-while-using-chaos-mesh-using-opa-3ae80c7f4b85).\\n\\n## Last but not least\\n\\nIf you are interested in Chaos Mesh and would like to learn more, you\'re welcome to join the [Slack channel](https://slack.cncf.io/) or submit your pull requests or issues to its [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/how-to-efficiently-stress-test-pod-memory","metadata":{"permalink":"/zh/blog/how-to-efficiently-stress-test-pod-memory","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-07-01-how-to-efficiently-stress-test-pod-memory.md","source":"@site/blog/2021-07-01-how-to-efficiently-stress-test-pod-memory.md","title":"How to efficiently stress test Pod memory","description":"banner","date":"2021-07-01T00:00:00.000Z","formattedDate":"2021\u5e747\u67081\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"StressChaos","permalink":"/zh/blog/tags/stress-chaos"},{"label":"Stress Testing","permalink":"/zh/blog/tags/stress-testing"}],"readingTime":10.015,"truncated":true,"authors":[{"name":"Yinghao Wang","title":"Contributor of Chaos Mesh","url":"https://github.com/AsterNighT","imageURL":"https://avatars.githubusercontent.com/u/22937027?v=4"}],"prevItem":{"title":"Securing tenant namespaces using restrict authorization feature in Chaos Mesh","permalink":"/zh/blog/securing-tenant-namespaces-using-restrict-authorization-feature"},"nextItem":{"title":"Chaos Mesh Remake: One Step Closer toward Chaos as a Service","permalink":"/zh/blog/chaos-mesh-remake-one-step-closer-towards-chaos-as-a-service"}},"content":"![banner](/img/how-to-efficiently-stress-test-pod-memory-banner.jpg)\\n\\n[Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) includes the StressChaos tool, which allows you to inject CPU and memory stress into your Pod. This tool can be very useful when you test or benchmark a CPU-sensitive or memory-sensitive program and want to know its behavior under pressure.\\n\\nHowever, as we tested and used StressChaos, we found some issues with usability and performance. For example, why does StressChaos use far less memory than we configured? To correct these issues, we developed a new set of tests. In this article, I\'ll describe how we troubleshooted these issues and corrected them. This information will enable you to get the most out of StressChaos.\\n\\n\x3c!--truncate--\x3e\\n\\nBefore you continue, you need to install Chaos Mesh in your cluster. You can find detailed instructions on our [website](https://chaos-mesh.org/docs/quick-start).\\n\\n## Injecting stress into a target\\n\\nI\u2019d like to demonstrate how to inject StressChaos into a target. In this example, I\u2019ll use [`hello-kubernetes`](https://github.com/paulbouwer/hello-kubernetes), which is managed by [helm charts](https://helm.sh/). The first step is to clone the [`hello-kubernetes`](https://github.com/paulbouwer/hello-kubernetes) repo and modify the chart to give it a resource limit.\\n\\n```bash\\ngit clone https://github.com/paulbouwer/hello-kubernetes.git\\ncode deploy/helm/hello-kubernetes/values.yaml # or whichever editor you prefer\\n```\\n\\nFind the resources line, and change it into:\\n\\n```yaml\\nresources:\\n  requests:\\n    memory: \'200Mi\'\\n  limits:\\n    memory: \'500Mi\'\\n```\\n\\nHowever, before we inject anything, let\'s see how much memory the target is consuming. Go into the Pod and start a shell. Enter the following, substituting the name of your Pod for the one in the example:\\n\\n```bash\\nkubectl exec -it -n hello-kubernetes hello-kubernetes-hello-world-b55bfcf68-8mln6 -- /bin/sh\\n```\\n\\nDisplay a summary of memory usage. Enter:\\n\\n```sh\\n/usr/src/app $ free -m\\n/usr/src/app $ top\\n```\\n\\nAs you can see from the output below, the Pod is consuming 4,269 MB of memory.\\n\\n```sh\\n/usr/src/app $ free -m\\n              used\\nMem:          4269\\nSwap:            0\\n\\n/usr/src/app $ top\\nMem: 12742432K used\\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\\n    1     0 node     S     285m   2%   0   0% npm start\\n   18     1 node     S     284m   2%   3   0% node server.js\\n   29     0 node     S     1636   0%   2   0% /bin/sh\\n   36    29 node     R     1568   0%   3   0% top\\n```\\n\\nThat doesn\u2019t seem right. We\u2019ve limited its memory usage to 500 MiBs, and now the Pod seems to be using several GBs of memory. If we total the amount of process memory being used, it doesn\u2019t equal 500 MiB. However, top and free at least give similar answers.\\n\\nWe will run a StressChaos on the Pod and see what happens. Here\'s the yaml we\u2019ll use:\\n\\n```yaml\\napiVersion: chaos-mesh.org/v1alpha1\\nkind: StressChaos\\nmetadata:\\n  name: mem-stress\\n  namespace: chaos-testing\\nspec:\\n  mode: all\\n  selector:\\n    namespaces:\\n      - hello-kubernetes\\n  stressors:\\n    memory:\\n      workers: 4\\n      size: 50MiB\\n      options: [\'\']\\n  duration: \'1h\'\\n```\\n\\nSave the yaml to a file. I named it `memory.yaml`. To apply the chaos, run\\n\\n```bash\\n~ kubectl apply -f memory.yaml\\nstresschaos.chaos-mesh.org/mem-stress created\\n```\\n\\nNow, let\'s check the memory usage again.\\n\\n```sh\\n              used\\nMem:          4332\\nSwap:            0\\n\\nMem: 12805568K used\\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\\n   54    50 root     R    53252   0%   1  24% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   57    52 root     R    53252   0%   0  22% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   55    53 root     R    53252   0%   2  21% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   56    51 root     R    53252   0%   3  21% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   18     1 node     S     289m   2%   2   0% node server.js\\n    1     0 node     S     285m   2%   0   0% npm start\\n   51    49 root     S    41048   0%   0   0% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   50    49 root     S    41048   0%   2   0% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   52    49 root     S    41048   0%   0   0% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   53    49 root     S    41048   0%   3   0% {stress-ng-vm} stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   49     0 root     S    41044   0%   0   0% stress-ng --vm 4 --vm-keep --vm-bytes 50000000\\n   29     0 node     S     1636   0%   3   0% /bin/sh\\n   48    29 node     R     1568   0%   1   0% top\\n```\\n\\nYou can see that stress-ng instances are being injected into the Pod. There is a 60 MiB rise in the Pod, which we didn\u2019t expect. The [documentation](https://manpages.ubuntu.com/manpages/focal/en/man1/stress-ng.1.html) indicates that the increase should 200 MiB (4 \\\\* 50 MiB).\\n\\nLet\'s increase the stress by changing the memory stress from 50 MiB to 3,000 MiB. This should break the Pod\u2019s memory limit. I\u2019ll delete the chaos, modify the size, and reapply it.\\n\\nAnd then, boom! The shell exits with code 137. A moment later, I reconnect to the container, and the memory usage returns to normal. No stress-ng instances are found! What happened?\\n\\n## Why does StressChaos disappear?\\n\\nKubernetes limits your container memory usage through a mechanism named [cgroup](https://man7.org/linux/man-pages/man7/cgroups.7.html). To see the 500 MiB limit in our Pod, go to the container and enter:\\n\\n```bash\\n/usr/src/app $ cat /sys/fs/cgroup/memory/memory.limit_in_bytes\\n524288000\\n```\\n\\nThe output is displayed in bytes and translates to `500 * 1024 * 1024`.\\n\\nRequests are used only for scheduling where to place the Pod. The Pod does not have a memory limit or request, but it can be seen as the sum of all its containers.\\n\\nWe\'ve been making a mistake since the very beginning. free and top are not \\"cgrouped.\\" They rely on `/proc/meminfo` (procfs) for data. Unfortunately, `/proc/meminfo` is old, so old it predates cgroup. It will provide you with **host** memory information instead of your container. Let\'s start from the beginning and see what memory usage we get this time.\\n\\nTo get the cgrouped memory usage, enter:\\n\\n```sh\\n/usr/src/app $ cat /sys/fs/cgroup/memory/memory.usage_in_bytes\\n39821312\\n```\\n\\nApplying the 50 MiB StressChaos, yields the following:\\n\\n```sh\\n/usr/src/app $ cat /sys/fs/cgroup/memory/memory.usage_in_bytes\\n93577216\\n```\\n\\nThat is about 51 MiB more memory usage than without StressChaos.\\n\\nNext, why did our shell exit? Exit code 137 indicates \\"failure as container received SIGKILL.\\" That leads us to check the Pod. Pay attention to the Pod state and events.\\n\\n```bash\\n~ kubectl describe pods -n hello-kubernetes\\n......\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    1\\n......\\nEvents:\\n  Type     Reason     Age                  From               Message\\n  ----     ------     ----                 ----               -------\\n......\\n  Warning  Unhealthy  10m (x4 over 16m)    kubelet            Readiness probe failed: Get \\"http://10.244.1.19:8080/\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\\n  Normal   Killing    10m (x2 over 16m)    kubelet            Container hello-kubernetes failed liveness probe, will be restarted\\n......\\n```\\n\\nThe events tell us why the shell crashed. `hello-kubernetes` has a liveness probe, and when the container memory is reaching the limit, the application starts to fail, and Kubernetes decides to terminate and restart it. When the Pod restarts, StressChaos stops. In that case, you can say that the chaos works fine. It finds vulnerability in your Pod. You could now fix it, and reapply the chaos. Everything seems perfect now\u2014except for one thing. Why do four 50 MiB vm workers result in 51 MiB in total? The answer will not reveal itself unless we go into the stress-ng source code [here](https://github.com/ColinIanKing/stress-ng/blob/819f7966666dafea5264cf1a2a0939fd344fcf08/stress-vm.c#L2074) :\\n\\n```c\\nvm_bytes /= args->num_instances;\\n```\\n\\nOops! So the document is wrong. The multiple vm workers will take up the total size specified, rather than `mmap` that much memory per worker. Now, finally, we get an answer for everything. In the following sections, we\u2019ll discuss some other situations involving memory stress.\\n\\n## What if there was no liveness probe?\\n\\nLet\'s delete the probes and try again. Find the following lines in `deploy/helm/hello-kubernetes/templates/deployment.yaml` and delete them.\\n\\n```yaml\\nlivenessProbe:\\n  httpGet:\\n    path: /\\n    port: http\\nreadinessProbe:\\n  httpGet:\\n    path: /\\n    port: http\\n```\\n\\nAfter that, upgrade the deployment.\\n\\nWhat is interesting in this scenario is that the memory usage goes up continuously, and then drops sharply; it goes back and forth. What is happening now? Let\'s check the kernel log. Pay attention to the last two lines.\\n\\n```sh\\n/usr/src/app $ dmesg\\n......\\n[189937.362908] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name\\n[189937.363092] [441060]  1000 441060    63955     3791      80     3030           988 node\\n[189937.363110] [441688]     0 441688   193367     2136     372   181097          1000 stress-ng-vm\\n......\\n[189937.363148] Memory cgroup out of memory: Kill process 443160 (stress-ng-vm) score 1272 or sacrifice child\\n[189937.363186] Killed process 443160 (stress-ng-vm), UID 0, total-vm:773468kB, anon-rss:152704kB, file-rss:164kB, shmem-rss:0kB\\n```\\n\\nIt\u2019s clear from the output that the `stress-ng-vm` processes are being killed because there are out of memory (OOM) errors.\\n\\nIf processes can\u2019t get the memory they want, things get tricky. They are very likely to fail. Rather than wait for processes to crash, it\u2019s better if you kill some of them to get more memory. The OOM killer stops processes by an order and tries to recover the most memory while causing the least trouble. For detailed information on this process, see [this introduction](https://lwn.net/Articles/391222/) to OOM killer.\\n\\nLooking at the output above, you can see that `node`, which is our application process that should never be terminated, has an `oom_score_adj` of 988. That is quite dangerous since it is the process with the highest score to get killed. But there is a simple way to stop the OOM killer from killing a specific process. When you create a Pod, it is assigned a Quality of Service (QoS) class. For detailed information, see [Configure Quality of Service for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/).\\n\\nGenerally, if you create a Pod with precisely-specified resource requests, it is classified as a `Guaranteed` Pod. OOM killers do not kill containers in a `Guaranteed` Pod if there are other things to kill. These entities include non-`Guaranteed` Pods and stress-ng workers. A Pod with no resource requests is marked as `BestEffort`, and the OOM killer stops it first.\\n\\nSo that\'s all for the tour. Our suggestion is that `free` and `top` should not be used to assess memory in containers. Be careful when you assign resource limits to your Pod and select the right QoS. In the future, we\u2019ll create a more detailed StressChaos document.\\n\\n## Deeper dive into Kubernetes memory management\\n\\nKubernetes tries to evict Pods that use too much memory (but not more memory than their limits). Kubernetes gets your Pod memory usage from `/sys/fs/cgroup/memory/memory.usage_in_bytes` and subtracts it by the `total_inactive_file` line in `memory.stat`.\\n\\nKeep in mind that Kuberenetes **does not** support swap. Even if you have a node with swap enabled, Kubernetes creates containers with `swappiness=0`, which means swap is eventually disabled. That is mainly for performance concerns.\\n\\n`memory.usage_in_bytes` equals `resident set` plus `cache`, and `total_inactive_file` is memory in cache that the OS can retrieve if the memory is running out. `memory.usage_in_bytes - total_inactive_file` is called `working_set`. You will get this `working_set` value by `kubectl top pod <your pod> --containers`. Kubernetes uses this value to decide whether or not to evict your Pods.\\n\\nKubernetes periodically inspects memory usage. If a container\'s memory usage increases too quickly or the container cannot be evicted, the OOM killer is invoked. Kubernetes has its way of protecting its own process, so it always picks the container. When a container is killed, it may or may not be restarted, depending on your restart policy. If it is killed, when you execute `kubectl describe pod <your pod>` you will see it is restarted and the reason is `OOMKilled`.\\n\\nAnother thing worth mentioning is the kernel memory. Since `v1.9`, Kubernetes\u2019 kernel memory support is enabled by default. It is also a feature of cgroup memory subsystems. You can limit container kernel memory usage. Unfortunately, this causes a cgroup leak on kernel versions up to `v4.2`. You can either upgrade your kernel to `v4.3` or disable it.\\n\\n## How we implement StressChaos\\n\\nStressChaos is a simple way to test your container\'s behavior when it is low on memory. StressChaos utilizes a powerful tool named `stress-ng` to allocate memory and continue writing to the allocated memory. Because containers have memory limits and container limits are bound to a cgroup, we must find a way to run `stress-ng` in a specific cgroup. Luckily, this part is easy. With enough privileges, we can assign any process to any cgroup by writing to files in `/sys/fs/cgroup/`.\\n\\nIf you are interested in Chaos Mesh and would like to help us improve it, you\'re welcome to join our [Slack channel](https://slack.cncf.io/) (#project-chaos-mesh)! Or submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/chaos-mesh-remake-one-step-closer-towards-chaos-as-a-service","metadata":{"permalink":"/zh/blog/chaos-mesh-remake-one-step-closer-towards-chaos-as-a-service","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-06-25-chaos-mesh-remake-one-step-closer-toward-chaos-as-a-service.md","source":"@site/blog/2021-06-25-chaos-mesh-remake-one-step-closer-toward-chaos-as-a-service.md","title":"Chaos Mesh Remake: One Step Closer toward Chaos as a Service","description":"Chaos engineering tools","date":"2021-06-25T00:00:00.000Z","formattedDate":"2021\u5e746\u670825\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":10.105,"truncated":true,"authors":[{"name":"Chang Yu, Xiang Wang","title":"Contributor of Chaos Mesh","url":"https://github.com/chaos-mesh/chaos-mesh/blob/master/MAINTAINERS.md","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"How to efficiently stress test Pod memory","permalink":"/zh/blog/how-to-efficiently-stress-test-pod-memory"},"nextItem":{"title":"From a Newbie in Software Engineering to a Graduated LFX-Mentee","permalink":"/zh/blog/lfx-mentorship-enriching-awschaos"}},"content":"![Chaos engineering tools](/img/chaos-engineering-tools-as-a-service.jpeg)\\n\\n[Chaos Mesh](https://chaos-mesh.org/) is a cloud-native Chaos Engineering platform that orchestrates chaos in Kubernetes environments. With Chaos Mesh, you can test your system\'s resilience and robustness on Kubernetes by injecting all types of faults into Pods, network, file system, and even the kernel.\\n\\n\x3c!--truncate--\x3e\\n\\nSince it was open-sourced and accepted by the Cloud Native Computing Foundation (CNCF) as a sandbox project, Chaos Mesh has attracted contributors worldwide and helped users test their systems. Yet it still has a lot of room for improvement:\\n\\n- It needs to improve usability. Some features are complicated to use. For example, when you apply a chaos experiment, you often have to manually check whether the experiment has started.\\n- It is mostly for Kubernetes environments. Because Chaos Mesh can\'t manage multiple Kubernetes clusters, you need to deploy Chaos Mesh for each Kubernetes cluster. Though [chaosd](https://github.com/chaos-mesh/chaosd) supports running chaos experiments on physical machines, the features are quite limited, and command line usage is not user friendly.\\n- It doesn\'t allow plugins. To apply a customized chaos experiment, you have to alter the source code. Moreover, Chaos Mesh only supports Golang.\\n\\nAdmittedly, Chaos Mesh is a first-rate Chaos Engineering platform, but is still a long way from offering Chaos as a Service (CaaS). Therefore, at [TiDB Hackathon 2020](https://pingcap.com/community/events/hackathon2020/), **we made changes to Chaos Mesh\'s architecture, moving it one step closer toward CaaS**.\\n\\nIn this article, I\'ll talk about what CaaS is, how we achieve it with Chaos Mesh, and our plans and lessons learned. I hope you find our experience helpful in building your own Chaos Engineering system.\\n\\n## What is Chaos as a Service?\\n\\nAs Matt Fornaciari, co-founder of Gremlin, [puts it](https://jaxenter.com/chaos-engineering-service-144113.html), CaaS \\"means you will get an intuitive UI, customer support, out-of-the-box integrations, and everything else you need to get experimenting in a matter of minutes.\\"\\n\\nFrom our perspective, CaaS should offer:\\n\\n- A unified console for management, where you can edit the configuration and create chaos experiments.\\n- Visualized metrics for you to see the experiment status.\\n- Operations to pause or archive experiments.\\n- Simple interaction. You can easily drag and drop the objects to orchestrate your experiments.\\n\\nSome companies already adapted Chaos Mesh to meet their own needs, such as [NetEase Fuxi AI Lab](https://pingcap.com/blog/how-a-top-game-company-uses-chaos-engineering-to-improve-testing) and FreeWheel, making it a mock-up for CaaS.\\n\\n## Developing Chaos Mesh towards CaaS\\n\\nBased on our understanding of CaaS, we refined the architecture of Chaos Mesh during Hackathon, including improved support for different systems and better observability. You can check out our code in [wuntun/chaos-mesh](https://github.com/wuntun/chaos-mesh/tree/caas) and [wuntun/chaosd](https://github.com/wuntun/chaosd/tree/caas).\\n\\n### Refactor Chaos Dashboard\\n\\nThe current Chaos Mesh architecture is suited for individual Kubernetes clusters. Chaos Dashboard, the web UI, is bound to a specified Kubernetes environment:\\n\\n![Chaos Mesh architecture](/img/chaos-mesh-remake-architecture.jpeg)\\n\\n<p className=\\"caption-center\\">The current Chaos Mesh architecture</p>\\n\\nDuring this refactor, **to allow Chaos Dashboard to manage multiple Kubernetes clusters, we separate Chaos Dashboard from the main architecture**. Now, if you deploy Chaos Dashboard outside of the Kubernetes cluster, you can add the cluster to Chaos Dashboard via the web UI. If you deploy Chaos Dashboard inside the cluster, it automatically obtains the cluster information through environment variables.\\n\\nYou can register Chaos Mesh (technically, the Kubernetes configuration) in Chaos Dashboard or ask `chaos-controller-manager` to report to Chaos Dashboard via configuration. Chaos Dashboard and `chaos-controller-manager` interact via CustomResourceDefinitions (CRDs). When `chaos-controller-manager` finds a Chaos Mesh CRD event, it invokes `chaos-daemon` to carry out the related chaos experiment. Therefore, Chaos Dashboard can manage experiments by operating on CRDs.\\n\\n### Refactor chaosd\\n\\nchaosd is a toolkit for running chaos experiments on physical machines. Previously, it was only a command line tool and had limited features.\\n\\n![chaosd, a Chaos Engineering command line tool](/img/chaosd-chaos-engineering-command-line-tool.jpeg)\\n\\n<p className=\\"caption-center\\">Previously, chaosd was a command line tool</p>\\n\\nDuring the refactoring, **we enabled chaosd to support the RESTful API and enhanced its services so that it can configure chaos experiments by parsing CRD-format JSON or YAML files**.\\n\\nNow, chaosd can register itself to Chaos Dashboard via configuration and send regular heartbeats to Chaos Dashboard. With the heartbeat signals, Chaos Dashboard can manage the chaosd node status. You can also add chaosd nodes to Chaos Dashboard via the web UI.\\n\\nMoreover, **chaosd can now schedule chaos experiments at specified time and manage experiment lifecycles, which unifies the user experience on Kubernetes and on physical machines**.\\n\\nWith new Chaos Dashboard and chaosd, the optimized architecture of Chaos Mesh is as follows:\\n\\n![Chaos Mesh\'s optimized architecture](/img/chaos-mesh-optimized-architecture.jpeg)\\n\\n<p className=\\"caption-center\\">Chaos Mesh\'s optimized architecture</p>\\n\\n### Improve observability\\n\\nAnother improvement is observability, namely how to tell if an experiment is carried out successfully.\\n\\nBefore the improvement, you had to manually check the experiment metrics. If you injected [StressChaos](https://chaos-mesh.org/docs/1.2.4/chaos_experiments/stresschaos) into a Pod, you had to enter the Pod to see if there was a `stress-ng` process and then use `top` commands to check CPU and memory utilization. These metrics told you whether your StressChaos experiment was created successfully.\\n\\nTo streamline the process, we now integrate `node_exporter` into `chaos-daemon` and chaosd to collect node metrics. We also deploy `kube-state-metrics` in the Kubernetes cluster, combined with cadvisor, to collect Kubernetes metrics. The collected metrics are saved and visualized by Prometheus and Grafana, which provide a simple method for you to check the experiment status.\\n\\n#### Further improvements needed\\n\\nOverall, metrics aim to help you:\\n\\n- Confirm that chaos is injected.\\n- Observe the chaos impact on the service and make periodic analysis.\\n- Respond to exceptional chaos events.\\n\\nTo achieve these goals, the system needs to monitor the experiment data metrics, the ordinary metrics, and the experiment events. Chaos Mesh still needs to improve:\\n\\n- Experiment data metrics, such as the exact latency duration of the injected network latency and the specific load of the simulated workload.\\n- Experiment events; that is, the Kubernetes events of creating, deleting, and running experiments.\\n\\nHere is a good example of metrics from [Litmus](https://github.com/litmuschaos/chaos-exporter#example-metrics).\\n\\n## Other proposals for Chaos Mesh\\n\\nBecause of the limited time at Hackathon, we didn\'t finish all our plans. Here are some of our proposals for the Chaos Mesh community to consider in the future.\\n\\n### Orchestration\\n\\nA closed loop of Chaos Engineering includes four steps: exploring chaos, discovering deficiencies in the system, analyzing root causes, and sending feedback for improvement.\\n\\n![A closed loop of Chaos Engineering](/img/closed-loop-of-chaos-engineering.jpeg)\\n\\n<p className=\\"caption-center\\">A closed loop of Chaos Engineering</p>\\n\\nHowever, **most of the current open source Chaos Engineering tools only focus on exploration and do not provide pragmatic feedback.** Based on the improved observability component, we can monitor chaos experiments in real time and compare and analyze the experiment results.\\n\\nWith these results, we will be able to realize a closed loop by adding another important component: orchestration. The Chaos Mesh community already proposed a [Workflow](https://github.com/chaos-mesh/rfcs/pull/10/files) feature, which enables you to easily orchestrate and call back chaos experiments or conveniently integrate Chaos Mesh with other systems. You can run chaos experiments in the CI/CD phase or after a canary release.\\n\\n**Combining observability and orchestration makes a closed feedback loop for Chaos Engineering.** If you were to launch a 100 ms network latency test on a Pod, you could observe the latency change using the observability component and check if the Pod service is still available using PromQL or other DSL based on orchestration. If the service was unavailable, you may conclude that the service is unavailable when the latency is >= 100 ms.\\n\\nBut 100 ms is not the threshold of your service; you need to know what is the largest latency your service can handle. By orchestrating the value of the chaos experiment, you\'ll know what is the threshold value you must ensure to meet your service-level objectives. Also, you\'ll find out the service performance under different network conditions and whether they meet your expectations.\\n\\n### Data format\\n\\nChaos Mesh uses CRDs to define its chaos objects. If we can convert CRDs to JSON files, we can achieve communication between components.\\n\\nIn terms of data format, chaosd just consumes and registers CRD data in JSON format. If a chaos tool can consume CRD data and register itself, it can run chaos experiments in different scenarios.\\n\\n### Plugins\\n\\nChaos Mesh has limited support for plugins. You can only [add a new Chaos](https://chaos-mesh.org/docs/1.2.4/development_guides/develop_a_new_chaos/) by registering a CRD in Kubernetes API. This brings about two problems:\\n\\n- You must develop the plugin using Golang, the same language in which Chaos Mesh is written.\\n- You must merge the extended code into the Chaos Mesh project. Because Chaos Mesh doesn\'t have a security mechanism like Berkeley Packet Filter (BPF), merging plugin code may introduce extra risks.\\n\\nTo enable full plugin support, we need to explore a new method to add plugins. As Chaos Mesh essentially carries out chaos experiments based on CRD, a chaos experiment only requires generating, listening to, and deleting CRDs. In this regard, we have several ideas worth trying:\\n\\n- Develop a controller or operator to manage CRDs.\\n- Handle CRD events uniformly and operate on CRDs via HTTP callback. This method only uses HTTP APIs, with no requirement on Golang. For an example, see [Whitebox Controller](https://github.com/summerwind/whitebox-controller).\\n- Use WebAssembly (Wasm). When you need to call chaos experiment logic, just call the Wasm program. See Vector\'s [WASM Transform](https://vector.dev/docs/reference/transforms/wasm/).\\n- Use SQL to query the chaos experiment status. Because Chaos Mesh is based on CRDs, you can use SQL to operate on Kubernetes. Examples include [Presto connector](https://github.com/xuxinkun/kubesql) and [osquery extension](https://github.com/aquasecurity/kube-query).\\n- Use SDK-based extensions, such as [Chaos Toolkit](https://docs.chaostoolkit.org/reference/api/experiment/).\\n\\n### Integration with other Chaos tools\\n\\nFor real-world systems, a single Chaos Engineering tool can hardly exhaust all possible use cases. That\'s why integrating with other chaos tools can make the Chaos Engineering ecosystem more powerful.\\n\\nThere are numerous Chaos Engineering tools on the market. Litmus\'s [Kubernetes implementation](https://github.com/litmuschaos/litmus-go/tree/master/chaoslib/powerfulseal) is based on [PowerfulSeal](https://github.com/powerfulseal/powerfulseal), while its [container implementation](https://github.com/litmuschaos/litmus-go/tree/master/chaoslib/pumba) is based on [Pumba](https://github.com/alexei-led/pumba). [Kraken](https://github.com/cloud-bulldozer/kraken) focuses on Kubernetes, [AWSSSMChaosRunner](https://github.com/amzn/awsssmchaosrunner) focuses on AWS, and [Toxiproxy](https://github.com/shopify/toxiproxy) targets TCP. There are also merging projects based on [Envoy](https://docs.google.com/presentation/d/1gMlmXqH6ufnb8eNO10WqVjqrPRGAO5-1S1zjcGo1Zr4/edit#slide=id.g58453c664c_2_75) and Istio.\\n\\nTo manage the various chaos tools, we may need a uniform pattern, such as [Chaos Hub](https://hub.litmuschaos.io/).\\n\\n## Voices from the community\\n\\nHere, we\'d like to share how a leading cyber security company in China as well as a Chaos Mesh user, adapts Chaos Mesh to meet their needs. Their adaptation has three aspects: physical node, container, and application.\\n\\n### Physical node\\n\\n- Support executing scripts on physical servers. You can configure the script directory in CRDs and run your scripts using `chaos-daemon`.\\n- Simulate reboot, shutdown, and kernel panic using the customized script.\\n- Shut down the node\'s NIC using the customized script.\\n- Create frequent context switching using sysbench to simulate the \\"noisy neighbor\\" effect.\\n- Intercept the container\'s system call using BPF\'s `seccomp`. This is achieved by passing and filtering PIDs.\\n\\n### Container\\n\\n- Randomly change the number of Deployment replicas to test if the application\'s traffic is abnormal.\\n- Embed based on CRD objects: fill Ingress objects in chaos CRDs to simulate the speed limit of the interface.\\n- Embed based on CRD objects: fill Cilium network policy objects in chaos CRDs to simulate fluctuating network conditions.\\n\\n### Application\\n\\n- Support running customized jobs. Currently, Chaos Mesh injects chaos using `chaos-daemon`, which doesn\'t guarantee fairness and affinity of scheduling. To address this issue, we can use `chaos-controller-manager` to directly create jobs for different CRDs.\\n- Support running [Newman](https://github.com/postmanlabs/newman) in customized jobs to randomly change HTTP parameters. This is to implement chaos experiments on the HTTP interface, which happens when a user performs exceptional behaviors.\\n\\n## Summary\\n\\nTraditional fault testing targets specific points in the system that are anticipated to be vulnerable. It is often an assertion: a specific condition produces a specific result.\\n\\n**Chaos Engineering is more powerful in that it helps you discover the \\"unknown unknowns.\\"** By exploring in the broader domain, Chaos Engineering deepens your knowledge of the system being tested and unearths new information.\\n\\nTo sum up, these are some of our personal thoughts and practice on Chaos Engineering and Chaos Mesh. Our Hackathon project is not ready for production yet, but we hope to shed some light on CaaS and draft a promising roadmap for Chaos Mesh. If you\'re interested in building Chaos as a Service, [join our Slack](https://slack.cncf.io/) (#project-chaos-mesh)!"},{"id":"/lfx-mentorship-enriching-awschaos","metadata":{"permalink":"/zh/blog/lfx-mentorship-enriching-awschaos","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-06-20-lfx-mentorship-enriching-awschaos.md","source":"@site/blog/2021-06-20-lfx-mentorship-enriching-awschaos.md","title":"From a Newbie in Software Engineering to a Graduated LFX-Mentee","description":"LFX Mentorship Experience","date":"2021-06-20T00:00:00.000Z","formattedDate":"2021\u5e746\u670820\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"LFX Mentorship","permalink":"/zh/blog/tags/lfx-mentorship"},{"label":"AWS Chaos","permalink":"/zh/blog/tags/aws-chaos"}],"readingTime":4.185,"truncated":true,"authors":[{"name":"Debabrata Panigrahi","title":"LFX Mentee at Chaos Mesh","url":"https://github.com/Debanitrkl","imageURL":"https://avatars.githubusercontent.com/u/50622005?v=4"}],"prevItem":{"title":"Chaos Mesh Remake: One Step Closer toward Chaos as a Service","permalink":"/zh/blog/chaos-mesh-remake-one-step-closer-towards-chaos-as-a-service"},"nextItem":{"title":"Celebrating One Year of Chaos Mesh: Looking Back and Ahead","permalink":"/zh/blog/celebrating-one-year-of-chaos-mesh-looking-back-and-ahead"}},"content":"![LFX Mentorship Experience](/img/mentorship_blog.jpeg)\\n\\n[I\u2019m](https://mentorship.lfx.linuxfoundation.org/mentee/6a0bf7de-9e18-4acb-9a66-f5fecdbeb42e) a junior undergraduate majoring in Biomedical Engineering in the Department of Biotechnology and Medical Engineering at the [National Institute of Technology Rourkela](https://nitrkl.ac.in/), India. For someone who started to code only because I was fascinated by it, it was all a journey of self-learning, filled with various adversities. But when I started with open-source contributions, it was all very beginner-friendly and I came across a lot of people who helped me learn the tech stack better.\\n\\n\x3c!--truncate--\x3e\\n\\n![img1](/img/mentroship_blog1.png)\\n\\n## The journey through the application\\n\\nIn the spring of 2021, I got to know about this LFX mentorship program and after browsing through all the [projects](https://github.com/cncf/mentoring/blob/master/lfx-mentorship/2021/01-Spring/README.md), it felt quite intimidating to me as I wasn\u2019t acquainted with most of the terms and was confused, and I thought it was not for newbies like me. Then I went through the program [docs](https://docs.linuxfoundation.org/lfx/mentorship), the mentorship [FAQ\u2019s](https://docs.linuxfoundation.org/lfx/mentorship/mentorship-faqs) followed the steps mentioned there and applied for a few projects that interested me, and used tech-stacks that I am familiar with, like Docker, AWS, Python, etc.\\n\\nThen I applied to both projects offered by [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) and submitted my CV and cover letter as immediate tasks. After a few days, I received an email from my mentor regarding an additional task to be submitted.\\n\\n![img2](/img/mentorship_blog2.png)\\n\\nI completed the above-mentioned task, uploaded the files to GitHub, and shared the link with my mentor.\\n\\n## The selection and Initial days as a mentee\\n\\nI distinctly remember the day when I received an email from my mentor regarding my selection in the mentorship program. I was elated, as it was my first involvement in any open-source program. I was glad to be accepted as a mentee in the program, I even received an email from CNCF regarding my selection.\\n\\n![img3](/img/mentorship_blog4.png)\\n\\nAlong with my mentor, we decided on our mode of communication: through Slack. He also enquired about my knowledge of Kubernetes and GOlang, as I didn\u2019t have much knowledge about either of them. He suggested a few resources and gave me 2 weeks to go through them. In the meantime, he also planned a few experiments for me to get acquainted with all these technologies.\\n\\nAs I was getting more comfortable with Kubernetes, I started exploring Chaos Mesh and completed the interactive [tutorial](https://chaos-mesh.org/interactive-tutorial), which gave me a clearer idea about the usage of Chaos Mesh. I then implemented the [hello-world chaos](https://chaos-mesh.org/docs/1.2.4/development_guides/develop_a_new_chaos), which helped me to know more about controllers and CRDs, considered to be the most important part of Chaos Mesh. Also, I got to know about the boilerplate codes, the [kube-builder client](https://github.com/kubernetes-sigs/kubebuilder), and how to use them for scaffolding, followed by writing our own controllers.\\n\\nAfter the initial days of experimenting and getting to know the project better, I started with solving a few good first issues to get acquainted with upstream contributions to Chaos Mesh.\\n\\n![img4](/img/mentorship_blog3.png)\\n\\nIn one of my contributions, I tried to add multi-container support to stress-chaos, which was not possible before. Though it was successfully implemented, it broke a few other features and couldn\u2019t be merged for the upcoming release. What\u2019s more, for the 2.0.0 release, this refactoring was already done, so this particular contribution was a learning experience for both me and my mentor. After that, we became careful and the next time we tried to implement any new features, we would first submit an [RFC](https://github.com/chaos-mesh/rfcs) and have discussions with the other contributors before starting.\\n\\n## My contribution to AWS Chaos\\n\\nInitially, I was asked to implement one type of AWS Chaos as part of this project, but as I started exploring more about it, I found [awsssmchaosrunner](https://github.com/amzn/awsssmchaosrunner), and given its functionality, we wanted to integrate it into Chaos Mesh.\\n\\nWe planned to do it in two parts, one part is the \u201c[runner thing](https://github.com/STRRL/awsssmchaosrunner-cli)\u201d project, which integrates with awsssmchaosrunner, that part should be written in kotlin, and a docker image is to be built out of it.\\n\\nAnother part is the definition of the AWS Chaos and its [controller](https://github.com/chaos-mesh/chaos-mesh/pull/1919), which is to be written in go, the controller of AWS Chaos will create a pod with that \u201ckotlin cli image\u201d, and send commands to AWS.\\n\\n## Other opportunities\\n\\nI was invited to one of the Chaos Mesh [community meetings](https://www.youtube.com/watch?v=ElG0pHRoXwI&t=2s) towards the end of the mentorship where I showcased my project.\\n\\nAfterwards, I applied for the CFP for [Kubernetes Community Days Bangalore](https://community.cncf.io/events/details/cncf-kcd-bengaluru-presents-kubernetes-community-days-bengaluru/), scheduled virtually from June 25\u201326, 2021, and was selected as a speaker and now I\u2019m all set to present my talk there.\\n\\n## Graduation and Next steps\\n\\nYayyyy!! After 12 weeks, I successfully graduated from the program, thanks to my mentor [Zhou Zhiqiang](https://mentorship.lfx.linuxfoundation.org/mentor/e78b3177-160c-4566-9f3d-8fc9b2ec3cea) and his guidance, because without whom, this wouldn\u2019t have been possible.\\n\\nI had an amazing time with the Chaos Mesh community, with the amazing members supporting and helping me throughout the journey. I look forward to contributing more to this project and being more active in the community.\\n\\n## Join the Chaos Mesh community\\n\\nTo join and learn more about Chaos Mesh, find the #project-chaos-mesh channel in [CNCF slack workspace](https://slack.cncf.io/) or their [GitHub](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/celebrating-one-year-of-chaos-mesh-looking-back-and-ahead","metadata":{"permalink":"/zh/blog/celebrating-one-year-of-chaos-mesh-looking-back-and-ahead","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-02-05-celebrating-one-year-of-chaos-mesh-looking-back-and-ahead.md","source":"@site/blog/2021-02-05-celebrating-one-year-of-chaos-mesh-looking-back-and-ahead.md","title":"Celebrating One Year of Chaos Mesh: Looking Back and Ahead","description":"Celebrating One Year of Chaos Mesh: Looking Back and Ahead","date":"2021-02-05T00:00:00.000Z","formattedDate":"2021\u5e742\u67085\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":7.055,"truncated":true,"authors":[{"name":"Cwen Yin, Calvin Weng","title":"Maintainer of Chaos Mesh","url":"https://github.com/chaos-mesh/chaos-mesh/blob/master/MAINTAINERS.md","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"From a Newbie in Software Engineering to a Graduated LFX-Mentee","permalink":"/zh/blog/lfx-mentorship-enriching-awschaos"},"nextItem":{"title":"How to Simulate I/O Faults at Runtime","permalink":"/zh/blog/how-to-simulate-io-faults-at-runtime"}},"content":"![Celebrating One Year of Chaos Mesh: Looking Back and Ahead](/img/celebrating-one-year-of-chaos-mesh-looking-back-and-ahead.jpg)\\n\\nIt\u2019s been a year since Chaos Mesh was first open-sourced on GitHub. Chaos Mesh started out as a mere fault injection tool and is now heading towards the goal of building a chaos engineering ecology. Meanwhile, the Chaos Mesh community was also built from scratch and has helped [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) join CNCF as a Sandbox project.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this article, we will share with you how Chaos Mesh has grown and changed in the past year, and also discuss its future goals and plans.\\n\\n## The project: thrive with a clear goal in mind\\n\\nIn this past year, Chaos Mesh has grown at an impressive speed with the joint efforts of the community. From the very first version to the recently released [v1.1.0](https://github.com/chaos-mesh/chaos-mesh/releases/tag/v1.1.0), Chaos Mesh has been greatly improved in terms of functionality, ease of use, and security.\\n\\n### Functionality\\n\\nWhen first open-sourced, Chaos Mesh supported only three fault types: PodChaos, NetworkChaos, and IOChaos. Within only a year, Chaos Mesh can perform all around fault injections into the network, system clock, JVM applications, filesystems, operating systems, and so on.\\n\\n![Chaos Tests](/img/chaos-tests.png)\\n\\nAfter continuous optimization, Chaos Mesh now provides a flexible scheduling mechanism, which enables users to better design their own chaos experiments. This laid the foundation for chaos orchestration.\\n\\nIn the meantime, we are happy to see that a number of users have started to [test Chaos Mesh on major cloud platforms](https://github.com/chaos-mesh/chaos-mesh/issues/1182), such as Amazon Web Services (AWS), Google Kubernetes Engine (GKE), Alibaba Cloud, and Tencent Cloud. We have continuously conducted compatibility testing and adaptations, in order to support [fault injection for specific cloud platforms](https://github.com/chaos-mesh/chaos-mesh/pull/1330).\\n\\nTo better support Kubernetes native components and node-level failures, we developed [Chaosd](https://github.com/chaos-mesh/chaosd), which provides physical node-level fault injection. We\'re extensively testing and refining this feature for release within the next few months.\\n\\n### Ease of use\\n\\nEase of use has been one of the guiding principles of Chaos Mesh development since day one. You can deploy Chaos Mesh with a single command line. The V1.0 release brought the long-awaited Chaos Dashboard, a one-stop web interface for users to orchestrate chaos experiments. You can define the scope of the chaos experiment, specify the type of chaos injection, define scheduling rules, and observe the results of the chaos experiment\u2014all in the same web interface with only a few clicks.\\n\\n![Chaos Dashboard](/img/chaos-dashboard1.png)\\n\\nPrior to V1.0, many users reported being blocked by various configuration problems when injecting IOChaos faults. After intense investigations and discussions, we gave up the original SideCar implementation. Instead, we used chaos-daemon to dynamically invade the target Pod, which significantly simplifies the logic. This optimization has made dynamic I/O fault injection possible with Chaos Mesh, and users can focus solely on their experiments without having to worry about additional configurations.\\n\\n### Security\\n\\nWe have improved the security of Chaos Mesh. It now provides a comprehensive set of selectors to control the scope of the experiments, and supports setting specific namespaces to protect important applications. What\u2019s more, the support of namespace permissions allows users to limit the \u201cexplosion radius\u201d of a chaos experiment to a specific namespace.\\n\\nIn addition, Chaos Mesh directly reuses Kubernetes\u2019 native permission mechanism and supports verification on the Chaos Dashboard. This protects you from other users\u2019 errors, which can cause chaos experiments to fail or become uncontrollable.\\n\\n## Cloud native ecosystem: integrations and cooperations\\n\\nIn July 2020, Chaos Mesh was successfully [accepted as a CNCF Sandbox project](https://chaos-mesh.org/blog/chaos-mesh-join-cncf-sandbox-project). This shows that Chaos Mesh has received initial recognition from the cloud native community. At the same time, it means that Chaos Mesh has a clear mission: to promote the application of chaos engineering in the cloud native field and to cooperate with other cloud native projects so we can grow together.\\n\\n### Grafana\\n\\nTo further improve the observability of chaos experiments, we have included a separate [Grafana plug-in](https://github.com/chaos-mesh/chaos-mesh-datasource) for Chaos Mesh, which allows users to directly display real-time chaos experiment information on the application monitoring panel. This way, users can simultaneously observe the running status of the application and the current chaos experiment information.\\n\\n### GitHub Action\\n\\nTo enable users to run chaos experiments even during the development phase, we developed the [chaos-mesh-action](https://github.com/chaos-mesh/chaos-mesh-action) project, allowing Chaos Mesh to run in the workflow of GitHub Actions. This way, Chaos Mesh can easily be integrated into daily system development and testing.\\n\\n### TiPocket\\n\\n[TiPocket](https://github.com/pingcap/tipocket) is an automated test platform that integrates Chaos Mesh and Argo, a workflow engine designed for Kubernetes. TiPocket is designed to be a fully automated chaos engineering testing loop for TiDB, a distributed database. There are a number of steps when we conduct chaos experiments, including deploying applications, running workloads, injecting exceptions, and business checks. To fully automate these steps, Argo was integrated into TiPocket. Chaos Mesh provides rich fault injection, while Argo provides flexible orchestration and scheduling.\\n\\n![TiPocket](/img/tipocket.png)\\n\\n## The community: built from the ground up\\n\\nChaos Mesh is a community-driven project, and cannot progress without an active, friendly, and open community. Since it was open-sourced, Chaos Mesh has quickly become one of the most eye-catching open-source projects in the chaos engineering world. Within a year, it has accumulated more than 3k stars on GitHub and 70+ contributors. Adopters include Tencent Cloud, XPeng Motors, Dailymotion, NetEase Fuxi Lab, JuiceFS, APISIX, and Meituan. Looking back on the past year, the Chaos Mesh community was built from scratch, and has laid the foundation for a transparent, open, friendly, and autonomous open source community.\\n\\n### Becoming part of the CNCF family\\n\\nCloud native has been in the DNA of Chaos Mesh since the very beginning. Joining CNCF was a natural choice, which marks a critical step for Chaos Mesh to becoming a vendor-neutral, open and transparent open-source community. Aside from integration within the cloud native ecosystem, joining CNCF gives Chaos Mesh:\\n\\n- More community and project exposure. Collaborations with other projects and various cloud native community activities such as Kubernetes Meetup and KubeCon have presented us great opportunities to communicate with the community. We are amazed how the high-quality content produced by the community has also played a positive and far-reaching role in promoting Chaos Mesh.\\n\\n- A more complete and open community framework. CNCF provides a rather mature framework for open-source community operations. Under CNCF\u2019s guidance, we established our basic community framework, including a Code of Conduct, Contributing Guide, and Roadmap. We\u2019ve also created our own channel, #project-chaos-mesh, under CNCF\u2019s Slack.\\n\\n### A friendly and supportive community\\n\\nThe quality of the open source community determines whether our adopters and contributors are willing to stick around and get involved in the community for the long run. In this regard, we\u2019ve been working hard on:\\n\\n- Continuously enriching documentation and optimizing its structure. So far, we have developed a complete set of documentation for different groups of audiences, including [a user guide](https://chaos-mesh.org/docs/1.2.4/user_guides/installation) and [developer guide](https://chaos-mesh.org/docs/1.2.4/development_guides/development_overview), [quick start guides](https://chaos-mesh.org/docs/1.2.4/get_started/get_started_on_kind), [use cases](https://chaos-mesh.org/docs/1.2.4/use_cases/multi_data_centers), and [a contributing guide](https://github.com/chaos-mesh/chaos-mesh/blob/master/CONTRIBUTING.md). All are constantly updated per each release.\\n\\n- Working with the community to publish blog posts, tutorials, use cases, and chaos engineering practices. So far, we\u2019ve produced 26 Chaos Mesh related articles. Among them is [an interactive tutorial](https://chaos-mesh.org/interactive-tutorial), published on O\u2019Reilly\u2019s Katakoda site. These materials make a great complement to the documentation.\\n\\n- Repurposing and amplifying videos and tutorials generated in community meetings, webinars, and meetups. Valuing and responding to community feedback and queries.\\n\\n## Looking ahead\\n\\nGoogle\u2019s recent global outage reminded us of the importance of system reliability, and it highlighted the importance of chaos engineering. Liz Rice, CNCF TOC Chair, shared [The 5 technologies to watch in 2021](https://twitter.com/CloudNativeFdn/status/1329863326428499971), and chaos engineering is on top of the list. We boldly predict that chaos engineering is about to enter a new stage in the near future. Chaos Mesh 2.0 is now in active development, and it includes community requirements such as an embedded workflow engine to support the definition and management of more flexible chaos scenarios, application state checking mechanisms, and more detailed experiments reports. Follow along through the project [roadmap](https://github.com/chaos-mesh/chaos-mesh/blob/master/ROADMAP.md).\\n\\n## Last but not least\\n\\nChaos Mesh has grown so much in the past year, yet it is still young, and we have just set sail towards our goal. In the meantime, we call for all of you to participate and help build the Chaos Engineering system ecology together!\\n\\nIf you are interested in Chaos Mesh and would like to help us improve it, you\'re welcome to join [our Slack channel](https://slack.cncf.io/) or submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/how-to-simulate-io-faults-at-runtime","metadata":{"permalink":"/zh/blog/how-to-simulate-io-faults-at-runtime","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2021-01-08-how-to-simulate-io-faults-at-runtime.md","source":"@site/blog/2021-01-08-how-to-simulate-io-faults-at-runtime.md","title":"How to Simulate I/O Faults at Runtime","description":"Chaos Engineering - How to simulate I/O faults at runtime","date":"2021-01-08T00:00:00.000Z","formattedDate":"2021\u5e741\u67088\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Fault Injection","permalink":"/zh/blog/tags/fault-injection"}],"readingTime":8.915,"truncated":true,"authors":[{"name":"Keao Yang","title":"Maintainer of Chaos Mesh","url":"https://github.com/YangKeao","imageURL":"https://avatars2.githubusercontent.com/u/5244316"}],"prevItem":{"title":"Celebrating One Year of Chaos Mesh: Looking Back and Ahead","permalink":"/zh/blog/celebrating-one-year-of-chaos-mesh-looking-back-and-ahead"},"nextItem":{"title":"How a Top Game Company Uses Chaos Engineering to Improve Testing","permalink":"/zh/blog/how-a-top-game-company-uses-chaos-engineering-to-improve-testing"}},"content":"![Chaos Engineering - How to simulate I/O faults at runtime](/img/how-to-simulate-io-faults-at-runtime.jpg)\\n\\nIn a production environment, filesystem faults might occur due to various incidents such as disk failures and administrator errors. As a Chaos Engineering platform, Chaos Mesh has supported simulating I/O faults in a filesystem ever since its early versions. By simply adding an IOChaos CustomResourceDefinition (CRD), we can watch how the filesystem fails and returns errors.\\n\\n\x3c!--truncate--\x3e\\n\\nHowever, before Chaos Mesh 1.0, this experiment was not easy and may have consumed a lot of resources. We needed to inject sidecar containers to the Pod through the mutating admission webhooks and rewrite the `ENTRYPOINT` command. Even if no fault was injected, the injected sidecar container caused a substantial amount of overhead.\\n\\nChaos Mesh 1.0 has changed all this. Now, we can use IOChaos to inject faults to a filesystem at runtime. This simplifies the process and greatly reduces system overhead. This blog post introduces how we implement the IOChaos experiment without using a sidecar.\\n\\n## I/O fault injection\\n\\nTo simulate I/O faults at runtime, we need to inject faults into a filesystem after the program starts [system calls](https://man7.org/linux/man-pages/man2/syscall.2.html) (such as reads and writes) but before the call requests arrive at the target filesystem. We can do that in one of two ways:\\n\\n- Use Berkeley Packet Filter (BPF); however, it [cannot be used to inject delay](https://github.com/iovisor/bcc/issues/2336).\\n- Add a filesystem layer called ChaosFS before the target filesystem. ChaosFS uses the target filesystem as the backend and receives requests from the operating system. The entire call link is **target program syscall** -> **Linux kernel** -> **ChaosFS** -> **target filesystem**. Because ChaosFS is customizable, we can inject delays and errors as we want. Therefore, ChaosFS is our choice.\\n\\nBut ChaosFS has several problems:\\n\\n- If ChaosFS reads and writes files in the target filesystem, we need to [mount](https://man7.org/linux/man-pages/man2/mount.2.html) ChaosFS to a different path than the target path specified in the Pod configuration. ChaosFS **cannot** be mounted to the path of the target directory.\\n- We need to mount ChaosFS **before** the target program starts running. This is because the newly-mounted ChaosFS takes effect only on files that are newly opened by the program in the target filesystem.\\n- We need to mount ChaosFS to the target containter\'s `mnt` namespace. For details, see [mount_namespaces(7) \u2014 Linux manual page](https://man7.org/linux/man-pages/man7/mount_namespaces.7.html).\\n\\nBefore Chaos Mesh 1.0, we used the [mutating admission webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) to implement IOChaos. This technique addressed the three problems lists above and allowed us to:\\n\\n- Run scripts in the target container. This action changed the target directory of the ChaosFS\'s backend filesystem (for example, from `/mnt/a` to `/mnt/a_bak`) so that we could mount ChaosFS to the target path (`/mnt/a`).\\n  Modify the command that starts the Pod. For example, we could modify the original command `/app` to `/waitfs.sh /app`.\\n- The `waitfs.sh` script kept checking whether the filesystem was successfully mounted. If it was mounted, `/app` was started.\\n- Add a new container in the Pod to run ChaosFS. This container needed to share a volume with the target container (for example, `/mnt`), and then we mounted this volume to the target directory (for example, `/mnt/a`). We also properly enabled [mount propagation](https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation) for this volume\'s mount to penetrate the share to host and then penetrate slave to the target.\\n\\nThese three approaches allowed us to inject I/O faults while the program was running. However, the injection was far from convenient:\\n\\n- We could only inject faults into a volume subdirectory, not into the entire volume. The workaround was to replace `mv` (rename) with `mount move` to move the mount point of the target volume.\\n- We had to explicitly write commands in the Pod rather than implicitly use the image commands. Otherwise, the `/waitfs.sh` script could not properly start the program after the filesystem was mounted.\\n- The corresponding container needed to have a proper configuration for mount propagation. Due to potential privacy and security issues, we **could not** modify the configuration via the mutating admission webhook.\\n- The injection configuration was troublesome. Worse still, we had to create a new Pod after the configuration was able to inject faults.\\n- We could not withdraw ChaosFS while the program was running. Even if no fault or error was injected, the performance was greatly affected.\\n\\n## Inject I/O faults without the mutating admission webhook\\n\\nWhat about cracking these tough nuts without the mutating admission webhook? Let\'s get back and think a bit about the reason why we used the mutating admission webhook to add a container in which ChaosFS runs. We do that to mount the filesystem to the target container.\\n\\nIn fact, there is another solution. Instead of adding containers to the Pod, we can first use the `setns` Linux system call to modify the namespace of the current process and then use the `mount` call to mount ChaosFS to the target container. Suppose that the filesystem to inject is `/mnt`. The new injection process is as follows:\\n\\n1. Use `setns` for the current process to enter the mnt namespace of the target container.\\n2. Execute `mount --move` to move `/mnt` to `/mnt_bak`.\\n3. Mount ChaosFS to `/mnt` and use `/mnt_bak` as the backend.\\n\\nAfter the process is finished, the target container will open, read, and write the files in `/mnt` through ChaosFS. In this way, delays or faults are injected much more easily. However, there are still two questions to answer:\\n\\n- How do you handle the files that are already opened by the target process?\\n- How do you recover the process given that we cannot unmount the filesystem when files are opened?\\n\\n### Dynamically replace file descriptors\\n\\n**ptrace solves both of the two questions above.** We can use ptrace to replace the opened file descriptors (FD) at runtime and replace the current working directory (CWD) and mmap.\\n\\n#### Use ptrace to allow a tracee to run a binary program\\n\\n[ptrace](https://man7.org/linux/man-pages/man2/ptrace.2.html) is a powerful tool that makes the target process (tracee) to run any system call or binary program. For a tracee to run the program, ptrace modifies the RIP-pointed address to the target process and adds an `int3` instruction to trigger a breakpoint. When the binary program stops, we need to restore the registers and memory.\\n\\n> **Note:**\\n>\\n> In the [x86_64 architecture](https://en.wikipedia.org/wiki/X86_assembly_language), the RIP register (also called an instruction pointer) always points to the memory address at which the next directive is run.\\n> To load the program into the target process memory spaces:\\n\\n1. Use ptrace to call mmap in the target program to allocate the needed memory.\\n2. Write the binary program to the newly allocated memory and make the RIP register point to it.\\n3. After the binary program stops, call munmap to clean up the memory section.\\n\\nAs a best practice, we often replace ptrace `POKE_TEXT` writes with `process_vm_writev` because if there is a huge amount of data to write, `process_vm_writev` performs more efficiently.\\n\\nUsing ptrace, we are able to make a process to replace its own FD. Now we only need a method to make that replacement happen. This method is the `dup2` system call.\\n\\n#### Use `dup2` to replace file descriptor\\n\\nThe signature of the `dup2` function is `int dup2(int oldfd, int newfd);`. It is used to create a copy of the old FD (`oldfd`). This copy has an FD number of `newfd`. If `newfd` already corresponds to the FD of an opened file, the FD on the file that\'s already opened is automatically closed.\\n\\nFor example, the current process opens `/var/run/__chaosfs__test__/a` whose FD is `1`. To replace this opened file with `/var/run/test/a`, this process performs the following operations:\\n\\n1. Uses the `fcntl` system call to get the `OFlags` (the parameter used by the `open` system call, such as `O_WRONLY`) of `/var/run/__chaosfs__test__/a`.\\n2. Uses the `Iseek` system call to get the current location of `seek`.\\n3. Uses the `open` system call to open `/var/run/test/a` using the same `OFlags`. Assume that the FD is `2`.\\n4. Uses `Iseek` to change the `seek` location of the newly opened FD `2`.\\n5. Uses `dup2(2, 1)` to replace the FD `1` of `/var/run/__chaosfs__test__/a` with the newly opened FD `2`.\\n6. Closes FD `2`.\\n\\nAfter the process is finished, FD `1` of the current process points to `/var/run/test/a`. So that we can inject faults, any subsequent operations on the target file go through the [Filesystem in Userspace](https://en.wikipedia.org/wiki/Filesystem_in_Userspace) (FUSE). FUSE is a software interface for Unix and Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code.\\n\\n#### Write a program to make the target process replace its own file descriptor\\n\\nThe combined functionality of ptrace and dup2 makes it possible for the tracer to make the tracee replace the opened FD by itself. Now, we need to write a binary program and make the target process run it:\\n\\n> **Note:**\\n>\\n> In the implementation above, we assume that:\\n>\\n> - The threads of the target process are POSIX threads and share the opened files.\\n> - When the target process creates threads using the `clone` function, the `CLONE_FILES` parameter is passed.\\n>\\n> Therefore, Chaos Mesh only replaces the FD of the first thread in the thread group.\\n\\n1. Write a piece of assembly code according to the two sections above and the usage of syscall directives. [Here](https://github.com/chaos-mesh/toda/blob/1d73871d8ab72b8d1eace55f5222b01957193531/src/replacer/fd_replacer.rs#L133) is an example of the assembly code.\\n2. Use an assembler to translate the code into a binary program. We use [dynasm-rs](https://github.com/CensoredUsername/dynasm-rs) as the assembler.\\n3. Use ptrace to make the target process run this program.\\n   When the program runs, the FD is replaced at runtime.\\n\\n### Overall fault injection process\\n\\nThe following diagram illustrates the overall I/O fault injection process:\\n\\n![Fault injection process](/img/fault-injection-process.jpg)\\n\\n<div style={{ margin: \'1rem 0\', fontStyle: \'italic\', textAlign: \'center\' }}> Fault injection process </div>\\n\\nIn this diagram, each horizontal line corresponds to a thread that runs in the direction of the arrows. The **Mount/Umount Filesystem** and **Replace FD** tasks are carefully arranged in sequence. Given the process above, this arrangement makes a lot of sense.\\n\\n## What\'s next\\n\\nI\'ve discussed how we implement fault injection to simulate I/O faults at runtime (see [chaos-mesh/toda](https://github.com/chaos-mesh/toda)). However, the current implementation is far from perfect:\\n\\n- Generation numbers are not supported.\\n- ioctl is not supported.\\n- Chaos Mesh does not immediately determine whether a filesystem is successfully mounted. It does so only after one second.\\n\\nIf you are interested in Chaos Mesh and would like to help us improve it, you\'re welcome to join [our Slack channel](https://slack.cncf.io/) or submit your pull requests or issues to our [GitHub repository](https://github.com/chaos-mesh/chaos-mesh).\\n\\nThis is the first post in a series on Chaos Mesh implementation. If you want to see how other types of fault injection are implemented, stay tuned."},{"id":"/how-a-top-game-company-uses-chaos-engineering-to-improve-testing","metadata":{"permalink":"/zh/blog/how-a-top-game-company-uses-chaos-engineering-to-improve-testing","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-11-26-how-a-top-game-company-uses-chaos-engineering-to-improve-testing.md","source":"@site/blog/2020-11-26-how-a-top-game-company-uses-chaos-engineering-to-improve-testing.md","title":"How a Top Game Company Uses Chaos Engineering to Improve Testing","description":"How-a-Top-Game-Company-Uses-Chaos-Engineering-to-Improve-Testing","date":"2020-11-26T00:00:00.000Z","formattedDate":"2020\u5e7411\u670826\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":3.765,"truncated":true,"authors":[{"name":"Hui Zhang @ Fuxi Lab, NetEase"}],"prevItem":{"title":"How to Simulate I/O Faults at Runtime","permalink":"/zh/blog/how-to-simulate-io-faults-at-runtime"},"nextItem":{"title":"Chaos Engineering - Breaking things Intentionally","permalink":"/zh/blog/chaos-engineering-breaking-things-intentionally"}},"content":"![How-a-Top-Game-Company-Uses-Chaos-Engineering-to-Improve-Testing](/img/fuxi-case-banner.jpg)\\n\\nNetEase Fuxi AI Lab is China\u2019s first professional game AI research institution. Researchers use our Kubernetes-based Danlu platform for algorithm development, training and tuning, and online publishing. Thanks to the integration with Kubernetes, our platform is much more efficient. However, due to Kubernetes- and microservices-related issues, we are constantly testing and improving our platform to make it more stable.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this article, I\u2019ll discuss one of our most valuable testing tools, [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh). Chaos Mesh is an open-source Chaos Engineering tool that provides a wide range of fault injections and excellent fault monitoring through its Dashboard.\\n\\n## Why Chaos Mesh\\n\\nWe started our search for a Chaos Engineering tool in 2018. We were looking for a tool with:\\n\\n- Cloud-native support. Kubernetes is practically the de facto standard for service orchestration and scheduling, and the application runtime has been fully standardized. For applications that run entirely on K8s, cloud-native support is a must for any tools that go with them.\\n\\n- Sufficient fault injection types. For stateful services, network failure simulation is particularly important. The platform must be able to simulate failures at different levels, such as Pods, network, and I/O.\\n\\n- Good observability. Knowing when a fault is injected and when it can be recovered is vital for us to tell whether there is an abnormality in the application.\\n\\n- Active community support. We want to use an open-source project that is thoroughly tested and consistently maintained. That\u2019s why we value sustained and timely community support.\\n\\n- No intrusion on existing applications, with no domain knowledge required.\\n\\n- Actual use cases for us to evaluate and build upon.\\n\\nIn 2019, when Chaos Mesh, a Chaos Engineering platform for Kubernetes was open-sourced, we found the tool we were looking for. It was still in its early stage; however, we were immediately struck with the richness of fault types it supported. This was a big advantage over other chaos engineering tools, because, to a certain degree, it determines the number of issues that we can locate in the system. We instantly realized that Chaos Mesh met our expectations in almost every way.\\n\\n![Chaos Mesh architecture](/img/chaos-mesh-architecture.png)\\n\\n## Our journey with Chaos Mesh\\n\\nChaos Mesh has helped us find several important bugs. For example, it detected a brain-split issue in [rabbitMQ](https://www.rabbitmq.com/), the open-source message-queueing software for Danlu. According to [Wikipedia](https://en.wikipedia.org/wiki/Split-brain), \u201ca split-brain condition indicates data or availability inconsistencies originating from the maintenance of two separate data sets with overlap in scope.\u201d When a rabbitMQ cluster has a brain split error, there will be data write conflicts or errors, which cause more serious problems such as data inconsistencies in the messaging service. As shown in our architecture below, when brain split happens, consumers do not function normally and keep reporting server exceptions.\\n\\n![Architecture of a RabbitMQ cluster](/img/architecture-of-a-rabbitmq-cluster.png)\\n\\nWith Chaos Mesh, we could stably reproduce this issue by injecting `pod-kill` faults into our container instances cloud.\\n\\nChaos Mesh also found several other issues including a startup failure, a join failure for crashed broker clusters, a heartbeat timeout, and a connection channel shutdown. Over time, our development team fixed these issues and greatly improved the stability of the Danlu platform.\\n\\n## A fast-growing project\\n\\nChaos Mesh is constantly updated and improved. When we first adopted it, it hadn\u2019t even reached a stable version. It didn\u2019t have a debugging or log collection tool, and the Dashboard component only applied to TiDB. The only way we could use Chaos Mesh to test other applications was to execute the YAML configuration file via `kubectl apply`.\\n\\n[Chaos Mesh 1.0](https://chaos-mesh.org/blog/chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier) fixed or improved most of these limitations. It offers more fine-grained and powerful chaos support, a generally-available Chaos Dashboard, enhanced observability, and more accurate chaos scope control. These are all driven by an open, collaborative, and vibrant community.\\n\\n![Chaos Dashboard is now generally available](/img/chaos-dashboard.gif)\\n\\n## Looking forward\\n\\nIt\u2019s amazing to see how much Chaos Mesh has grown and how much traction it\u2019s gaining. We\u2019re also happy with what we have achieved with it.\\n\\nHowever, Chaos Engineering is a big area to work on. In the future, we\u2019d like to see the following features:\\n\\n- Atomic fault injection\\n\\n- Unattended fault inject that combines customized fault types with standardized methods to validate experimental objects\\n\\n- Standard test cases for general components such as MySQL, Redis, and Kafka\\n\\nWe\u2019ve discussed these features with those who maintain Chaos Mesh, and they said these features are on the Chaos Mesh 2.0 roadmap.\\n\\nIf you are interested, join the Chaos Mesh community via [Slack](https://slack.cncf.io/) (#project-chaos-mesh) or [GitHub](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"/chaos-engineering-breaking-things-intentionally","metadata":{"permalink":"/zh/blog/chaos-engineering-breaking-things-intentionally","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-10-30-chaos-engineering-breaking-things-intentionally.md","source":"@site/blog/2020-10-30-chaos-engineering-breaking-things-intentionally.md","title":"Chaos Engineering - Breaking things Intentionally","description":"Chaos-Engineering-Breaking-things-Intentionally","date":"2020-10-30T00:00:00.000Z","formattedDate":"2020\u5e7410\u670830\u65e5","tags":[{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Open Source","permalink":"/zh/blog/tags/open-source"}],"readingTime":3.695,"truncated":true,"authors":[{"name":"Manish Dangi","url":"https://www.linkedin.com/in/manishdangi/","imageURL":"https://avatars1.githubusercontent.com/u/43807816?s=400"}],"prevItem":{"title":"How a Top Game Company Uses Chaos Engineering to Improve Testing","permalink":"/zh/blog/how-a-top-game-company-uses-chaos-engineering-to-improve-testing"},"nextItem":{"title":"Chaos Mesh X Hacktoberfest 2020 - An Invitation to Open Source","permalink":"/zh/blog/chaos-mesh-x-hacktoberfest-2020"}},"content":"![Chaos-Engineering-Breaking-things-Intentionally](/img/chaos-engineering2.png)\\n\\n\u201cNecessity is the mother of invention\u201d; similarly, Netflix is not only a platform for online media streaming. Netflix gave birth to Chaos engineering because of their necessity.\\n\\n\x3c!--truncate--\x3e\\n\\nIn 2008, Netflix [experienced a major database corruption](https://about.netflix.com/en/news/completing-the-netflix-cloud-migration). They couldn\'t deliver DVDs for three days. This encouraged Netflix engineers to think about their monolithic architecture\u2019s migration to a distributed cloud-based architecture.\\n\\nThe new distributed architecture of Netflix composed of hundreds of microservices. Migration to distributed architecture solved their single point failure problem, but it gave rise to many other complexities requiring a more reliable and fault-tolerant system. At this point, Netflix engineers came up with an innovative idea to test the system\u2019s fault tolerance without impacting customer service.\\n\\nThey created [Chaos Monkey](https://github.com/Netflix/chaosmonkey): a tool that causes random failures at different places with different intervals of time. With the development of Chaos Monkey, a new discipline arises: Chaos Engineering.\\n\\n\u201cChaos Engineering is the discipline of experimenting on a system in order to build confidence in the system\u2019s capability to withstand turbulent conditions in production.\u201d - [Principle of Chaos](https://principlesofchaos.org/)\\n\\nChaos Engineering is an approach for learning how your system behaves by applying a discipline of empirical exploration. Just as scientists conduct experiments to study physical and social phenomena, Chaos Engineering uses experiments to learn about a particular system - the systems\' reliability, stability, and capability to survive in unexpected or unstable conditions.\\n\\nWhen we have a large-scale distributed system, failures could be caused by a number of factors like application failure, infrastructure failure, dependency failure, network failure, and many more. These failures could not be all covered by traditional methods such as integration testing or unit testing, which makes Chaos Engineering a necessity:\\n\\n- To improve resiliency of the system\\n- To expose hidden threats and vulnerability of the system\\n- To figure out system weaknesses before they cause any failure in production\\n\\nLots of people think that they are not as big compared to Netflix and other tech giants; nor do they have any databases or systems of that scale.\\n\\nThey are probably right, but over the period, Chaos engineering has evolved so much that it\u2019s no longer limited to digital companies like Netflix. To ensure consistent performance and constant availability of their systems, more and more companies from different industries are implementing chaos experiments.\\n\\n## Chaos-Mesh\\n\\nTo test the resiliency and reliability of [TiDB](https://pingcap.com/products/tidb), engineers at [PingCAP](https://pingcap.com/) came up with a fantastic tool for Chaos testing called [Chaos Mesh](https://chaos-mesh.org/), a cloud-native Chaos Engineering platform that orchestrates chaos on Kubernetes environments.\\nChaos Mesh takes into account the possible faults of a distributed system, covering the pod, the network, system I/O, and the kernel.\\n\\nChaos Mesh provides many fault injection methods:\\n\\n- **clock-skew:** Simulates clock skew\\n- **container-kill:** Simulates the container being killed\\n- **cpu-burn:** Simulates CPU pressure\\n- **io-attribution-override:** Simulates file exceptions\\n- **io-fault:** Simulates file system I/O errors\\n- **io-latency:** Simulates file system I/O latency\\n- **kernel-injection:** Simulates kernel failures\\n- **memory-burn:** Simulates memory pressure\\n- **network-corrupt:** Simulates network packet corruption\\n- **network-duplication:** Simulates network packet duplication\\n- **network-latency:** Simulate network latency\\n- **network-loss:** Simulates network loss\\n- **network-partition:** Simulates network partition\\n- **pod-failure:** Simulates continuous unavailability of Kubernetes Pods\\n- **pod-kill:** Simulates the Kubernetes Pod being killed\\n\\nChaos Mesh mainly focuses on the simplicity of how all chaos tests are done quickly and easily understandable to anyone using it.\\n\\nThe recent [1.0 release](https://chaos-mesh.org/blog/chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier/) provides the general availability of Chaos Dashboard, which Chaos simplifies the complexities of chaos experiment. With a few mouse clicks, you can define the Chaos experiment\'s scope, specify the type of chaos injection, define scheduling rules, and observe the chaos experiment results- all in the dashboard of Chaos Mesh.\\n\\nIn case you want to try Chaos Mesh in your browser, checkout [Katakoda interactive tutorial](https://chaos-mesh.org/interactive-tutorial), where you can get your hands on Chaos Mesh without even deploying it. To understand the design principles and how Chaos Mesh works, read [this blog](https://chaos-mesh.org/blog/chaos_mesh_your_chaos_engineering_solution) by the project\'s maintainer, [Cwen Yin](https://www.linkedin.com/in/cwen-yin-81985318b/).\\n\\n## Join the community\\n\\nAnyone who wants to explore the area of chaos engineering or Chaos Mesh are welcomed to join the Chaos Mesh community. Being a member of the Chaos Mesh community, I would like to say it is a lovely community where project maintainers love to engage and hear your views and suggestions for the improvement of the project and the community.\\n\\nTo join and learn more about Chaos Mesh, find the #project-chaos-mesh channel in [CNCF slack workspace](https://slack.cncf.io/)."},{"id":"/chaos-mesh-x-hacktoberfest-2020","metadata":{"permalink":"/zh/blog/chaos-mesh-x-hacktoberfest-2020","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-10-14-chaos-mesh-x-hacktoberfest.md","source":"@site/blog/2020-10-14-chaos-mesh-x-hacktoberfest.md","title":"Chaos Mesh X Hacktoberfest 2020 - An Invitation to Open Source","description":"Chaos-Mesh-X-Hacktoberfest-An-Invitation-to-Open-Source","date":"2020-10-14T00:00:00.000Z","formattedDate":"2020\u5e7410\u670814\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Open Source","permalink":"/zh/blog/tags/open-source"}],"readingTime":2.445,"truncated":true,"authors":[{"name":"Chaos Mesh Community","url":"https://github.com/chaos-mesh","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"Chaos Engineering - Breaking things Intentionally","permalink":"/zh/blog/chaos-engineering-breaking-things-intentionally"},"nextItem":{"title":"Chaos Mesh 1.0: Chaos Engineering on Kubernetes Made Easier","permalink":"/zh/blog/chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier"}},"content":"![Chaos-Mesh-X-Hacktoberfest-An-Invitation-to-Open-Source](/img/chaos-mesh-x-hacktoberfest.jpg)\\n\\n[Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is proud to be in [Hacktoberfest 2020](https://hacktoberfest.digitalocean.com/)!\\n\\nHosted by DigitalOcean, Intel and DEV, Hacktoberfest is an open source celebration open to everyone in our global community. This month-long (Oct 1 - Oct 31) event encourages everyone to help drive the growth of open source and make positive contributions to an ever-growing community, whether you\u2019re an experienced developer or open-source newbie learning to code. As long as you submit 4 PRs before Oct 31, you are eligible to claim a limit edition T-shirt (70000 in total on a first-come-first-served basis)!\\n\\n\x3c!--truncate--\x3e\\n\\n![Hacktoberfest T-shirt](/img/hacktoberfest-shirt.png)\\n\\n## Open source is the spirit\\n\\nChaos Mesh has always been a dedicated and firm advocate of open source from day 1. Within only 10 months since it was open-sourced on December 31st, 2019, Chaos Mesh has received around 2.5k GitHub stars, with 59 contributors from multiple organizations. And it was accepted as a [CNCF sandbox project](https://www.cncf.io/sandbox-projects/) in July 2020. The amazing growth of the project as well as the community could not have been possible without our shared commitment to the open-source community and spirit.\\n\\nWe hereby invite you to be part of us, starting from our handpicked issues with proper mentoring and assistance along your journey, which we hope you will find rewarding, inspiring, and most of all, fun.\\n\\n## How can you participate\\n\\nSo we are all set up for you in Hacktoberfest - labeled [suitable issues](https://github.com/chaos-mesh/chaos-mesh/issues?q=is%3Aissue+is%3Aopen+label%3AHacktoberfest) with \u201cHacktoberfest\u201d, and updated the [Contributing Guide](https://github.com/chaos-mesh/chaos-mesh/blob/master/CONTRIBUTING.md).\\n\\nHow can you participate? It could not be easier with the following steps:\\n\\n1. Sign up for [Hacktoberfest](https://hacktoberfest.digitalocean.com/login) using your GitHub account between Oct 1 and Oct 31.\\n2. Pick up an issue. Note that the issues are still being updated, but you don\u2019t have to be limited to issues with the Hacktoberfest label, which only serve as a starting point.\\n3. Start coding and submit your PRs. Again the PR does not need to be corresponding to the labeled issue.\\n4. Our maintainers review your PRs. Once you successfully merged, or have gained approval for 4 or more of them, the PRs will be automatically counted on the Hacktoberfest end, and you will be eligible to claim your SWAG.\\n\\n   ![PR count](/img/PR-count.png)\\n\\n**Note:** If your PRs are merged or approved but you haven\u2019t seen the number reflected on Hacktoberfest, comment under your PR.\\n\\n## Strive for quality, learning, and no spammy\\n\\nIn the spirit of open source and Hacktoberfest, we welcome all contributions and honor only valid PRs. However, we would not encourage or tolerate spammy contributions, which would not only cause waste to our maintainer\u2019s time but also hurt the feelings and the integrity of the entire open source community. Spammy PRs will be labeled as \\"invalid\\" or \\"spam\\", and will be closed as invalid.\\n\\nHappy hacking! But don\u2019t hack alone. Join #project-chaos-mesh in the [CNCF Slack](https://slack.cncf.io/) to share your experience, provide your feedback on your experience, or let us help with any problem you have."},{"id":"/chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier","metadata":{"permalink":"/zh/blog/chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-9-25-chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier.md","source":"@site/blog/2020-9-25-chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier.md","title":"Chaos Mesh 1.0: Chaos Engineering on Kubernetes Made Easier","description":"Chaos-Mesh-1.0 - Chaos-Engineering-on-Kubernetes-Made-Easier","date":"2020-09-25T00:00:00.000Z","formattedDate":"2020\u5e749\u670825\u65e5","tags":[{"label":"Announcement","permalink":"/zh/blog/tags/announcement"},{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"}],"readingTime":2.91,"truncated":true,"authors":[{"name":"Chaos Mesh Maintainers","url":"https://github.com/chaos-mesh","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"Chaos Mesh X Hacktoberfest 2020 - An Invitation to Open Source","permalink":"/zh/blog/chaos-mesh-x-hacktoberfest-2020"},"nextItem":{"title":"chaos-mesh-action: Integrate Chaos Engineering into Your CI","permalink":"/zh/blog/chaos-mesh-action-integrate-chaos-engineering-into-your-ci"}},"content":"![Chaos-Mesh-1.0 - Chaos-Engineering-on-Kubernetes-Made-Easier](/img/chaos-mesh-1.0.png)\\n\\nToday, we are proud to announce the general availability of Chaos Mesh 1.0, following its entry into CNCF as a [sandbox project](https://pingcap.com/blog/announcing-chaos-mesh-as-a-cncf-sandbox-project) in July, 2020.\\n\\n\x3c!--truncate--\x3e\\n\\nChaos Mesh 1.0 is a major milestone in the project\u2019s development. After 10 months of effort within the open-source community, Chaos Mesh is now ready in terms of functionality, scalability, and ease of use. Here are some highlights.\\n\\n## Powerful chaos support\\n\\n[Chaos Mesh](https://chaos-mesh.org) originated in the testing framework of [TiDB](https://pingcap.com/products/tidb), a distributed database, so it takes into account the possible faults of a distributed system. Chaos Mesh provides comprehensive and fine-grained fault types, covering the Pod, the network, system I/O, and the kernel. Chaos experiments are defined in YAML, which is fast and easy to use.\\n\\nChaos Mesh 1.0 supports the following fault types:\\n\\n- clock-skew: Simulates clock skew\\n- container-kill: Simulates the container being killed\\n- cpu-burn: Simulates CPU pressure\\n- io-attribution-override: Simulates file exceptions\\n- io-fault: Simulates file system I/O errors\\n- io-latency: Simulates file system I/O latency\\n- kernel-injection: Simulates kernel failures\\n- memory-burn: Simulates memory pressure\\n- network-corrupt: Simulates network packet corruption\\n- network-duplication: Simulates network packet duplication\\n- network-latency: Simulate network latency\\n- network-loss: Simulates network loss\\n- network-partition: Simulates network partition\\n- pod-failure: Simulates continuous unavailability of Kubernetes Pods\\n- pod-kill: Simulates the Kubernetes Pod being killed\\n\\n## Visual chaos orchestration\\n\\nThe Chaos Dashboard component is a one-stop web interface for Chaos Mesh users to orchestrate chaos experiments. Previously, Chaos Dashboard was only available for testing TiDB. With Chaos Mesh 1.0, it is available to everyone. Chaos Dashboard greatly simplifies the complexity of chaos experiments. With only a few mouse clicks, you can define the scope of the chaos experiment, specify the type of chaos injection, define scheduling rules, and observe the results of the chaos experiment\u2014all in the same web interface.\\n\\n![Chaos Dashboard](/img/chaos-dashboard.gif)\\n\\n## Grafana plug-in for enhanced observability\\n\\nTo further improve the observability of chaos experiments, Chaos Mesh 1.0 includes a Grafana plug-in to allow you to directly display real-time chaos experiment information on your application monitoring panel. Currently, the chaos experiment information is displayed as annotations. This way, you can simultaneously observe the running status of the application and the current chaos experiment information.\\n\\n![Chaos status and application status on Grafana](/img/chaos-status.png)\\n\\n## Safe and controllable chaos\\n\\nWhen we conduct chaos experiments, it is vital that we keep strict control over the chaos scope or \u201cblast radius.\u201d Chaos Mesh 1.0 not only provides a wealth of selectors to accurately control the scope of the experiment, but it also enables you to set protected Namespaces to protect important applications. You can also use Namespace permissions to limit the scope of Chaos Mesh to a specific Namespace. Together, these features make chaos experiments with Chaos Mesh safe and controllable.\\n\\n## Try it out now\\n\\nYou can quickly deploy Chaos Mesh in your Kubernetes environment through the `install.sh` script or the Helm tool. For specific installation steps, please refer to the [Chaos Mesh Getting Started](https://chaos-mesh.org/docs/1.2.4/user_guides/installation) document. In addition, thanks to the [Katakoda interactive tutorial](https://chaos-mesh.org/interactive-tutorial), you can also quickly get your hands on Chaos Mesh without having to deploy it.\\n\\nIf you haven\u2019t upgraded to 1.0 GA, please refer to the [1.0 Release Notes](https://github.com/chaos-mesh/chaos-mesh/releases/tag/v1.0.0) for the changes and upgrade guidelines.\\n\\n## Thanks\\n\\nThanks to all our Chaos Mesh [contributors](https://github.com/chaos-mesh/chaos-mesh/graphs/contributors)!\\n\\nIf you are interested in Chaos Mesh, you\u2019re welcome to join us by submitting issues, or contributing code, documentation, or articles. We look forward to your participation and feedback!"},{"id":"/chaos-mesh-action-integrate-chaos-engineering-into-your-ci","metadata":{"permalink":"/zh/blog/chaos-mesh-action-integrate-chaos-engineering-into-your-ci","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-9-18-chaos-mesh-action-integrate-chaos-engineering-into-your-ci.md","source":"@site/blog/2020-9-18-chaos-mesh-action-integrate-chaos-engineering-into-your-ci.md","title":"chaos-mesh-action: Integrate Chaos Engineering into Your CI","description":"chaos-mesh-action - Integrate Chaos Engineering into Your CI","date":"2020-09-18T00:00:00.000Z","formattedDate":"2020\u5e749\u670818\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"GitHub Action","permalink":"/zh/blog/tags/git-hub-action"},{"label":"CI","permalink":"/zh/blog/tags/ci"}],"readingTime":5.38,"truncated":true,"authors":[{"name":"Xiang Wang","title":"Contributor of Chaos Mesh","url":"https://github.com/WangXiangUSTC","imageURL":"https://avatars3.githubusercontent.com/u/5793595?v=4"}],"prevItem":{"title":"Chaos Mesh 1.0: Chaos Engineering on Kubernetes Made Easier","permalink":"/zh/blog/chaos-mesh-1.0-chaos-engineering-on-kubernetes-made-easier"},"nextItem":{"title":"Building an Automated Testing Framework Based on Chaos Mesh and Argo","permalink":"/zh/blog/building_automated_testing_framework"}},"content":"![chaos-mesh-action - Integrate Chaos Engineering into Your CI](/img/chaos-mesh-action.png)\\n\\n[Chaos Mesh](https://chaos-mesh.org) is a cloud-native chaos testing platform that orchestrates chaos in Kubernetes environments. While it\u2019s well received in the community with its rich fault injection types and easy-to-use dashboard, it was difficult to use Chaos Mesh with end-to-end testing or the continuous integration (CI) process. As a result, problems introduced during system development could not be discovered before the release.\\n\\nIn this article, I will share how we use chaos-mesh-action, a GitHub action to integrate Chaos Mesh into the CI process.\\n\\n\x3c!--truncate--\x3e\\n\\nchaos-mesh-action is available on [GitHub market](https://github.com/marketplace/actions/chaos-mesh), and the source code is on [GitHub](https://github.com/chaos-mesh/chaos-mesh-action).\\n\\n## Design of chaos-mesh-action\\n\\n[GitHub Action](https://docs.github.com/en/actions) is a CI/CD feature natively supported by GitHub, through which we can easily build automated and customized software development workflows in the GitHub repository.\\n\\nCombined with GitHub actions, Chaos Mesh can be more easily integrated into the daily development and testing of the system, thus guaranteeing that each code submission on GitHub is bug-free and won\u2019t damage existing code. The following figure shows chaos-mesh-action integrated into the CI workflow:\\n\\n![chaos-mesh-action integration in the CI workflow](/img/chaos-mesh-action-integrate-in-the-ci-workflow.png)\\n\\n## Using chaos-mesh-action in GitHub workflow\\n\\n[chaos-mesh-action](https://github.com/marketplace/actions/chaos-mesh) works in Github workflows. A GitHub workflow is a configurable automated process that you can set up in your repository to build, test, package, release, or deploy any GitHub project. To integrate Chaos Mesh in your CI, do the following:\\n\\n1. Design a workflow.\\n2. Create a workflow.\\n3. Run the workflow.\\n\\n### Design a workflow\\n\\nBefore you design a workflow, you must consider the following issues:\\n\\n- What functions are we going to test in this workflow?\\n- What types of faults will we inject?\\n- How do we verify the correctness of the system?\\n\\nAs an example, let\u2019s design a simple test workflow that includes the following steps:\\n\\n1. Create two Pods in a Kubernetes cluster.\\n2. Ping one pod from the other.\\n3. Use Chaos Mesh to inject network delay chaos and test whether the ping command is affected.\\n\\n### Create the workflow\\n\\nAfter you design the workflow, the next step is to create it.\\n\\n1. Navigate to the GitHub repository that contains the software you want to test.\\n2. To start creating a workflow, click **Actions**, and then click the **New workflow** button:\\n\\n![Creating a workflow](/img/creating-a-workflow.png)\\n\\nA workflow is essentially the configuration of jobs that take place sequentially and automatically. Note that the jobs are configured in a single file. For better illustration, we split the script into different job groups as shown below:\\n\\n- Set the workflow name and trigger rules.\\n\\n  This job names the workflow \\"Chaos.\u201d When the code is pushed to the master branch or a pull request is submitted to the master branch, this workflow is triggered.\\n\\n  ```yaml\\n  name: Chaos\\n\\n  on:\\n    push:\\n      branches:\\n        - master\\n    pull_request:\\n      branches:\\n        - master\\n  ```\\n\\n- Install the CI-related environment.\\n\\n  This configuration specifies the operating system (Ubuntu), and that it uses [helm/kind-action](https://github.com/marketplace/actions/kind-cluster) to create a Kind cluster. Then, it outputs related information about the cluster. Finally, it checks out the GitHub repository for the workflow to access.\\n\\n  ```yaml\\n  jobs:\\n    build:\\n      runs-on: ubuntu-latest\\n      steps:\\n        - name: Creating kind cluster\\n          uses: helm/kind-action@v1.0.0-rc.1\\n\\n        - name: Print cluster information\\n          run: |\\n            kubectl config view\\n            kubectl cluster-info\\n            kubectl get nodes\\n            kubectl get pods -n kube-system\\n            helm version\\n            kubectl version\\n\\n        - uses: actions/checkout@v2\\n  ```\\n\\n- Deploy the application.\\n\\n  In our example, this job deploys an application that creates two Kubernetes Pods.\\n\\n  ```yaml\\n  - name: Deploy an application\\n       run: |\\n         kubectl apply -f https://raw.githubusercontent.com/chaos-mesh/apps/master/ping/busybox-statefulset.yaml\\n  ```\\n\\n- Inject chaos with chaos-mesh-action.\\n\\n  ```yaml\\n  - name: Run chaos mesh action\\n      uses: chaos-mesh/chaos-mesh-action@v0.5\\n      env:\\n        CHAOS_MESH_VERSION: v1.0.0\\n        CFG_BASE64: YXBpVmVyc2lvbjogY2hhb3MtbWVzaC5vcmcvdjFhbHBoYTEKa2luZDogTmV0d29ya0NoYW9zCm1ldGFkYXRhOgogIG5hbWU6IG5ldHdvcmstZGVsYXkKICBuYW1lc3BhY2U6IGJ1c3lib3gKc3BlYzoKICBhY3Rpb246IGRlbGF5ICMgdGhlIHNwZWNpZmljIGNoYW9zIGFjdGlvbiB0byBpbmplY3QKICBtb2RlOiBhbGwKICBzZWxlY3RvcjoKICAgIHBvZHM6CiAgICAgIGJ1c3lib3g6CiAgICAgICAgLSBidXN5Ym94LTAKICBkZWxheToKICAgIGxhdGVuY3k6ICIxMG1zIgogIGR1cmF0aW9uOiAiNXMiCiAgc2NoZWR1bGVyOgogICAgY3JvbjogIkBldmVyeSAxMHMiCiAgZGlyZWN0aW9uOiB0bwogIHRhcmdldDoKICAgIHNlbGVjdG9yOgogICAgICBwb2RzOgogICAgICAgIGJ1c3lib3g6CiAgICAgICAgICAtIGJ1c3lib3gtMQogICAgbW9kZTogYWxsCg==\\n  ```\\n\\n  With chaos-mesh-action, the installation of Chaos Mesh and the injection of chaos complete automatically. You simply need to prepare the chaos configuration that you intend to use to get its Base64 representation. Here, we want to inject network delay chaos into the Pods, so we use the original chaos configuration as follows:\\n\\n  ```yaml\\n  apiVersion: chaos-mesh.org/v1alpha1\\n  kind: NetworkChaos\\n  metadata:\\n    name: network-delay\\n    namespace: busybox\\n  spec:\\n    action: delay # the specific chaos action to inject\\n    mode: all\\n    selector:\\n      pods:\\n        busybox:\\n          - busybox-0\\n    delay:\\n      latency: \'10ms\'\\n    duration: \'5s\'\\n    scheduler:\\n      cron: \'@every 10s\'\\n    direction: to\\n    target:\\n      selector:\\n        pods:\\n          busybox:\\n            - busybox-1\\n      mode: all\\n  ```\\n\\n  You can obtain the Base64 value of the above chaos configuration file using the following command:\\n\\n  ```shell\\n  $ base64 chaos.yaml\\n  ```\\n\\n- Verify the system correctness.\\n\\n  In this job, the workflow pings one Pod from the other and observes the changes in network delay.\\n\\n  ```yaml\\n  - name: Verify\\n       run: |\\n         echo \\"do some verification\\"\\n         kubectl exec busybox-0 -it -n busybox -- ping -c 30 busybox-1.busybox.busybox.svc\\n  ```\\n\\n### Run the workflow\\n\\nNow that the workflow is configured, we can trigger it by submitting a pull request to the master branch. When the workflow completes, the verification job outputs of the results that look similar to the following:\\n\\n```shell\\ndo some verification\\nUnable to use a TTY - input is not a terminal or the right kind of file\\nPING busybox-1.busybox.busybox.svc (10.244.0.6): 56 data bytes\\n64 bytes from 10.244.0.6: seq=0 ttl=63 time=0.069 ms\\n64 bytes from 10.244.0.6: seq=1 ttl=63 time=10.136 ms\\n64 bytes from 10.244.0.6: seq=2 ttl=63 time=10.192 ms\\n64 bytes from 10.244.0.6: seq=3 ttl=63 time=10.129 ms\\n64 bytes from 10.244.0.6: seq=4 ttl=63 time=10.120 ms\\n64 bytes from 10.244.0.6: seq=5 ttl=63 time=0.070 ms\\n64 bytes from 10.244.0.6: seq=6 ttl=63 time=0.073 ms\\n64 bytes from 10.244.0.6: seq=7 ttl=63 time=0.111 ms\\n64 bytes from 10.244.0.6: seq=8 ttl=63 time=0.070 ms\\n64 bytes from 10.244.0.6: seq=9 ttl=63 time=0.077 ms\\n\u2026\u2026\\n```\\n\\nThe output indicates a regular series of 10-millisecond delays that last about 5 seconds each. This is consistent with the chaos configuration we injected into chaos-mesh-action.\\n\\n## Current status and next steps\\n\\nAt present, we have applied chaos-mesh-action to the [TiDB Operator](https://github.com/pingcap/tidb-operator) project. The workflow is injected with the Pod chaos to verify the restart function of the specified instances of the operator. The purpose is to ensure that tidb-operator can work normally when the pods of the operator are randomly deleted by the injected faults. You can view the [TiDB Operator page](https://github.com/pingcap/tidb-operator/actions?query=workflow%3Achaos) for more details.\\n\\nIn the future, we plan to apply chaos-mesh-action to more tests to ensure the stability of TiDB and related components. You are welcome to create your own workflow using chaos-mesh-action.\\n\\nIf you find a bug or think something is missing, feel free to file an issue, open a pull request (PR), or join us on the [#project-chaos-mesh](https://slack.cncf.io/) channel in the [CNCF](https://www.cncf.io/) slack workspace."},{"id":"/building_automated_testing_framework","metadata":{"permalink":"/zh/blog/building_automated_testing_framework","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-08-14-building_automated_testing_framework.md","source":"@site/blog/2020-08-14-building_automated_testing_framework.md","title":"Building an Automated Testing Framework Based on Chaos Mesh and Argo","description":"TiPocket - Automated Testing Framework","date":"2020-08-14T00:00:00.000Z","formattedDate":"2020\u5e748\u670814\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Test Automation","permalink":"/zh/blog/tags/test-automation"}],"readingTime":7.83,"truncated":true,"authors":[{"name":"Ben Ye, Chengwen Yin","title":"Maintainer of Chaos Mesh","url":"https://github.com/chaos-mesh/chaos-mesh/blob/master/MAINTAINERS.md","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"chaos-mesh-action: Integrate Chaos Engineering into Your CI","permalink":"/zh/blog/chaos-mesh-action-integrate-chaos-engineering-into-your-ci"},"nextItem":{"title":"Chaos Mesh Joins CNCF as a Sandbox Project","permalink":"/zh/blog/chaos-mesh-join-cncf-sandbox-project"}},"content":"![TiPocket - Automated Testing Framework](/img/automated_testing_framework.png)\\n\\n[Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is an open-source chaos engineering platform for Kubernetes. Although it provides rich capabilities to simulate abnormal system conditions, it still only solves a fraction of the Chaos Engineering puzzle. Besides fault injection, a full chaos engineering application consists of hypothesizing around defined steady states, running experiments in production, validating the system via test cases, and automating the testing.\\n\\nThis article describes how we use [TiPocket](https://github.com/pingcap/tipocket), an automated testing framework to build a full Chaos Engineering testing loop for TiDB, our distributed database.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why do we need TiPocket?\\n\\nBefore we can put a distributed system like [TiDB](https://github.com/pingcap/tidb) into production, we have to ensure that it is robust enough for day-to-day use. For this reason, several years ago we introduced Chaos Engineering into our testing framework. In our testing framework, we:\\n\\n1. Observe the normal metrics and develop our testing hypothesis.\\n2. Inject a list of failures into TiDB.\\n3. Run various test cases to verify TiDB in fault scenarios.\\n4. Monitor and collect test results for analysis and diagnosis.\\n\\nThis sounds like a solid process, and we\u2019ve used it for years. However, as TiDB evolves, the testing scale multiplies. We have multiple fault scenarios, against which dozens of test cases run in the Kubernetes testing cluster. Even with Chaos Mesh helping to inject failures, the remaining work can still be demanding\u2014not to mention the challenge of automating the pipeline to make the testing scalable and efficient.\\n\\nThis is why we built TiPocket, a fully-automated testing framework based on Kubernetes and Chaos Mesh. Currently, we mainly use it to test TiDB clusters. However, because of TiPocket\u2019s Kubernetes-friendly design and extensible interface, you can use Kubernetes\u2019 create and delete logic to easily support other applications.\\n\\n## How does it work\\n\\nBased on the above requirements, we need an automatic workflow that:\\n\\n- [Injects chaos](#injecting-chaos---chaos-mesh)\\n- [Verifies the impact of that chaos](#verifying-chaos-impacts-test-cases)\\n- [Automates the chaos pipeline](#automating-the-chaos-pipeline---argo)\\n- [Visualizes the results](#visualizing-the-results-loki)\\n\\n### Injecting chaos - Chaos Mesh\\n\\nFault injection is the core chaos testing. In a distributed database, faults can happen anytime, anywhere\u2014from node crashes, network partitions, and file system failures, to kernel panics. This is where Chaos Mesh comes in.\\n\\nCurrently, TiPocket supports the following types of fault injection:\\n\\n- **Network**: Simulates network partitions, random packet loss, disorder, duplication, or delay of links.\\n- **Time skew**: Simulates clock skew of the container to be tested.\\n- **Kill**: Kills the specified pod, either randomly in a cluster or within a component (TiDB, TiKV, or Placement Driver (PD)).\\n- **I/O**: Injects I/O delays in TiDB\u2019s storage engine, TiKV, to identify I/O related issues.\\n\\nWith fault injection handled, we need to think about verification. How do we make sure TiDB can survive these faults?\\n\\n## Verifying chaos impacts: test cases\\n\\nTo validate how TiDB withstands chaos, we implemented dozens of test cases in TiPocket, combined with a variety of inspection tools. To give you an overview of how TiPocket verifies TiDB in the event of failures, consider the following test cases. These cases focus on SQL execution, transaction consistency, and transaction isolation.\\n\\n### Fuzz testing: SQLsmith\\n\\n[SQLsmith](https://github.com/pingcap/tipocket/tree/master/pkg/go-sqlsmith) is a tool that generates random SQL queries. TiPocket creates a TiDB cluster and a MySQL instance.. The random SQL generated by SQLsmith is executed on TiDB and MySQL, and various faults are injected into the TiDB cluster to test. In the end, execution results are compared. If we detect inconsistencies, there are potential issues with our system.\\n\\n### Transaction consistency testing: Bank and Porcupine\\n\\n[Bank](https://github.com/pingcap/tipocket/tree/master/cmd/bank) is a classical test case that simulates the transfer process in a banking system. Under snapshot isolation, all transfers must ensure that the total amount of all accounts must be consistent at every moment, even in the face of system failures. If there are inconsistencies in the total amount, there are potential issues with our system.\\n\\n[Porcupine](https://github.com/anishathalye/porcupine) is a linearizability checker in Go built to test the correctness of distributed systems. It takes a sequential specification as executable Go code, along with a concurrent history, and it determines whether the history is linearizable with respect to the sequential specification. In TiPocket, we use the [Porcupine](https://github.com/pingcap/tipocket/tree/master/pkg/check/porcupine) checker in multiple test cases to check whether TiDB meets the linearizability constraint.\\n\\n### Transaction Isolation testing: Elle\\n\\n[Elle](https://github.com/jepsen-io/elle) is an inspection tool that verifies a database\u2019s transaction isolation level. TiPocket integrates [go-elle](https://github.com/pingcap/tipocket/tree/master/pkg/elle), the Go implementation of the Elle inspection tool, to verify TiDB\u2019s isolation level.\\n\\nThese are just a few of the test cases TiPocket uses to verify TiDB\u2019s accuracy and stability. For more test cases and verification methods, see our [source code](https://github.com/pingcap/tipocket).\\n\\n## Automating the chaos pipeline - Argo\\n\\nNow that we have Chaos Mesh to inject faults, a TiDB cluster to test, and ways to validate TiDB, how can we automate the chaos testing pipeline? Two options come to mind: we could implement the scheduling functionality in TiPocket, or hand over the job to existing open-source tools. To make TiPocket more dedicated to the testing part of our workflow, we chose the open-source tools approach. This, plus our all-in-K8s design, lead us directly to [Argo](https://github.com/argoproj/argo).\\n\\nArgo is a workflow engine designed for Kubernetes. It has been an open source product for a long time, and has received widespread attention and application.\\n\\nArgo has abstracted several custom resource definitions (CRDs) for workflows. The most important ones include Workflow Template, Workflow, and Cron Workflow. Here is how Argo fits in TiPocket:\\n\\n- **Workflow Template** is a template defined in advance for each test task. Parameters can be passed in when the test is running.\\n- **Workflow** schedules multiple workflow templates in different orders, which form the tasks to be executed. Argo also lets you add conditions, loops, and directed acyclic graphs (DAGs) in the pipeline.\\n- **Cron Workflow** lets you schedule a workflow like a cron job. It is perfectly suitable for scenarios where you want to run test tasks for a long time.\\n\\nThe sample workflow for our predefined bank test is shown below:\\n\\n```yml\\nspec:\\n  entrypoint: call-tipocket-bank\\n  arguments:\\n    parameters:\\n      - name: ns\\n        value: tipocket-bank\\n            - name: nemesis\\n        value: random_kill,kill_pd_leader_5min,partition_one,subcritical_skews,big_skews,shuffle-leader-scheduler,shuffle-region-scheduler,random-merge-scheduler\\n  templates:\\n    - name: call-tipocket-bank\\n      steps:\\n        - - name: call-wait-cluster\\n            templateRef:\\n              name: wait-cluster\\n              template: wait-cluster\\n        - - name: call-tipocket-bank\\n            templateRef:\\n              name: tipocket-bank\\n              template: tipocket-bank\\n```\\n\\nIn this example, we use the workflow template and nemesis parameters to define the specific failure to inject. You can reuse the template to define multiple workflows that suit different test cases. This allows you to add more customized failure injections in the flow.\\n\\nBesides [TiPocket\u2019s](https://github.com/pingcap/tipocket/tree/master/argo/workflow) sample workflows and templates, the design also allows you to add your own failure injection flows. Handling complicated logics using codable workflows makes Argo developer-friendly and an ideal choice for our scenarios.\\n\\nNow, our chaos experiment is running automatically. But if our results do not meet our expectations? How do we locate the problem? TiDB saves a variety of monitoring information, which makes log collecting essential for enabling observability in TiPocket.\\n\\n## Visualizing the results: Loki\\n\\nIn cloud-native systems, observability is very important. Generally speaking, you can achieve observability through **metrics**, **logging**, and **tracing**. TiPocket\u2019s main test cases evaluate TiDB clusters, so metrics and logs are our default sources for locating issues.\\n\\nOn Kubernetes, Prometheus is the de-facto standard for metrics. However, there is no common way for log collection. Solutions such as [Elasticsearch](https://en.wikipedia.org/wiki/Elasticsearch), [Fluent Bit](https://fluentbit.io/), and [Kibana](https://www.elastic.co/kibana) perform well, but they may cause system resource contention and high maintenance costs. We decided to use [Loki](https://github.com/grafana/loki), the Prometheus-like log aggregation system from [Grafana](https://grafana.com/).\\n\\nPrometheus processes TiDB\u2019s monitoring information. Prometheus and Loki have a similar labeling system, so we can easily combine Prometheus\' monitoring indicators with the corresponding pod logs and use a similar query language. Grafana also supports the Loki dashboard, which means we can use Grafana to display monitoring indicators and logs at the same time. Grafana is the built-in monitoring component in TiDB, which Loki can reuse.\\n\\n## Putting them all together - TiPocket\\n\\nNow, everything is ready. Here is a simplified diagram of TiPocket:\\n\\n![TiPocket Architecture](/img/tipocket-architecture.png)\\n\\nAs you can see, the Argo workflow manages all chaos experiments and test cases. Generally, a complete test cycle involves the following steps:\\n\\n1. Argo creates a Cron Workflow, which defines the cluster to be tested, the faults to inject, the test case, and the duration of the task. If necessary, the Cron Workflow also lets you view case logs in real-time.\\n\\n![Argo Workflow](/img/argo-workflow.png)\\n\\n1. At a specified time, a separate TiPocket thread is started in the workflow, and the Cron Workflow is triggered. TiPocket sends TiDB-Operator the definition of the cluster to test. In turn, TiDB-Operator creates a target TiDB cluster. Meanwhile, Loki collects the related logs.\\n2. Chaos Mesh injects faults in the cluster.\\n3. Using the test cases mentioned above, the user validates the health of the system. Any test case failure leads to workflow failure in Argo, which triggers Alertmanager to send the result to the specified Slack channel. If the test cases complete normally, the cluster is cleared, and Argo stands by until the next test.\\n\\n![Alert in Slack](/img/alert_message.png)\\n\\nThis is the complete TiPocket workflow. .\\n\\n## Join us\\n\\n[Chaos Mesh](https://github.com/pingcap/chaos-mesh) and [TiPocket](https://github.com/pingcap/tipocket) are both in active iterations. We have donated Chaos Mesh to [CNCF](https://github.com/cncf/toc/pull/367), and we look forward to more community members joining us in building a complete Chaos Engineering ecosystem. If this sounds interesting to you, check out our [website](https://chaos-mesh.org/), or join #project-chaos-mesh in the [CNCF Slack](hthttps://slack.cncf.io/)."},{"id":"/chaos-mesh-join-cncf-sandbox-project","metadata":{"permalink":"/zh/blog/chaos-mesh-join-cncf-sandbox-project","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-07-28-chaos-mesh-join-cncf-sandbox-project.md","source":"@site/blog/2020-07-28-chaos-mesh-join-cncf-sandbox-project.md","title":"Chaos Mesh Joins CNCF as a Sandbox Project","description":"Chaos Mesh Join CNCF as Sandbox Project","date":"2020-07-28T00:00:00.000Z","formattedDate":"2020\u5e747\u670828\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Kubernetes","permalink":"/zh/blog/tags/kubernetes"},{"label":"CNCF","permalink":"/zh/blog/tags/cncf"},{"label":"Cloud Native","permalink":"/zh/blog/tags/cloud-native"}],"readingTime":1.63,"truncated":true,"authors":[{"name":"Chaos Mesh Authors","title":"Maintainer of Chaos Mesh","url":"https://github.com/chaos-mesh","imageURL":"https://avatars1.githubusercontent.com/u/59082378?v=4"}],"prevItem":{"title":"Building an Automated Testing Framework Based on Chaos Mesh and Argo","permalink":"/zh/blog/building_automated_testing_framework"},"nextItem":{"title":"Simulating Clock Skew in K8s Without Affecting Other Containers on the Node","permalink":"/zh/blog/simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node"}},"content":"![Chaos Mesh Join CNCF as Sandbox Project](/img/chaos-mesh-cncf.png)\\nWe\u2019re thrilled to announce that [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is now officially accepted as a CNCF Sandbox project. As maintainers of Chaos Mesh, we\u2019d like to thank all the contributors and adopters. This would not be possible without your trust, support, and contributions.\\n\\n\x3c!--truncate--\x3e\\n\\nChaos Mesh is a powerful Chaos Engineering platform that orchestrates chaos experiments on Kubernetes environments. By covering comprehensive fault injection methods in Pod, network, file system, and even the kernel, we aim at providing a neutral, universal Chaos Engineering platform that enables cloud-native applications to be as resilient as they should be.\\n\\n![Architecture](/img/chaos-mesh.svg)\\n\\nWithin only 7 months since it was open-sourced on December 31st, 2019, Chaos Mesh has already received 2000 GitHub stars, with 44 contributors from multiple organizations. As a young project, the adoption in production has been the key recognition and motivation that pushes us forward constantly. Here is a list of our adopters so far:\\n\\n- [PingCAP](http://www.pingcap.com)\\n- [Xpeng Motor](https://en.xiaopeng.com/)\\n- [NetEase Fuxi Lab](https://fuxi.163.com/en/about.html)\\n- [JuiceFS](http://juicefs.com/?hl=en)\\n- [Dailymotion](https://www.dailymotion.com/)\\n- [Meituan-Dianping](https://about.meituan.com/en)\\n- [Celo](https://celo.org/)\\n\\nBeing a CNCF Sandbox project marks a major step forward for the project. It means that Chaos Mesh has become part of the great vendor-neutral cloud-native community. With the guidance and help from CNCF, Chaos Mesh will strive to develop a community with transparent, meritocracy-based governance for open communication and open collaboration, while driving the project forward, towards our ultimate goal of establishing the Chaos Engineering standards on Cloud.\\nCurrently, Chaos Mesh is in active development for 1.0 GA. Going forward, we will be focusing on the following aspects:\\n\\n- Lowering the bar of chaos engineering by improving Chaos Dashboard.\\n- Extending chaos injection to application layers\\n- Completing the full chaos engineering loop with status checking, reporting, and scenario defining, etc.\\n\\nIf you are interested in the project, check out our [website](https://chaos-mesh.org/), join our [Slack](https://slack.cncf.io/) discussions, or attend our [monthly meeting](https://docs.google.com/document/d/1H8IfmhIJiJ1ltg-XLjqR_P_RaMHUGrl1CzvHnKM_9Sc/edit) to know more. Or better yet, become part of us."},{"id":"/simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node","metadata":{"permalink":"/zh/blog/simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-04-20-simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node.md","source":"@site/blog/2020-04-20-simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node.md","title":"Simulating Clock Skew in K8s Without Affecting Other Containers on the Node","description":"Clock synchronization in distributed system","date":"2020-04-20T00:00:00.000Z","formattedDate":"2020\u5e744\u670820\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Kubernetes","permalink":"/zh/blog/tags/kubernetes"},{"label":"Distributed System","permalink":"/zh/blog/tags/distributed-system"}],"readingTime":8.43,"truncated":true,"authors":[{"name":"Cwen Yin","title":"Maintainer of Chaos Mesh","url":"https://github.com/cwen0","imageURL":"https://avatars1.githubusercontent.com/u/22956341?v=4"}],"prevItem":{"title":"Chaos Mesh Joins CNCF as a Sandbox Project","permalink":"/zh/blog/chaos-mesh-join-cncf-sandbox-project"},"nextItem":{"title":"Run Your First Chaos Experiment in 10 Minutes","permalink":"/zh/blog/run_your_first_chaos_experiment"}},"content":"![Clock synchronization in distributed system](/img/clock-sync-chaos-engineering-k8s.jpg)\\n\\n[Chaos Mesh\u2122](https://github.com/chaos-mesh/chaos-mesh), an easy-to-use, open-source, cloud-native chaos engineering platform for Kubernetes (K8s), has a new feature, TimeChaos, which simulates the [clock skew](https://en.wikipedia.org/wiki/Clock_skew#On_a_network) phenomenon. Usually, when we modify clocks in a container, we want a [minimized blast radius](https://learning.oreilly.com/library/view/chaos-engineering/9781491988459/ch07.html), and we don\'t want the change to affect the other containers on the node. In reality, however, implementing this can be harder than you think. How does Chaos Mesh solve this problem?\\n\\n\x3c!--truncate--\x3e\\n\\nIn this post, I\'ll describe how we hacked through different approaches of clock skew and how TimeChaos in Chaos Mesh enables time to swing freely in containers.\\n\\n## Simulating clock skew without affecting other containers on the node\\n\\nClock skew refers to the time difference between clocks on nodes within a network. It might cause reliability problems in a distributed system, and it\'s a concern for designers and developers of complex distributed systems. For example, in a distributed SQL database, it\'s vital to maintain a synchronized local clock across nodes to achieve a consistent global snapshot and ensure the ACID properties for transactions.\\n\\nCurrently, there are well-recognized [solutions to synchronize clocks](https://pingcap.com/blog/Time-in-Distributed-Systems/), but without proper testing, you can never be sure that your implementation is solid.\\n\\nThen how can we test global snapshot consistency in a distributed system? The answer is obvious: we can simulate clock skew to test whether distributed systems can keep a consistent global snapshot under abnormal clock conditions. Some testing tools support simulating clock skew in containers, but they have an impact on physical nodes.\\n\\n[TimeChaos](https://github.com/chaos-mesh/chaos-mesh/wiki/Time-Chaos) is a tool that **simulates clock skew in containers to test how it impacts your application without affecting the whole node**. This way, we can precisely identify the potential consequences of clock skew and take measures accordingly.\\n\\n## Various approaches for simulating clock skew we\'ve explored\\n\\nReviewing the existing choices, we know clearly that they cannot be applied to Chaos Mesh, which runs on Kubernetes. Two common ways of simulating clock skew--changing the node clock directly and using the Jepsen framework--change the time for all processes on the node. These are not acceptable solutions for us. In a Kubernetes container, if we inject a clock skew error that affects the entire node, other containers on the same node will be disturbed. Such a clumsy approach is not tolerable.\\n\\nThen how are we supposed to tackle this problem? Well, the first thing that comes into our mind is finding solutions in the kernel using [Berkeley Packet Filter](https://en.wikipedia.org/wiki/Berkeley_Packet_Filter) (BPF).\\n\\n### `LD_PRELOAD`\\n\\n`LD_PRELOAD` is a Linux environment variable that lets you define which dynamic link library is loaded before the program execution.\\n\\nThis variable has two advantages:\\n\\n- We can call our own functions without being aware of the source code.\\n- We can inject code into other programs to achieve specific purposes.\\n\\nFor some languages that use applications to call the time function in glibc, such as Rust and C, using `LD_PRELOAD` is enough to simulate clock skew. But things are trickier for Golang. Because languages such as Golang directly parse virtual Dynamic Shared Object ([vDSO](http://man7.org/linux/man-pages/man7/vdso.7.html)), a mechanism to speed up system calls. To obtain the time function address, we can\'t simply use `LD_PRELOAD` to intercept the glic interface. Therefore, `LD_PRELOAD` is not our solution.\\n\\n### Use BPF to modify the return value of `clock_gettime` system call\\n\\nWe also tried to filter the task [process identification number](http://www.linfo.org/pid.html) (PID) with BPF. This way, we could simulate clock skew on a specified process and modify the return value of the `clock_gettime` system call.\\n\\nThis seemed like a good idea, but we also encountered a problem: in most cases, vDSO speeds up `clock_gettime`, but `clock_gettime` doesn\'t make a system call. This selection didn\'t work, either. Oops.\\n\\nThankfully, we determined that if the system kernel version is 4.18 or later, and if we use the [HPET](https://www.kernel.org/doc/html/latest/timers/hpet.html) clock, `clock_gettime()` gets time by making normal system calls instead of vDSO. We implemented [a version of clock skew](https://github.com/chaos-mesh/bpfki) using this approach, and it works fine for Rust and C. As for Golang, the program can get the time right, but if we perform `sleep` during the clock skew injection, the sleep operation is very likely to be blocked. Even after the injection is canceled, the system cannot recover. Thus, we have to give up this approach, too.\\n\\n## TimeChaos, our final hack\\n\\nFrom the previous section, we know that programs usually get the system time by calling `clock_gettime`. In our case, `clock_gettime` uses vDSO to speed up the calling process, so we cannot use `LD_PRELOAD` to hack the `clock_gettime` system calls.\\n\\nWe figured out the cause; then what\'s the solution? Start from vDSO. If we can redirect the address that stores the `clock_gettime` return value in vDSO to an address we define, we can solve the problem.\\n\\nEasier said than done. To achieve this goal, we must tackle the following problems:\\n\\n- Know the user-mode address used by vDSO\\n- Know vDSO\'s kernel-mode address, if we want to modify the `clock_gettime` function in vDSO by any address in the kernel mode\\n- Know how to modify vDSO data\\n\\nFirst, we need to peek inside vDSO. We can see the vDSO memory address in `/proc/pid/maps`.\\n\\n```\\n$ cat /proc/pid/maps\\n...\\n7ffe53143000-7ffe53145000 r-xp 00000000 00:00 0                     [vdso]\\n```\\n\\nThe last line is vDSO information. The privilege of this memory space is `r-xp`: readable and executable, but not writable. That means the user mode cannot modify this memory. We can use [ptrace](http://man7.org/linux/man-pages/man2/ptrace.2.html) to avoid this restriction.\\n\\nNext, we use `gdb dump memory` to export the vDSO and use `objdump` to see what\'s inside. Here is what we get:\\n\\n```\\n(gdb) dump memory vdso.so 0x00007ffe53143000 0x00007ffe53145000\\n$ objdump -T vdso.so\\nvdso.so:    file format elf64-x86-64\\nDYNAMIC SYMBOL TABLE:\\nffffffffff700600  w  DF .text   0000000000000545  LINUX_2.6  clock_gettime\\n```\\n\\nWe can see that the whole vDSO is like a `.so` file, so we can use an executable and linkable format (ELF) file to format it. With this information, a basic workflow for implementing TimeChaos starts to take shape:\\n\\n![TimeChaos workflow](/img/timechaos-workflow.jpg)\\n\\n<div className=\\"caption-center\\"> TimeChaos workflow </div>\\n\\nThe chart above is the process of **TimeChaos**, an implementation of clock skew in Chaos Mesh.\\n\\n1. Use ptrace to attach the specified PID process to stop the current process.\\n2. Use ptrace to create a new mapping in the virtual address space of the calling process and use [`process_vm_writev`](https://linux.die.net/man/2/process_vm_writev) to write the `fake_clock_gettime` function we defined into the memory space.\\n3. Use `process_vm_writev` to write the specified parameters into `fake_clock_gettime`. These parameters are the time we would like to inject, such as two hours backward or two days forward.\\n4. Use ptrace to modify the `clock_gettime` function in vDSO and redirect to the `fake_clock_gettime` function.\\n5. Use ptrace to detach the PID process.\\n\\nIf you are interested in the details, see the [Chaos Mesh GitHub repository](https://github.com/chaos-mesh/chaos-mesh/blob/master/pkg/time/time_linux.go).\\n\\n## Simulating clock skew on a distributed SQL database\\n\\nStatistics speak volumes. Here we\'re going to try TimeChaos on [TiDB](https://pingcap.com/docs/stable/overview/), an open source, [NewSQL](https://en.wikipedia.org/wiki/NewSQL), distributed SQL database that supports [Hybrid Transactional/Analytical Processing](https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing) (HTAP) workloads, to see if the chaos testing can really work.\\n\\nTiDB uses a centralized service Timestamp Oracle (TSO) to obtain the globally consistent version number, and to ensure that the transaction version number increases monotonically. The TSO service is managed by the Placement Driver (PD) component. Therefore, we choose a random PD node and inject TimeChaos regularly, each with a 10-millisecond-backward clock skew. Let\'s see if TiDB can meet the challenge.\\n\\nTo better perform the testing, we use [bank](https://github.com/cwen0/bank) as the workload, which simulates the financial transfers in a banking system. It\'s often used to verify the correctness of database transactions.\\n\\nThis is our test configuration:\\n\\n```\\napiVersion: chaos-mesh.org/v1alpha1\\nkind: TimeChaos\\nmetadata:\\n  name: time-skew-example\\n  namespace: tidb-demo\\nspec:\\n  mode: one\\n  selector:\\n    labelSelectors:\\n      \\"app.kubernetes.io/component\\": \\"pd\\"\\n  timeOffset:\\n    sec: -600\\n  clockIds:\\n    - CLOCK_REALTIME\\n  duration: \\"10s\\"\\n  scheduler:\\n    cron: \\"@every 1m\\"\\n```\\n\\nDuring this test, Chaos Mesh injects TimeChaos into a chosen PD Pod every 1 millisecond for 10 seconds. Within the duration, the time acquired by PD will have a 600 second offset from the actual time. For further details, see [Chaos Mesh Wiki](https://github.com/chaos-mesh/chaos-mesh/wiki/Time-Chaos).\\n\\nLet\'s create a TimeChaos experiment using the `kubectl apply` command:\\n\\n```\\nkubectl apply -f pd-time.yaml\\n```\\n\\nNow, we can retrieve the PD log by the following command:\\n\\n```\\nkubectl logs -n tidb-demo tidb-app-pd-0 | grep \\"system time jump backward\\"\\n```\\n\\nHere\'s the log:\\n\\n```\\n[2020/03/24 09:06:23.164 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585041383060109693]\\n[2020/03/24 09:16:32.260 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585041992160476622]\\n[2020/03/24 09:20:32.059 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585042231960027622]\\n[2020/03/24 09:23:32.059 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585042411960079655]\\n[2020/03/24 09:25:32.059 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585042531963640321]\\n[2020/03/24 09:28:32.060 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585042711960148191]\\n[2020/03/24 09:33:32.063 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585043011960517655]\\n[2020/03/24 09:34:32.060 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585043071959942937]\\n[2020/03/24 09:35:32.059 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585043131978582964]\\n[2020/03/24 09:36:32.059 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585043191960687755]\\n[2020/03/24 09:38:32.060 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585043311959970737]\\n[2020/03/24 09:41:32.060 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585043491959970502]\\n[2020/03/24 09:45:32.061 +00:00] [ERROR] [systime_mon.go:32] [\\"system time jump backward\\"] [last=1585043731961304629]\\n...\\n```\\n\\nFrom the log above, we see that every now and then, PD detects that the system time rolls back. This means:\\n\\n- TimeChaos successfully simulates clock skew.\\n- PD can deal with the clock skew situation.\\n\\nThat\'s encouraging. But does TimeChaos affect services other than PD? We can check it out in the Chaos Dashboard:\\n\\n![Chaos Dashboard](/img/chaos-dashboard.jpg)\\n\\n<div className=\\"caption-center\\"> Chaos Dashboard </div>\\n\\nIt\'s clear that in the monitor, TimeChaos was injected every 1 millisecond and the whole duration lasted 10 seconds. What\'s more, TiDB was not affected by that injection. The bank program ran normally, and performance was not affected.\\n\\n## Try out Chaos Mesh\\n\\nAs a cloud-native chaos engineering platform, Chaos Mesh features all-around [fault injection methods for complex systems on Kubernetes](https://pingcap.com/blog/chaos-mesh-your-chaos-engineering-solution-for-system-resiliency-on-kubernetes/), covering faults in Pods, the network, the file system, and even the kernel.\\n\\nWanna have some hands-on experience in chaos engineering? Welcome to [Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh). This [10-minute tutorial](https://pingcap.com/blog/run-first-chaos-experiment-in-ten-minutes/) will help you quickly get started with chaos engineering and run your first chaos experiment with Chaos Mesh."},{"id":"/run_your_first_chaos_experiment","metadata":{"permalink":"/zh/blog/run_your_first_chaos_experiment","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-03-18-run-your-first-chaos-experiment.md","source":"@site/blog/2020-03-18-run-your-first-chaos-experiment.md","title":"Run Your First Chaos Experiment in 10 Minutes","description":"Run your first chaos experiment in 10 minutes","date":"2020-03-18T00:00:00.000Z","formattedDate":"2020\u5e743\u670818\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Kubernetes","permalink":"/zh/blog/tags/kubernetes"}],"readingTime":5.73,"truncated":true,"authors":[{"name":"Cwen Yin","title":"Maintainer of Chaos Mesh","url":"https://github.com/cwen0","imageURL":"https://avatars1.githubusercontent.com/u/22956341?v=4"}],"prevItem":{"title":"Simulating Clock Skew in K8s Without Affecting Other Containers on the Node","permalink":"/zh/blog/simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node"},"nextItem":{"title":"Chaos Mesh - Your Chaos Engineering Solution for System Resiliency on Kubernetes","permalink":"/zh/blog/chaos_mesh_your_chaos_engineering_solution"}},"content":"![Run your first chaos experiment in 10 minutes](/img/run-first-chaos-experiment-in-ten-minutes.jpg)\\n\\nChaos Engineering is a way to test a production software system\'s robustness by simulating unusual or disruptive conditions. For many people, however, the transition from learning Chaos Engineering to practicing it on their own systems is daunting. It sounds like one of those big ideas that require a fully-equipped team to plan ahead. Well, it doesn\'t have to be. To get started with chaos experimenting, you may be just one suitable platform away.\\n\\n\x3c!--truncate--\x3e\\n\\n[Chaos Mesh](https://github.com/chaos-mesh/chaos-mesh) is an **easy-to-use**, open-source, cloud-native Chaos Engineering platform that orchestrates chaos in Kubernetes environments. This 10-minute tutorial will help you quickly get started with Chaos Engineering and run your first chaos experiment with Chaos Mesh.\\n\\nFor more information about Chaos Mesh, refer to our [previous article](https://pingcap.com/blog/chaos-mesh-your-chaos-engineering-solution-for-system-resiliency-on-kubernetes/) or the [chaos-mesh project](https://github.com/chaos-mesh/chaos-mesh) on GitHub.\\n\\n## A preview of our little experiment\\n\\nChaos experiments are similar to experiments we do in a science class. It\'s perfectly fine to stimulate turbulent situations in a controlled environment. In our case here, we will be simulating network chaos on a small web application called [web-show](https://github.com/chaos-mesh/web-show). To visualize the chaos effect, web-show records the latency from its pod to the kube-controller pod (under the namespace of `kube-system`) every 10 seconds.\\n\\nThe following clip shows the process of installing Chaos Mesh, deploying web-show, and creating the chaos experiment within a few commands:\\n\\n![The whole process of the chaos experiment](/img/whole-process-of-chaos-experiment.gif)\\n\\n<div className=\\"caption-center\\"> The whole process of the chaos experiment </div>\\n\\nNow it\'s your turn! It\'s time to get your hands dirty.\\n\\n## Let\'s get started!\\n\\nFor our simple experiment, we use Kubernetes in the Docker ([Kind](https://kind.sigs.k8s.io/)) for Kubernetes development. You can feel free to use [Minikube](https://minikube.sigs.k8s.io/) or any existing Kubernetes clusters to follow along.\\n\\n### Prepare the environment\\n\\nBefore moving forward, make sure you have [Git](https://git-scm.com/) and [Docker](https://www.docker.com/) installed on your local computer, with Docker up and running. For macOS, it\'s recommended to allocate at least 6 CPU cores to Docker. For details, see [Docker configuration for Mac](https://docs.docker.com/docker-for-mac/#advanced).\\n\\n1. Get Chaos Mesh:\\n\\n   ```bash\\n   git clone https://github.com/chaos-mesh/chaos-mesh.git\\n   cd chaos-mesh/\\n   ```\\n\\n2. Install Chaos Mesh with the `install.sh` script:\\n\\n   ```bash\\n   ./install.sh --local kind\\n   ```\\n\\n   `install.sh` is an automated shell script that checks your environment, installs Kind, launches Kubernetes clusters locally, and deploys Chaos Mesh. To see the detailed description of `install.sh`, you can include the `--help` option.\\n\\n   > **Note:**\\n   >\\n   > If your local computer cannot pull images from `docker.io` or `gcr.io`, use the local gcr.io mirror and execute `./install.sh --local kind --docker-mirror` instead.\\n\\n3. Set the system environment variable:\\n\\n   ```bash\\n   source ~/.bash_profile\\n   ```\\n\\n> **Note:**\\n>\\n> - Depending on your network, these steps might take a few minutes.\\n> - If you see an error message like this:\\n>\\n>   ```bash\\n>   ERROR: failed to create cluster: failed to generate kubeadm config content: failed to get kubernetes version from node: failed to get file: command \\"docker exec --privileged kind-control-plane cat /kind/version\\" failed with error: exit status 1\\n>   ```\\n>\\n>   increase the available resources for Docker on your local computer and execute the following command:\\n>\\n>   ```bash\\n>   ./install.sh --local kind --force-local-kube\\n>   ```\\n\\nWhen the process completes you will see a message indicating Chaos Mesh is successfully installed.\\n\\n### Deploy the application\\n\\nThe next step is to deploy the application for testing. In our case here, we choose web-show because it allows us to directly observe the effect of network chaos. You can also deploy your own application for testing.\\n\\n1. Deploy web-show with the `deploy.sh` script:\\n\\n   ```bash\\n   # Make sure you are in the Chaos Mesh directory\\n   cd examples/web-show &&\\n   ./deploy.sh\\n   ```\\n\\n   > **Note:**\\n   >\\n   > If your local computer cannot pull images from `docker.io`, use the `local gcr.io` mirror and execute `./deploy.sh --docker-mirror` instead.\\n\\n2. Access the web-show application. From your web browser, go to `http://localhost:8081`.\\n\\n### Create the chaos experiment\\n\\nNow that everything is ready, it\'s time to run your chaos experiment!\\n\\nChaos Mesh uses [CustomResourceDefinitions](https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/) (CRD) to define chaos experiments. CRD objects are designed separately based on different experiment scenarios, which greatly simplifies the definition of CRD objects. Currently, CRD objects that have been implemented in Chaos Mesh include PodChaos, NetworkChaos, IOChaos, TimeChaos, and KernelChaos. Later, we\'ll support more fault injection types.\\n\\nIn this experiment, we are using [NetworkChaos](https://github.com/chaos-mesh/chaos-mesh/blob/master/examples/web-show/network-delay.yaml) for the chaos experiment. The NetworkChaos configuration file, written in YAML, is shown below:\\n\\n```\\napiVersion: chaos-mesh.org/v1alpha1\\nkind: NetworkChaos\\nmetadata:\\n  name: network-delay-example\\nspec:\\n  action: delay\\n  mode: one\\n  selector:\\n    namespaces:\\n      - default\\n    labelSelectors:\\n      \\"app\\": \\"web-show\\"\\n  delay:\\n    latency: \\"10ms\\"\\n    correlation: \\"100\\"\\n    jitter: \\"0ms\\"\\n  duration: \\"30s\\"\\n  scheduler:\\n    cron: \\"@every 60s\\"\\n```\\n\\nFor detailed descriptions of NetworkChaos actions, see [Chaos Mesh wiki](https://github.com/chaos-mesh/chaos-mesh/wiki/Network-Chaos). Here, we just rephrase the configuration as:\\n\\n- target: `web-show`\\n- mission: inject a `10ms` network delay every `60s`\\n- attack duration: `30s` each time\\n\\nTo start NetworkChaos, do the following:\\n\\n1. Run `network-delay.yaml`:\\n\\n   ```bash\\n   # Make sure you are in the chaos-mesh/examples/web-show directory\\n   kubectl apply -f network-delay.yaml\\n   ```\\n\\n2. Access the web-show application. In your web browser, go to `http://localhost:8081`.\\n\\n   From the line graph, you can tell that there is a 10 ms network delay every 60 seconds.\\n\\n![Using Chaos Mesh to insert delays in web-show](/img/using-chaos-mesh-to-insert-delays-in-web-show.png)\\n\\n<div className=\\"caption-center\\"> Using Chaos Mesh to insert delays in web-show </div>\\n\\nCongratulations! You just stirred up a little bit of chaos. If you are intrigued and want to try out more chaos experiments with Chaos Mesh, check out [examples/web-show](https://github.com/chaos-mesh/chaos-mesh/tree/master/examples/web-show).\\n\\n### Delete the chaos experiment\\n\\nOnce you\'re finished testing, terminate the chaos experiment.\\n\\n1. Delete `network-delay.yaml`:\\n\\n   ```bash\\n   # Make sure you are in the chaos-mesh/examples/web-show directory\\n   kubectl delete -f network-delay.yaml\\n   ```\\n\\n2. Access the web-show application. From your web browser, go to `http://localhost:8081`.\\n\\nFrom the line graph, you can see the network latency level is back to normal.\\n\\n![Network latency level is back to normal](/img/network-latency-level-is-back-to-normal.png)\\n\\n<div className=\\"caption-center\\"> Network latency level is back to normal </div>\\n\\n### Delete Kubernetes clusters\\n\\nAfter you\'re done with the chaos experiment, execute the following command to delete the Kubernetes clusters:\\n\\n```bash\\nkind delete cluster --name=kind\\n```\\n\\n> **Note:**\\n>\\n> If you encounter the `kind: command not found` error, execute `source ~/.bash_profile` command first and then delete the Kubernetes clusters.\\n\\n## Cool! What\'s next?\\n\\nCongratulations on your first successful journey into Chaos Engineering. How does it feel? Chaos Engineering is easy, right? But perhaps Chaos Mesh is not that easy-to-use. Command-line operation is inconvenient, writing YAML files manually is a bit tedious, or checking the experiment results is somewhat clumsy? Don\'t worry, Chaos Dashboard is on its way! Running chaos experiments on the web sure does sound exciting! If you\'d like to help us build testing standards for cloud platforms or make Chaos Mesh better, we\'d love to hear from you!\\n\\nIf you find a bug or think something is missing, feel free to file an issue, open a pull request (PR), or join us on the #project-chaos-mesh channel in the [CNCF slack workspace](https://slack.cncf.io/).\\n\\nGitHub: [https://github.com/chaos-mesh/chaos-mesh](https://github.com/chaos-mesh/chaos-mesh)"},{"id":"/chaos_mesh_your_chaos_engineering_solution","metadata":{"permalink":"/zh/blog/chaos_mesh_your_chaos_engineering_solution","editUrl":"https://github.com/chaos-mesh/website/edit/master/blog/2020-01-15-chaos-mesh-your-chaos-engineering-solution.md","source":"@site/blog/2020-01-15-chaos-mesh-your-chaos-engineering-solution.md","title":"Chaos Mesh - Your Chaos Engineering Solution for System Resiliency on Kubernetes","description":"Chaos Engineering","date":"2020-01-15T00:00:00.000Z","formattedDate":"2020\u5e741\u670815\u65e5","tags":[{"label":"Chaos Mesh","permalink":"/zh/blog/tags/chaos-mesh"},{"label":"Chaos Engineering","permalink":"/zh/blog/tags/chaos-engineering"},{"label":"Kubernetes","permalink":"/zh/blog/tags/kubernetes"}],"readingTime":10.94,"truncated":true,"authors":[{"name":"Cwen Yin","title":"Maintainer of Chaos Mesh","url":"https://github.com/cwen0","imageURL":"https://avatars1.githubusercontent.com/u/22956341?v=4"}],"prevItem":{"title":"Run Your First Chaos Experiment in 10 Minutes","permalink":"/zh/blog/run_your_first_chaos_experiment"}},"content":"![Chaos Engineering](/img/chaos-engineering.png)\\n\\n## Why Chaos Mesh?\\n\\nIn the world of distributed computing, faults can happen to your clusters unpredictably any time, anywhere. Traditionally we have unit tests and integration tests that guarantee a system is production ready, but these cover just the tip of the iceberg as clusters scale, complexities amount, and data volumes increase by PB levels. To better identify system vulnerabilities and improve resilience, Netflix invented [Chaos Monkey](https://netflix.github.io/chaosmonkey/) and injects various types of faults into the infrastructure and business systems. This is how Chaos Engineering was originated.\\n\\n\x3c!--truncate--\x3e\\n\\nAt [PingCAP](https://chaos-mesh.org/), we are facing the same problem while building [TiDB](https://github.com/pingcap/tidb), an open source distributed NewSQL database. To be fault tolerant, or resilient holds especially true to us, because the most important asset for any database users, the data itself, is at stake. To ensure resilience, we started [practicing Chaos Engineering](https://pingcap.com/blog/chaos-practice-in-tidb/) internally in our testing framework from a very early stage. However, as TiDB grew, so did the testing requirements. We realized that we needed a universal chaos testing platform, not just for TiDB, but also for other distributed systems.\\n\\nTherefore, we present to you Chaos Mesh, a cloud-native Chaos Engineering platform that orchestrates chaos experiments on Kubernetes environments. It\'s an open source project available at [https://github.com/chaos-mesh/chaos-mesh](https://github.com/chaos-mesh/chaos-mesh).\\n\\nIn the following sections, I will share with you what Chaos Mesh is, how we design and implement it, and finally I will show you how you can use it in your environment.\\n\\n## What can Chaos Mesh do?\\n\\nChaos Mesh is a versatile Chaos Engineering platform that features all-around fault injection methods for complex systems on Kubernetes, covering faults in Pod, network, file system, and even the kernel.\\n\\nHere is an example of how we use Chaos Mesh to locate a TiDB system bug. In this example, we simulate Pod downtime with our distributed storage engine ([TiKV](https://pingcap.com/docs/stable/architecture/#tikv-server)) and observe changes in queries per second (QPS). Regularly, if one TiKV node is down, the QPS may experience a transient jitter before it returns to the level before the failure. This is how we guarantee high availability.\\n\\n![Chaos Mesh discovers downtime recovery exceptions in TiKV](/img/chaos-mesh-discovers-downtime-recovery-exceptions-in-tikv.png)\\n\\n<div className=\\"caption-center\\"> Chaos Mesh discovers downtime recovery exceptions in TiKV</div>\\n\\nAs you can see from the dashboard:\\n\\n- During the first two downtimes, the QPS returns to normal after about 1 minute.\\n- After the third downtime, however, the QPS takes much longer to recover\u2014about 9 minutes. Such a long downtime is unexpected, and it would definitely impact online services.\\n\\nAfter some diagnosis, we found the TiDB cluster version under test (V3.0.1) had some tricky issues when handling TiKV downtimes. We resolved these issues in later versions.\\n\\nBut Chaos Mesh can do a lot more than just simulate downtime. It also includes these fault injection methods:\\n\\n- **pod-kill:** Simulates Kubernetes Pods being killed\\n- **pod-failure:** Simulates Kubernetes Pods being continuously unavailable\\n- **network-delay:** Simulates network delay\\n- **network-loss:** Simulates network packet loss\\n- **network-duplication:** Simulates network packet duplication\\n- **network-corrupt:** Simulates network packet corruption\\n- **network-partition:** Simulates network partition\\n- **I/O delay:** Simulates file system I/O delay\\n- **I/O errno:** Simulates file system I/O errors\\n\\n## Design principles\\n\\nWe designed Chaos Mesh to be easy to use, scalable, and designed for Kubernetes.\\n\\n### Easy to use\\n\\nTo be easy to use, Chaos Mesh must:\\n\\n- Require no special dependencies, so that it can be deployed directly on Kubernetes clusters, including [Minikube](https://github.com/kubernetes/minikube).\\n- Require no modification to the deployment logic of the system under test (SUT), so that chaos experiments can be performed in a production environment.\\n- Easily orchestrate fault injection behaviors in chaos experiments, and easily view experiment status and results. You should also be able to quickly rollback injected failures.\\n- Hide underlying implementation details so that users can focus on orchestrating the chaos experiments.\\n\\n### Scalable\\n\\nChaos Mesh should be scalable, so that we can \\"plug\\" new requirements into it conveniently without reinventing the wheel. Specifically, Chaos Mesh must:\\n\\n- Leverage existing implementations so that fault injection methods can be easily scaled.\\n- Easily integrate with other testing frameworks.\\n\\n### Designed for Kubernetes\\n\\nIn the container world, Kubernetes is the absolute leader. Its growth rate of adoption is far beyond everybody\'s expectations, and it has won the war of containerized orchestration. In essence, Kubernetes is an operating system for the cloud.\\n\\nTiDB is a cloud-native distributed database. Our internal automated testing platform was built on Kubernetes from the beginning. We had hundreds of TiDB clusters running on Kubernetes every day for various experiments, including extensive chaos testing to simulate all kinds of failures or issues in a production environment. To support these chaos experiments, the combination of chaos and Kubernetes became a natural choice and principle for our implementation.\\n\\n## CustomResourceDefinitions design\\n\\nChaos Mesh uses [CustomResourceDefinitions](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) (CRD) to define chaos objects. In the Kubernetes realm, CRD is a mature solution for implementing custom resources, with abundant implementation cases and toolsets available. Using CRD makes Chaos Mesh naturally integrate with the Kubernetes ecosystem.\\n\\nInstead of defining all types of fault injections in a unified CRD object, we allow flexible and separate CRD objects for different types of fault injection. If we add a fault injection method that conforms to an existing CRD object, we scale directly based on this object; if it is a completely new method, we create a new CRD object for it. With this design, chaos object definitions and the logic implementation are extracted from the top level, which makes the code structure clearer. This approach also reduces the degree of coupling and the probability of errors. In addition, Kubernetes\' [controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) is a great wrapper for implementing controllers. This saves us a lot of time because we don\'t have to repeatedly implement the same set of controllers for each CRD project.\\n\\nChaos Mesh implements the PodChaos, NetworkChaos, and IOChaos objects. The names clearly identify the corresponding fault injection types.\\n\\nFor example, Pod crashing is a very common problem in a Kubernetes environment. Many native resource objects automatically handle such errors with typical actions such as creating a new Pod. But can our application really deal with such errors? What if the Pod won\'t start?\\n\\nWith well-defined actions such as `pod-kill`, PodChaos can help us pinpoint these kinds of issues more effectively. The PodChaos object uses the following code:\\n\\n```yml\\nspec:\\n action: pod-kill\\n mode: one\\n selector:\\n   namespaces:\\n     - tidb-cluster-demo\\n   labelSelectors:\\n     \\"app.kubernetes.io/component\\": \\"tikv\\"\\n  scheduler:\\n   cron: \\"@every 2m\\"\\n```\\n\\nThis code does the following:\\n\\n- The `action` attribute defines the specific error type to be injected. In this case, `pod-kill` kills Pods randomly.\\n- The `selector` attribute limits the scope of chaos experiment to a specific scope. In this case, the scope is TiKV Pods for the TiDB cluster with the `tidb-cluster-demo` namespace.\\n- The `scheduler` attribute defines the interval for each chaos fault action.\\n\\nFor more details on CRD objects such as NetworkChaos and IOChaos, see the [Chaos-mesh documentation](https://github.com/chaos-mesh/chaos-mesh).\\n\\n## How does Chaos Mesh work?\\n\\nWith the CRD design settled, let\'s look at the big picture on how Chaos Mesh works. The following major components are involved:\\n\\n- **controller-manager**\\n\\n  Acts as the platform\'s \\"brain.\\" It manages the life cycle of CRD objects and schedules chaos experiments. It has object controllers for scheduling CRD object instances, and the [admission-webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) controller dynamically injects sidecar containers into Pods.\\n\\n- **chaos-daemon**\\n\\n  Runs as a privileged DaemonSet that can operate network devices on the node and Cgroup.\\n\\n- **sidecar**\\n\\n  Runs as a special type of container that is dynamically injected into the target Pod by the admission-webhooks. For example, the `chaosfs` sidecar container runs a fuse-daemon to hijack the I/O operation of the application container.\\n\\n![Chaos Mesh workflow](/img/chaos-mesh-workflow.png)\\n\\n<div className=\\"caption-center\\"> Chaos Mesh workflow </div>\\n\\nHere is how these components streamline a chaos experiment:\\n\\n1. Using a YAML file or Kubernetes client, the user creates or updates chaos objects to the Kubernetes API server.\\n2. Chaos Mesh uses the API server to watch the chaos objects and manages the lifecycle of chaos experiments through creating, updating, or deleting events. In this process, controller-manager, chaos-daemon, and sidecar containers work together to inject errors.\\n3. When admission-webhooks receives a Pod creation request, the Pod object to be created is dynamically updated; for example, it is injected into the sidecar container and the Pod.\\n\\n## Running chaos\\n\\nThe above sections introduce how we design Chaos Mesh and how it works. Now let\'s get down to business and show you how to use Chaos Mesh. Note that the chaos testing time may vary depending on the complexity of the application to be tested and the test scheduling rules defined in the CRD.\\n\\n### Preparing the environment\\n\\nChaos Mesh runs on Kubernetes v1.12 or later. Helm, a Kubernetes package management tool, deploys and manages Chaos Mesh. Before you run Chaos Mesh, make sure that Helm is properly installed in the Kubernetes cluster. To set up the environment, do the following:\\n\\n1. Make sure you have a Kubernetes cluster. If you do, skip to step 2; otherwise, start one locally using the script provided by Chaos Mesh:\\n\\n   ```bash\\n   // install kind\\n   curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.6.1/kind-$(uname)-amd64\\n   chmod +x ./kind\\n   mv ./kind /some-dir-in-your-PATH/kind\\n\\n   // get script\\n   git clone https://github.com/chaos-mesh/chaos-mesh\\n   cd chaos-mesh\\n   // start cluster\\n   hack/kind-cluster-build.sh\\n   ```\\n\\n   **Note:** Starting Kubernetes clusters locally affects network-related fault injections.\\n\\n2. If the Kubernetes cluster is ready, use [Helm](https://helm.sh/) and [Kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) to deploy Chaos Mesh:\\n\\n   ```bash\\n   git clone https://github.com/chaos-mesh/chaos-mesh.git\\n   cd chaos-mesh\\n   // create CRD resource\\n   kubectl apply -f manifests/\\n   // install chaos-mesh\\n   helm install helm/chaos-mesh --name=chaos-mesh --namespace=chaos-testing\\n   ```\\n\\n   Wait until all components are installed, and check the installation status using:\\n\\n   ```bash\\n   // check chaos-mesh status\\n   kubectl get pods --namespace chaos-testing -l app.kubernetes.io/instance=chaos-mesh\\n   ```\\n\\n   If the installation is successful, you can see all pods up and running. Now, time to play.\\n\\n   You can run Chaos Mesh using a YAML definition or a Kubernetes API.\\n\\n### Running chaos using a YAML file\\n\\nYou can define your own chaos experiments through the YAML file method, which provides a fast, convenient way to conduct chaos experiments after you deploy the application. To run chaos using a YAML file, follow the steps below:\\n\\n**Note:** For illustration purposes, we use TiDB as our system under test. You can use a target system of your choice, and modify the YAML file accordingly.\\n\\n1. Deploy a TiDB cluster named `chaos-demo-1`. You can use [TiDB Operator](https://github.com/pingcap/tidb-operator) to deploy TiDB.\\n2. Create the YAML file named `kill-tikv.yaml` and add the following content:\\n\\n   ```yml\\n   apiVersion: chaos-mesh.org/v1alpha1\\n   kind: PodChaos\\n   metadata:\\n     name: pod-kill-chaos-demo\\n     namespace: chaos-testing\\n   spec:\\n     action: pod-kill\\n     mode: one\\n     selector:\\n       namespaces:\\n         - chaos-demo-1\\n       labelSelectors:\\n         \'app.kubernetes.io/component\': \'tikv\'\\n     scheduler:\\n       cron: \'@every 1m\'\\n   ```\\n\\n3. Save the file.\\n4. To start chaos, `kubectl apply -f kill-tikv.yaml`.\\n\\nThe following chaos experiment simulates the TiKV Pods being frequently killed in the `chaos-demo-1` cluster:\\n\\n![Chaos experiment running](/img/chaos-experiment-running.gif)\\n\\n<div className=\\"caption-center\\"> Chaos experiment running </div>\\n\\nWe use a sysbench program to monitor the real-time QPS changes in the TiDB cluster. When errors are injected into the cluster, the QPS show a drastic jitter, which means a specific TiKV Pod has been deleted, and Kubernetes then re-creates a new TiKV Pod.\\n\\nFor more YAML file examples, see <https://github.com/chaos-mesh/chaos-mesh/tree/master/examples>.\\n\\n### Running chaos using the Kubernetes API\\n\\nChaos Mesh uses CRD to define chaos objects, so you can manipulate CRD objects directly through the Kubernetes API. This way, it is very convenient to apply Chaos Mesh to your own applications with customized test scenarios and automated chaos experiments.\\n\\nIn the [test-infra](https://github.com/pingcap/tipocket/tree/35206e8483b66f9728b7b14823a10b3e4114e0e3/test-infra) project, we simulate potential errors in [etcd](https://github.com/pingcap/tipocket/blob/35206e8483b66f9728b7b14823a10b3e4114e0e3/test-infra/tests/etcd/nemesis_test.go) clusters on Kubernetes, including nodes restarting, network failure, and file system failure.\\n\\nThe following is a Chaos Mesh sample script using the Kubernetes API:\\n\\n```\\nimport (\\n    \\"context\\"\\n\\n \\"github.com/chaos-mesh/chaos-mesh/api/v1alpha1\\"\\n    \\"sigs.k8s.io/controller-runtime/pkg/client\\"\\n)\\n\\nfunc main() {\\n  ...\\n  delay := &chaosv1alpha1.NetworkChaos{\\n  Spec: chaosv1alpha1.NetworkChaosSpec{...},\\n      }\\n      k8sClient := client.New(conf, client.Options{ Scheme: scheme.Scheme })\\n  k8sClient.Create(context.TODO(), delay)\\n      k8sClient.Delete(context.TODO(), delay)\\n}\\n```\\n\\n## What does the future hold?\\n\\nIn this article, we introduced you to Chaos Mesh, our open source cloud-native Chaos Engineering platform. There are still many pieces in progress, with more details to unveil regarding the design, use cases, and development. Stay tuned.\\n\\nOpen sourcing is just a starting point. In addition to the infrastructure-level chaos experiments introduced in previous sections, we are in the process of supporting a wider range of fault types of finer granularity, such as:\\n\\n- Injecting errors at the system call and kernel levels with the assistance of eBPF and other tools\\n- Injecting specific error types into the application function and statement levels by integrating [failpoint](https://github.com/pingcap/failpoint), which will cover scenarios that are otherwise impossible with conventional injection methods\\n\\nMoving forward, we will continuously improve the Chaos Mesh Dashboard, so that users can easily see if and how their online businesses are impacted by fault injections. In addition, our roadmap includes an easy-to-use fault orchestration interface. We\'re planning other cool features, such as Chaos Mesh Verifier and Chaos Mesh Cloud.\\n\\nIf any of these sound interesting to you, join us in building a world class Chaos Engineering platform. May our applications dance in chaos on Kubernetes!\\n\\nIf you find a bug or think something is missing, feel free to file an [issue](https://github.com/chaos-mesh/chaos-mesh/issues), open a PR, or message us in the #project-chaos-mesh channel in the [CNCF Slack](https://slack.cncf.io/) workspace.\\n\\nGitHub: [https://github.com/chaos-mesh/chaos-mesh](https://github.com/chaos-mesh/chaos-mesh)"}]}')}}]);